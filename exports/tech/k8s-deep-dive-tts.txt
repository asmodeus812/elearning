Introduction

Kubernetes is an application orchestrator. For the most part, it orchestrates containerized, cloud-native microservices. An orchestrator is a system that deploys and manages applications. It can deploy your app and dynamically respond to changes. For example, Kubernetes can deploy your app, scale it up and down dynamically based on demand, self-heal when things break, perform zero-downtime rolling updates and rollbacks, and do many other things.

Let’s clarify what a containerized app is. A containerized app is simply an application that runs inside a container. Before containers, applications ran on physical servers or virtual machines. Containers are the next evolution in how we package and run applications. They are faster, more lightweight, and better suited to modern business requirements than traditional servers or virtual machines.

A cloud-native app is designed to meet the demands of the cloud, such as auto-scaling, self-healing, rolling updates, and rollbacks. It’s important to note that cloud-native apps are not limited to running in public clouds. While they can run on public cloud platforms, they can also run anywhere you have Kubernetes, including your own on-premises data center.

Microservice apps are built from many independent, small, specialized parts that work together to form a meaningful application. For example, an e-commerce app might include a web front end, a catalog service, a shopping cart, an authentication service, a logging service, and a persistent store. Each of these individual services is called a microservice. Typically, each microservice is coded and owned by a different team, can have its own release cycle, and can be scaled independently. For instance, you can patch and scale the logging microservice without affecting any of the others. Building applications this way is essential for enabling cloud-native features. Usually, each microservice runs as a container. So, in our e-commerce example, there would be one or more containers for each microservice, such as the web front end, catalog, shopping cart, and so on.

With all of this in mind, Kubernetes deploys and manages, or orchestrates, applications that are packaged and run as containers. These applications are built in ways—cloud-native and microservice-based—that allow them to scale, self-heal, and be updated in line with modern, cloud-like requirements.

History

Since Amazon introduced Amazon Web Services, the world of technology changed dramatically. Since then, everyone has been trying to catch up. One of the companies working to catch up was Google. Google has its own robust cloud platform and needed a way to abstract the value of AWS, making it easier for potential customers to move away from AWS and into Google’s cloud.

Google also has extensive experience working with containers at scale. Huge Google applications, such as Search and Gmail, have been running at extreme scale on containers for many years—long before Docker made containers easy to use. To orchestrate and manage these containerized apps, Google developed a couple of in-house proprietary systems called Borg and Omega.

Google took the lessons learned from these systems and created a new platform called Kubernetes. In 2014, Google donated Kubernetes to the newly formed Cloud Native Computing Foundation, or CNCF, as an open-source project.

Kubernetes enables two things that Google and the rest of the industry need. First, it abstracts underlying infrastructure, such as AWS. Second, it makes it easy to move applications on and off different clouds.

Since its introduction in twenty fourteen, Kubernetes has become the most important cloud-native technology on the planet. Like many modern cloud-native projects, it is written in the Go programming language. It is built in the open on GitHub, actively discussed on IRC channels, and followed widely on social media. There are also regular conferences and meetups dedicated to Kubernetes.

Meaning

The name Kubernetes comes from the Greek word meaning “helmsman”—the person who steers a seafaring ship. This theme is reflected in the logo, which is the wheel, or helm control, of a ship. You will often see Kubernetes shortened to “k eight s,” pronounced “Kate.” The number eight replaces the eight characters between the K and the S in the name. Some people even joke that Kubernetes has a girlfriend named Kate.

Kubernetes and Docker

Kubernetes and Docker are two complementary technologies. Docker provides tools to build and package applications as container images. It can also run containers. Kubernetes, on the other hand, does not build or run containers directly. Instead, Kubernetes operates at a higher level, providing orchestration services such as self-healing, scaling, and updating.

It is common practice to use Docker for build-time tasks, such as packaging applications as containers, and then use a combination of Kubernetes and Docker to run them. In this model, Kubernetes performs high-level orchestration tasks, while Docker handles low-level tasks like starting and stopping containers.

Imagine you have a Kubernetes cluster with ten nodes running your production app. Behind the scenes, each cluster node is running Docker as its container runtime. This means Docker is the low-level technology that starts and stops the containerized apps. Kubernetes is the higher-level technology that manages the bigger picture. It tells Docker what to do, decides which nodes to run containers on, when to scale up or down, and when to execute updates.

Docker is not the only container runtime that Kubernetes supports. It also supports other runtimes such as gVisor, containerd, and Kata. Kubernetes has features that abstract the container runtime and make it interchangeable. The Container Runtime Interface, or CRI, is an abstraction layer that standardizes the way third-party container runtimes work with Kubernetes. Runtime Classes allow you to create different classes of runtimes. For example, the gVisor or Kata Containers runtimes might provide better workload isolation than Docker or containerd.

Kubernetes and Swarm

In twenty sixteen and twenty seventeen, there was a period known as the orchestrator wars. During this time, Docker Swarm, Mesosphere DCOS, and Kubernetes competed to become the de facto container orchestrator. To make a long story short, Kubernetes won.

You may hear people talk about how Kubernetes relates to Google’s Borg and Omega systems. As mentioned earlier, Google has been running containers at scale for a long time—apparently processing billions of containers each week. Google has been running services like Search, Gmail, and the Google File System on many containers for a very long time. Orchestrating these containerized apps was the job of in-house technologies called Borg and Omega.

It is not a stretch to connect Kubernetes with Borg and Omega. All three are involved in orchestrating containers at scale, and all are related to Google. However, Kubernetes is not an open-source version of Borg or Omega. While it shares common traits, technologies, and even some of the same people, Borg and Omega remain proprietary, closed-source projects. All three are separate, but they are related. In fact, some of the people who built Borg and Omega were, and still are, involved with Kubernetes. So, although Kubernetes was built from scratch, it leverages much of what was learned at Google with Omega and Borg.

Theory

At the highest level, Kubernetes is two things: a cluster to run applications on, and an orchestrator of cloud-native microservice applications.

Kubernetes as an Operating System

Kubernetes has emerged as the de facto platform for deploying and managing cloud-native applications. In many ways, it is like an operating system for the cloud. Just as Linux abstracts the hardware differences between server platforms, Kubernetes abstracts the differences between various private and public clouds. The result is that, as long as you are running Kubernetes, it does not matter if the underlying systems are on-premises, in your own data center, on edge devices, or in the public cloud. Kubernetes provides a consistent platform.

Kubernetes as a Cluster

Kubernetes is like any other cluster—a group of machines to host applications on. These machines are called nodes, and they can be physical servers, virtual machines, cloud instances, Raspberry Pis, and more. A Kubernetes cluster is made up of a control plane and nodes. The control plane exposes the API, has a scheduler for assigning work, and records the state of the cluster and applications in a persistent store. Nodes are where user applications run.

It can be helpful to think of the control plane as the brains of the cluster, and the nodes as the muscle. The control plane is the brains because it implements clever features such as scheduling, auto-scaling, and zero-downtime rolling updates.

Kubernetes as Orchestrator

Now, let’s look at Kubernetes as an orchestrator.


Orchestrator is just a fancy word for a system that takes care of deploying and managing applications. To make this clearer, let’s use a real-world analogy. Think of a football team. The team is made up of individuals, each with different skills and roles. Some players defend, some attack, some are great at passing, others excel at tackling or shooting. Then, the coach steps in. The coach assigns positions, organizes the team with a purpose, and ensures everyone sticks to the game plan. The coach also handles unexpected events, like injuries or changes in circumstances.

Microservices applications on Kubernetes work in a similar way. You start with many specialized microservices. Some serve web pages, others handle authentication, some perform searches, and others persist data. Kubernetes acts like the coach. It organizes all these services into a cohesive application and keeps everything running smoothly. It also responds to events and changes, such as auto-scaling, updating, or rolling releases.

When you begin with an application, you package it as a container and hand it over to the Kubernetes cluster. The cluster consists of one or more control plane nodes and a group of worker nodes. As mentioned earlier, control plane nodes provide the cluster’s intelligence, while worker nodes are where user applications actually run.

Control plane

A Kubernetes cluster is made up of control plane nodes and worker nodes. These nodes are Linux hosts, which can be virtual machines, physical servers in your datacenter or even your basement, or instances in a private or public cloud. Kubernetes can even run on ARM and IoT devices.

A control plane node is a server running a collection of system services that make up the control plane of the cluster. These are sometimes called masters, heads, or head nodes. The simplest setups use a single control plane node, which is fine for labs and testing. However, in production environments, it’s vital to have multiple control plane nodes configured for high availability. It’s also considered best practice not to run user applications on control plane nodes, so they can focus entirely on managing the cluster.

Here are some of the core components of the control plane. First, etcd is a distributed key-value store that holds all cluster data. It’s often called the cluster store. Next, the kube-apiserver acts as the front-end for the Kubernetes control plane, exposing the Kubernetes API. The kube-scheduler assigns workloads, known as pods, to nodes based on resource availability and constraints. Finally, the kube-controller-manager runs controllers that regulate the state of the cluster, such as the node controller and the replication controller.

Kubernetes is self-bootstrapping. This means that components like the API server, and other system components, are themselves backed by Kubernetes-native objects—such as pods, deployments, and services—located in the kube-system namespace. This is the same way a user would deploy their own images and containers. In fact, almost the entire set of system Kubernetes components are represented with the same types of objects: services, deployments, endpoints, replica sets, and so on. The Kubernetes environment is self-inspecting, so you can easily see the different components that are part of the control plane. These are deployed as native Kubernetes objects, and you can view them using the kubectl command by looking in the kube-system namespace. Other system components, like ingress controllers, might be deployed in namespaces other than kube-system, but the general rule still holds: Kubernetes is, in a way, self-bootstrapping.

For example, if you run a command to list the pods in the kube-system namespace, you’ll see entries for core components like coredns, etcd, kube-apiserver, kube-controller-manager, kube-proxy, kube-scheduler, and storage-provisioner. This output shows a simple example of what the control plane consists of. Each of these is an actual pod, backed by an image, responsible for the internal mechanisms and workings of Kubernetes orchestration. In other words, Kubernetes runs on itself—it is a self-bootstrapping system.

Each of these components will be explored in more detail later, but for now, this gives you an overarching view of the Kubernetes architecture.

The API Server

The API server is the Grand Central Station of Kubernetes. All communication between components—both internal system components and external user components—must go through the API server. In other words, all roads lead to the API server.

The API server exposes a RESTful API. You send YAML configuration files to it over HTTPS. These YAML files, sometimes called manifests, describe the desired state of an application. This desired state includes details like which container image to use, which ports to expose, and how many pod replicas to run.

All requests to the API server are subject to authentication and authorization checks. Once these checks are complete, the configuration in the YAML file is validated, persisted to the cluster store, and work is scheduled to the server.

As mentioned earlier, the API server is essentially a pod backed by an image and a service object. You can see these by listing the pods in the system namespace. There, you’ll find a pod called kube-apiserver.

The Cluster Store

The cluster store is the only stateful part of the control plane. It persistently stores the entire configuration and state of the cluster. As such, it is a vital component of every Kubernetes cluster—without the cluster store, there is no cluster.

Currently, the cluster store is based on etcd, a popular distributed database. Because it is the single source of truth for a cluster, you should run between three and five replicas of the etcd service for high availability. You should also have adequate ways to recover when things go wrong.

A default installation of Kubernetes installs a replica of the cluster store on every control plane node and automatically configures high availability. Regarding availability, etcd prefers consistency over availability. This means it does not tolerate split-brain scenarios and will halt updates to the cluster to maintain consistency. However, if this happens, user applications should continue to work—you just won’t be able to update the cluster configuration.

As with all distributed databases, consistency of writes is vital. For example, multiple writes to the same value from different places need to be handled. Etcd uses the RAFT consensus algorithm to achieve this.

The Controller Manager

The controller manager implements all the background controllers that monitor cluster components and respond to events. Architecturally, it is a controller of controllers. It spawns all the independent controllers and monitors them.

Some of the controllers include the deployment controller, the stateful set controller, and the replica set controller. Each one is responsible for a small subset of cluster intelligence and runs as a background watch loop, constantly watching the API server for changes.

The goal is to ensure the observed state of the cluster matches the desired state. The logic implemented by each controller follows a pattern that is at the heart of Kubernetes and declarative design patterns. First, the controller obtains the desired state. Next, it observes the current state. Then, it determines the differences between the two. Finally, it reconciles those differences.

Each controller is highly specialized and only interested in its own area of the Kubernetes cluster. The design avoids unnecessary complexity by not making controllers aware of other parts of the system. Each controller takes care of its own responsibilities and leaves everything else alone. This is key to the distributed design of Kubernetes and follows the Unix philosophy.

Controllers are control loops that watch the state of the cluster and make changes to bring the current state closer to the desired state. For example, the node controller manages the lifecycle of nodes, while the service controller ensures that traffic is routed to the correct pods on the fly.

Controllers run on a watch loop. In practice, these are pods or processes that watch the API server for changes in the desired state and then work to match that in the actual state within the Kubernetes environment.


Deployment Controller

The Deployment Controller is responsible for managing updates to Pods and ReplicaSet objects. It ensures that the desired state of your application, as defined in your deployment configuration, is maintained across the cluster.

Ingress Controller

The Ingress Controller is a special type of controller that handles Ingress resources. Unlike core control plane components, it is not built into Kubernetes by default. Instead, it is a user-deployed component that runs as a Pod within the cluster. The Ingress Controller watches for Ingress resources and configures external load balancers or proxies, such as NGINX or Traefik, to route incoming traffic to the appropriate services inside the cluster.

Annotations in Kubernetes

Kubernetes objects allow you to define annotations, which are key-value pairs used to store metadata or configuration information. Annotations provide a general-purpose mechanism for different Kubernetes controllers to configure their own behavior. Each object in Kubernetes is managed by a corresponding controller, which is responsible for handling the object within the control plane. Controllers can enable specific features by reading annotation metadata defined in the object’s manifest. The annotation values are attached to the object instance, but it is the controller that interprets and enforces them.

It is important to understand that controllers are stateless processors operating within the stateful Kubernetes environment. They react to changes in object state and apply the necessary logic based on the annotations and other configuration.

Manifest Example: Annotations

Here is an example of how annotations are defined in a Kubernetes manifest. In this example, annotations are attached to Ingress and Deployment objects. The key point is that annotation values are bound to the specific object instance, which is uniquely identified by the metadata.name property. This name is not just for human readability—it is used by the Kubernetes control plane and controllers to uniquely identify and manage the object.

In the manifest, the Ingress object is given a name and several annotations, such as rewriting URL paths and redirecting HTTP to HTTPS. The Deployment object is also named and includes annotations for custom scaling, enabling Istio sidecar injection, and configuring autoscaling metrics. These annotations are interpreted by the relevant controllers to adjust the behavior of each object.

The Scheduler

At a high level, the Kubernetes scheduler monitors the API server for new work tasks and assigns them to appropriate, healthy worker nodes. The scheduler uses complex logic to filter out nodes that cannot run a given task, then ranks the remaining nodes based on various criteria. The node with the highest ranking score is selected to run the task.

When evaluating nodes, the scheduler checks for factors such as node taints, affinity or anti-affinity rules, required network port availability, and sufficient resources. Nodes that do not meet the requirements are ignored. The remaining nodes are ranked based on factors like whether they already have the required image, how much free resources they have, and how many tasks they are currently running. Each factor contributes points, and the node with the most points is chosen.

If the scheduler cannot find a suitable node, the task is not scheduled and is marked as pending. It is important to note that the scheduler does not run tasks itself—it only selects the nodes where tasks, typically Pods or containers, will run.

The Node

The kubelet is the main agent running on every cluster node. In practice, the terms “node” and “kubelet” are often used interchangeably. When a node joins a cluster, the kubelet is installed and registers the node’s CPU, memory, and storage with the cluster.

One of the kubelet’s primary responsibilities is to watch the API server for new work tasks. When it detects a task assigned to its node, it executes the task and maintains a reporting channel back to the control plane. If the kubelet cannot run a task, it reports this back to the control plane, which then decides what action to take. The kubelet does not attempt to find another node for the task—it simply reports the issue and lets the control plane handle it.

The Proxy

The kube-proxy is another essential component that runs on every node. It is responsible for local cluster networking, ensuring each node receives a unique IP address. The kube-proxy sets up local iptables or IPVS rules to handle routing and load balancing of traffic within the Pod network.

The kube-proxy is crucial for enabling node-to-node internal cluster communication. It does this by monitoring for new Services and Endpoints, then configuring the appropriate IPVS or iptables rules on the node. The actual traffic routing is handled by the kernel, which reads these rules and performs IP translation and mapping between nodes. The kube-proxy itself does not actively participate in the traffic flow; it simply sets up the necessary state.

The Runtime

The kubelet requires a container runtime to perform container-related tasks, such as pulling images and starting or stopping containers. In the early days of Kubernetes, Docker was natively supported. However, Kubernetes has since moved to a plugin model called the Container Runtime Interface, or CRI. The CRI abstracts the internal workings of Kubernetes and provides a clean, documented interface for third-party container runtimes.

Kubernetes is phasing out support for Docker as a container runtime because Docker is considered bloated and does not natively support the CRI, requiring a shim instead. The most common replacement is containerd, which is a lightweight container runtime with strong community support. Other CRI implementations also exist.

In addition to the kubelet and kube-proxy, Kubernetes nodes run other essential management components. One key component is the Container Runtime Interface, or CRI, which is responsible for running and managing containers. Popular CRI implementations include containerd, which is also used in OpenShift. Another important component is the Container Network Interface, or CNI, which handles networking for Pods, including IP address allocation and routing. Different CNI plugins, such as Calico, Flannel, Cilium, and Weave Net, provide networking capabilities tailored to the needs of the cluster.

Containerd, in particular, is the container supervisor and runtime logic that was originally part of the Docker engine. It was donated to the Cloud Native Computing Foundation by Docker Incorporated and has significant community support.

The DNS

Every Kubernetes cluster includes an internal DNS service, which is vital for service discovery. The cluster’s DNS service has a static IP address that is hard-coded into every Pod. This ensures that every container and Pod can locate the DNS service and use it for discovery.

Service registration with DNS is automatic, so applications do not need to be coded with special logic to register themselves with Kubernetes service discovery. The cluster DNS is based on the open source CoreDNS project.

Control Plane Summary

Kubernetes control plane nodes are servers that run the cluster’s control plane services. These services are the brains of the cluster, where all control and scheduling decisions are made. The main control plane services include the API server, the cluster store, the scheduler, and specialized controllers.

The API server acts as the front end to the control plane. All instructions and communication pass through it. By default, it exposes a RESTful endpoint on port four hundred forty-three.

Packaging Applications

To run an application on a Kubernetes cluster, it must meet a few requirements. First, the application must be packaged as a container image. Second, the image must be wrapped as a container instance within a Pod. Third, the deployment must be defined using a declarative configuration manifest file.

The process works as follows. You write an application microservice in your preferred programming language. Then, you build it into a container image and store it in a registry. At this point, the application service is containerized. Next, you define a Kubernetes Pod to run the containerized application. At a high level, a Pod is simply a wrapper that allows a container to run on a Kubernetes cluster. Once you have defined the Pod, you are ready to deploy the application to Kubernetes.


While it is possible to run static Pods directly on a Kubernetes cluster, the preferred approach is to deploy all Pods using higher-level controllers. The most common of these controllers is the Deployment. Deployments provide important features such as scalability, self-healing, and rolling updates for stateless applications. You define Deployments using YAML manifest files, specifying details like the number of replicas to deploy and how updates should be performed. Once you have defined everything in the Deployment YAML file, you can use Kubernetes command line tools to submit it to the API server as the desired state of your application. Kubernetes then takes care of implementing this state.

Declarative model

The declarative model, along with the concept of desired state, is central to how Kubernetes operates. It is essential to understand these concepts. In Kubernetes, the declarative model works as follows.

First, you declare the desired state of an application or microservice in a manifest file. Next, you post this desired state to the API server. Kubernetes then stores this information in the cluster store as the application's desired state. After that, Kubernetes works to implement the target desired state within the cluster. Finally, a controller ensures that the observed state of the application does not drift from the desired state.

Manifest files are written in simple YAML format and describe what the application should look like. This is known as the desired state. The manifest includes details such as which container image to use, how many replicas to run, which network ports to listen on, and how updates should be performed. Once you have created the manifest, you submit it to the API server. The easiest way to do this is by using the kubectl command line utility. This tool sends the manifest to the control plane as an HTTPS POST request, typically on port four hundred forty-three.

Once the request is authenticated and authorized, Kubernetes inspects the manifest, determines which controller should handle it—such as the Deployments controller—and records the configuration in the cluster store as part of the overall desired state. After this, any required tasks are scheduled to the cluster nodes, where the kubelet coordinates the work of pulling images, starting containers, attaching to networks, and launching application processes.

Controllers then run as background reconciliation loops, constantly monitoring the state of the system. If the observed state ever deviates from the desired state, Kubernetes performs the necessary actions to reconcile the differences and bring the observed state back in line with the desired state.

It is important to understand that this approach is the opposite of the traditional imperative model. In the imperative model, you write long scripts filled with platform-specific commands to build and monitor resources. The declarative model is much simpler than managing complex scripts with many imperative commands. It also enables self-healing, scaling, and lends itself well to version control and self-documentation. The declarative model works by telling the cluster how things should look. If the actual state starts to differ, the appropriate controller detects the discrepancy and does the work needed to reconcile the situation.

For example, imagine you have an application with a desired state that includes ten replicas of a web front end Pod. If a node running two of those replicas fails, the observed state drops to eight replicas, but the desired state remains at ten. A controller will notice this difference, and Kubernetes will schedule two new replicas to bring the total back up to ten. The same process occurs if you intentionally scale the number of replicas up or down.

You can also change the container image you want to use, which is called a rollout. For instance, if your application is currently using version two point zero zero of an image, and you update the desired state to specify version two point zero one, the relevant controller will detect the change and update the cluster so that all ten replicas are running the new version.

To be clear, instead of writing a complex script to update every replica to the new version, you simply tell Kubernetes what you want, and Kubernetes handles the hard work for you.

Pods

In the VMware world, the atomic unit of scheduling is the virtual machine. In the Docker world, it is the container. In Kubernetes, the atomic unit is the Pod. While Kubernetes does run containerized applications, it requires that every container runs inside a Pod.

Pods are objects in the Kubernetes API, and it is standard practice to capitalize the first letter for clarity. You can think of Pods as specifications and rules for running containers. They define various container-specific rules and boundaries, which are used throughout the container lifecycle—starting, running, and terminating—by the container runtime, such as containerd.

Pods and Containers

The term Pod comes from the phrase "a pod of whales." In English, a group of whales is called a pod, and since the Docker logo is a whale, Kubernetes adopted this concept. The simplest model is to run a single container in each Pod, which is why the terms Pod and container are often used interchangeably. However, there are advanced use cases where multiple containers run within a single Pod. Powerful examples of multi-container Pods include service meshes, containers paired with tightly coupled log scrapers, or web containers supported by a helper container that pulls updated content.

The key point is that a Kubernetes Pod is a construct for running one or more containers. A Pod is an object defined declaratively in the Kubernetes state. Pods are not physical entities that run on the nodes; instead, they are used by the kubelet service or daemon to control containers.

Pod anatomy

At a high level, a Pod is a ring-fenced environment for running containers. Pods themselves do not actually run applications; applications always run inside containers. The Pod serves as a sandbox for one or more containers.

Pods isolate an area of the host operating system, build a network stack, create several kernel namespaces, and then run one or more containers. If you are running multiple containers in a Pod, they all share the same Pod environment. This includes the network stack, volumes, inter-process communication namespace, shared memory, and more. For example, all containers in the same Pod share the same IP address. If two containers in the same Pod need to communicate, they can use the Pod's localhost interface.

Multi-container Pods are ideal when you have tightly coupled containers that need to share memory and storage. However, if you do not need this level of coupling, it is better to place containers in their own Pods and connect them over the network. This approach keeps things clean, with each Pod dedicated to a single task, but it can result in a lot of potentially unencrypted network traffic. In such cases, you should seriously consider using a service mesh to secure traffic between Pods and application services.

Now, here is an interesting detail about how Pods work internally. Although Pods are configuration entities managed by the kubelet, the kubelet itself spawns something called pause containers. Each Pod is associated with a pause container, which acts as a namespace holder. The pause container is responsible for holding the network namespace and other resources that the Pod environment promises to its running containers. This is important in case all containers managed by a Pod die, or if a Pod temporarily has no running containers for any reason.

The statement that a Pod is not a container still holds true. The kubelet simply uses auxiliary structures to retain the resources and overall state associated with a Pod configuration object. This is mostly an implementation detail, and end users cannot interact directly with pause containers.

For example, imagine a Pod with two containers—a web server and a logging sidecar. The kubelet starts the pause container first. The pause container sets up the shared network and inter-process communication namespaces. The kubelet then starts the web server container and joins it to the pause container's namespace. Next, the kubelet starts the logging container and joins it to the same namespace. Now, both containers share the same networking and inter-process communication namespaces. They can communicate over localhost and use shared memory.

Pause containers simply reserve the Linux kernel namespaces, which are then shared between the actual running containers configured for the Pod. All resources shared between containers inside a Pod are actually bound to the namespace of the pause container for that Pod, which the kubelet has started.

Pods as unit of scaling

Pods are also the minimum unit of scheduling in Kubernetes. If you need to scale an application, you add or remove Pods. You do not scale by adding more containers to existing Pods. Multi-container Pods are only used in situations where different but complementary containers need to share resources.


You never scale an app by adding more of the same app containers to a single Pod. Multi-container Pods are not a way to scale an application. Instead, they are used for co-scheduling and co-locating containers that need to work closely together. For example, you might pair a web service with a logging service, or an in-memory data store, within the same Pod. If you need to scale your application, you add more Pods or remove them as needed. This process is called horizontal scaling.

Pods atomic operations

The deployment of a Pod is an atomic operation. This means a Pod is only considered ready for service when all its containers are up and running. The entire Pod either comes up and is put into service, or it does not and fails as a whole. A single Pod can only be scheduled to a single node. You cannot schedule a single Pod across multiple nodes. This rule also applies to multi-container Pods—all containers in the same Pod run on the same node.

Pod lifecycle

Pods are mortal. They are created, they live, and eventually, they die. If a Pod dies unexpectedly, you do not need to bring it back to life. Instead, Kubernetes starts a new Pod in its place. However, even though the new Pod may look, feel, and behave like the old one, it is not the same. It is a brand new Pod with a new ID and a new IP address. This has important implications for how you design your application. Do not design your app to be tightly coupled to a particular instance of a Pod. Instead, design it so that when Pods fail, a completely new one can appear somewhere else in the cluster and seamlessly take its place.

Pod immutability

Pods are also immutable. This means you do not change them once they are running. If you need to change or update a Pod, you replace it with a new Pod instance running the new configuration. When we talk about updating Pods, what we really mean is deleting the old one and replacing it with a new one that has the updated configuration. The immutable nature of Pods is a key aspect of cloud-native microservices design and patterns. It enforces the following practices: when updates are needed, replace all old Pods with new ones that have the updates; when failures occur, replace failed Pods with new, identical ones. To be clear, you never update a running Pod. You always replace it with a new Pod containing the updates. You also never log onto failed Pods and attempt fixes. Instead, you build fixes into an updated Pod and replace failed ones with the updated version.

Pods versus Nodes

It is vital to understand the difference between Pods and Nodes. While both are part of the Kubernetes infrastructure, their purposes are vastly different. Usually, a Pod corresponds to a single container instance, but that is not always the case. As mentioned earlier, a Pod can, in theory, run multiple containers of the same image or, more commonly, different images. These containers can share a single state, making integration between these services more robust and easier. In some situations, this is desirable.

Pods are not Nodes. Nodes are the computing environments that run the containers, the container runtimes, the kubelet, and any other component of the Kubernetes infrastructure. Nodes could be virtual machines, physical machines, embedded devices, or anything else that supports running the Kubernetes runtime. Pods, on the other hand, are a logical collection of containers that run on a Kubernetes Node.

Pod strategies

The atomic unit of scheduling on Kubernetes is the Pod. This simply means that applications deployed to Kubernetes are always managed by Pods. Why do we need Pods? Why not just run the container on the Kubernetes node directly? The short answer is that you cannot. Kubernetes does not allow containers to run directly on a cluster or a node; they always have to be wrapped in a Pod object. There are three main reasons why Pods exist: Pods augment containers, Pods assist in scheduling, and Pods enable resource sharing.

On the augmentation front, Pods enhance containers in several ways. They provide labels and annotations, restart policies, probes such as startup, readiness, and liveness checks, affinity and anti-affinity rules, termination control, security policies, and resource requests and limits.

To clarify, containers still technically run directly on the node, or more precisely, on the container runtime, which is running on the node. However, the container runtime and the containers themselves are managed and run by the Pods. Pods are not physically running the containers. Instead, Pods are Kubernetes objects or manifests. Through their Pod manifest specification, they instruct the kubelet, and the kubelet instructs the container runtime—such as containerd—on what to do and how to run the set of containers the Pods manage, what resources to allocate for them, and all the fine-grained control a container might require. This is necessary because there is no equivalent 'container' specification that can do this, so an intermediate control object like the Pod is needed.

Labels let you group Pods and associate them with other objects in powerful ways. Annotations let you add experimental features and integrations with third-party tools and services. Probes let you test the health and status of Pods, enabling advanced scheduling and updates. Affinity and anti-affinity rules give you control over where Pods run. Termination control lets you gracefully terminate Pods and the applications they run. Security policies let you enforce security features. Resource requests and limits let you specify minimum and maximum values for things like CPU, memory, and disk input and output.

Despite bringing so many features to the table, Pods are lightweight and add very little overhead. Pods also enable resource sharing. They provide an execution environment for one or more containers. This shared environment includes things such as a shared file system, network stack, memory, and file system volumes.

Pods deployed directly from the Pod manifest are called static Pods. Static Pods do not have advanced features such as self-healing, scaling, or rolling updates. This is because they are only monitored and managed by the local kubelet process, which is limited to attempting container and Pod restarts on the local node. If the node they are running on fails, there is no control plane process watching and capable of starting a new one on a different node.

Pods deployed via controllers have all the benefits of being monitored and managed by a highly available controller running on the control plane. The local kubelet on the node they are running on can still attempt local restarts, but if restart attempts fail or the node itself fails, the observing controller can start a replacement Pod on a different node.

It is vital to understand that Pods are mortal. When they die, they are gone. There is no fixing them or bringing them back from the dead. This firmly places them in the "cattle" category in the pets versus cattle paradigm. Pods are cattle, and when they die, they get replaced by another. This is why applications should always store state and data outside the Pod. You should not rely on individual Pods, as they are ephemeral and last for only a short time.

Pods are objects in the Nodes, or workers. You can think of a Pod as a room in a house. The house itself is the Node, the room is the Pod, and the people living in that room are the containers. The room, or Pod, does not run anything. It only manages the containers, providing them with shared state. It acts as a bridge or adapter between multiple containers. As already mentioned, one Pod object can manage multiple containers and can provide them with shared state, which is isolated from other Pods and containers.

Each Pod has its own IP address, allowing it to communicate with other Pods, even across several Nodes. The container interface plugin is responsible for assigning IP addresses and setting up networking. Pods on the same node communicate through a bridge, while Pods on different nodes communicate through routing rules set in the container network interface plugin.

Storage between containers in a given Pod is shared using volumes. This means that a given Pod mounts volumes from the host or node to all containers it is responsible for. These volumes are then used and shared only by the containers that this Pod governs.

The governing Pod also ensures that each configured container does not exceed the resources allocated for it. If a container does exceed its resources, the Pod will restart or kill the container.

Remember, Pods are not physical services running on the host. They are merely objects defined in the Kubernetes deployment configuration. These Pod objects are picked up by the kubelet, which is the service running on the Node. The kubelet is the active service that manages Pods and, by extension, the containers defined for these Pods. The Pods themselves are just configuration objects, which tell the kubelet how to manage a set of containers and what to do with them in the event of abnormal occurrences or if the desired state diverges from the actual state.

Pods deployment

Now, let's move on to how Pods are deployed.


The process of deploying pods in Kubernetes is straightforward. Pods are defined in files, and as mentioned earlier, pods are simply objects within the Kubernetes environment.

First, you define the pod in a YAML manifest file. Next, you post this YAML file to the Kubernetes REST API server. The server then authenticates your request. After authentication, the configuration file is validated. Once validated, the scheduler deploys the pod to a healthy node in the cluster. Finally, the local kubelet on that node monitors the pod.

When a pod is deployed through a controller, its configuration is added to the cluster store as part of the overall desired state that Kubernetes maintains. A controller will continuously monitor this state. The deployment process for a pod is atomic, meaning it either fully succeeds or fails—there is no scenario where a partially deployed pod is serving requests. Only after all of a pod’s resources are running and ready will it begin to handle requests.

Pod lifecycle

The lifecycle of a pod begins with the YAML object, which is sent to the API server. The pod then enters the pending phase. During this phase, it is scheduled to a healthy node with enough resources. The local kubelet instance on that node instructs the container runtime to pull all required images and start all containers. Once all containers are pulled and running, the pod enters the running phase.

If the pod is short-lived, as soon as all containers terminate successfully, the pod itself is terminated and enters the succeeded state. For long-running pods, they remain in the running phase indefinitely. Short-lived pods can run various types of applications. For example, web servers are intended to be long-lived and should stay in the running phase. If any containers in a long-lived pod fail, the local kubelet may attempt to restart them. This behavior depends on the container’s restart policy, which is defined in the pod object itself.

The restart policy options include Always, OnFailure, and Never. The default is Always, which is suitable for most long-lived pods. Other workloads, such as batch jobs, are designed to be short-lived and only run until a task is complete. Once all containers in a short-lived pod terminate, the pod’s status is set to successful. For these short-lived pods, the Never and OnFailure restart policies are appropriate.

Pod multi-container control

Multi-container pods are a powerful pattern and are widely used in real-world scenarios. At a high level, each container should have a single, clearly defined responsibility. For example, consider an application that pulls content from a repository and serves it as a web page. This application has two clear functions: pulling the content and serving the content. In this case, you should design two containers—one responsible for pulling the content and the other for serving the web page. This approach is known as separation of concerns.

Designing containers this way keeps each one small and simple, encourages reuse, and makes troubleshooting easier. However, there are scenarios where it makes sense to tightly couple two or more functions. For instance, in the example of pulling and serving content, the sync container (which pulls content) can place updates in a volume shared with the web container. For this to work, both containers need to run in the same pod so they can share the same storage from the pod’s execution environment.

Co-locating multiple containers in the same pod allows each container to have a single responsibility while working closely with others. Kubernetes offers several well-defined multi-container pod patterns, including the sidecar pattern, the adapter pattern, the ambassador pattern, and the init pattern.

Sidecar

The sidecar pattern is probably the most popular and generic multi-container pattern. It consists of a main application container and a sidecar container. The sidecar’s job is to augment or perform a secondary task for the main application container. In the earlier example, the sync container that pulls content from an external repository acts as the sidecar. An increasingly important use of the sidecar model is in service meshes. At a high level, service meshes inject sidecar containers into application pods, and these sidecars handle tasks like encrypting traffic and exposing telemetry and metrics.

Adapter

The adapter pattern is a specific variation of the sidecar pattern. In this case, the helper container takes non-standardized output from the main container and transforms it into a format required by an external system. For example, NGINX logs are not natively understood by Prometheus. A common approach is to add an adapter sidecar to the NGINX pod, which converts the logs into a format that Prometheus can accept.

Ambassador

The ambassador pattern is another variation of the sidecar pattern. Here, the helper container brokers connectivity to an external system. For example, the main application container can send its output to a port that the ambassador container is listening on. The ambassador container then handles the task of delivering this output to the external system.

Init

The init pattern is not a form of the sidecar pattern. Instead, it involves running a special init container that is guaranteed to start and complete before the main application container. As the name suggests, the init container’s job is to run tasks and initialize the environment for the main application container. For example, if the main application container needs permissions set, an external API to be up, or a remote repository cloned to a local volume, the init container can handle this preparation. The main application container will not start until the init container has completed its work.

Takeaways

It is important to note that while the ambassador and adapter patterns might seem similar, they serve different purposes. The adapter pattern is primarily for translating or normalizing data from one form to another. In contrast, the ambassador pattern is focused on handling communication between containers or services. It abstracts away the communication details. For example, Envoy is a sidecar mesh that abstracts database connection and communication details between a service and a database. It provides a common communications protocol that the container can use, without needing to know what is on the other side, as long as the other side understands the protocol. In practice, the other side might also be using the same pattern to receive the communication.

Deployments

Most of the time, you will deploy pods indirectly through higher-level controllers. Examples of these controllers include Deployments, DaemonSets, and StatefulSets. For instance, a Deployment is a higher-level Kubernetes object that wraps around a pod and adds features such as self-healing, scaling, zero-downtime rollouts, and versioned rollbacks. Behind the scenes, Deployments, DaemonSets, and StatefulSets are implemented as controllers that run as watch loops. These controllers constantly observe the cluster and the Kubernetes API server, ensuring that the observed state matches the desired state.

Services

Since pods can die and are managed by higher-level controllers, they are replaced when they fail. However, replacements come with entirely new IP addresses. This also happens during rollouts and scaling operations. Rollouts replace old pods with new ones that have new IPs. Scaling up adds new pods with new IP addresses, while scaling down removes existing pods. These events cause a lot of IP churn.

This presents a challenge. Imagine you have a microservice application with several pods performing video rendering. If other parts of the application rely on the rendering service, they cannot depend on the rendering pods always being available at the same address.

This is where services come into play. Services provide reliable networking for a set of pods. They are fully-fledged objects in the Kubernetes API, just like pods and deployments. A service has a front end consisting of a DNS name, an IP address, and a port. On the back end, it load balances traffic across a dynamic set of pods.

As pods come and go, the service observes these changes, automatically updates itself, and continues to provide a stable networking endpoint. The same applies when you scale the number of pods up or down. New pods are seamlessly added to the service and begin receiving traffic, while terminated pods are removed and no longer receive traffic.

In summary, the job of a service is to act as a stable network abstraction point. It provides TCP and UDP load balancing across a dynamic set or number of pods and containers.


As they operate at the TCP and UDP layer, these components do not possess application intelligence. This means they cannot provide application layer host and path routing. For that, you need an Ingress, which understands HTTP and provides host and path-based routing.

Services bring stable IP addresses and DNS names to the otherwise unstable world of Pods. They act as an abstraction layer, allowing other Services, Pods, or Containers to communicate without worrying about the fact that a target Pod can die or be replaced at any time.

Clusters

Namespaces are the native way to divide a single Kubernetes cluster into multiple virtual clusters. These are not the standard Linux kernel namespaces that are responsible for namespacing processes at the kernel level. Instead, Kubernetes namespaces divide the cluster into virtual clusters called Namespaces.

Namespaces partition a Kubernetes cluster and are designed as an easy way to apply quotas and policies to groups of objects. However, they are not designed for strong workload isolation. Most Kubernetes objects are deployed into a Namespace. These objects are said to be namespaced, and include common resources like Pods, Services, and Deployments.

If you do not explicitly define a target namespace when deploying a namespaced object, it will be deployed to the default namespace. Namespaces are a good way of sharing a single cluster among different departments and environments. For example, a single cluster might have Namespaces for development, testing, and quality assurance. Each one can have its own set of users and permissions, as well as unique resource quotas.

However, Namespaces are not good for isolating hostile workloads. This is because a compromised container or Pod in one namespace can still affect other namespaces. In other words, you should not run competitive or untrusted workloads together in the same cluster, even if they are in different namespaces.

Every Kubernetes cluster comes with a set of pre-created namespaces, which are virtual clusters. For example, you can use a command to show the list of all namespaces in the current cluster. This command will display namespaces such as kube-system, default, kube-public, and kube-node-lease.

The default namespace is where newly created objects go unless you specify otherwise. The kube-system namespace is where DNS, the metrics server, and other control plane components run. The kube-public namespace is for objects that need to be readable by anyone. Finally, the kube-node-lease namespace is used for node heartbeat and managing node leases.

Namespaces are first-class resources in the core version one API group. This means they are stable, well-understood, and have been around for a long time. It also means you can create and manage them imperatively with kubectl, or declaratively with YAML manifests.

For example, you can define a sample namespace that is not the default one by creating a YAML manifest. In this manifest, you specify the kind as Namespace, set the API version to v1, and provide metadata such as the name and labels for the namespace.

To apply this configuration to the cluster, you use a command that applies the YAML file. This creates the namespace in your cluster.

When you start using Namespaces, you may quickly realize it is painful to remember to add the namespace flag to all kubectl commands. A better way is to set your kubeconfig to automatically work with a particular namespace. There is a command that configures kubectl to run all future commands against a specific namespace, such as shield.

To deploy to a given Namespace, as already mentioned, most objects are always tied to a namespace. If you do not specify otherwise, the default namespace will be used when deploying objects. There are two different ways to deploy objects to a specific namespace: imperatively and declaratively. The imperative method requires you to add the namespace flag to the command. The declarative method specifies the namespace in the YAML manifest file.

For example, you can declaratively deploy a simple app to the shield namespace and test it. In the YAML manifest, you define a ServiceAccount, a Service, and a Pod, all with their metadata specifying the shield namespace.

Note the use of metadata. This is a common pattern in Kubernetes configuration manifests. The metadata field is not just for humans to read; it is often used to provide control flow to the Kubernetes cluster itself. Based on the metadata, the environment knows what to do with the object. The metadata provides context for the object it is defined for. In this case, we define that this particular object is created for this namespace. Other metadata keys also exist and are used, for example, to define the name of the object. That same name can be used to reference the object in other objects. The namespace itself, which we created above, was defined in the metadata section.

To deploy these resources, save the YAML manifest as a file, and then simply run a command to apply the file. To clean up the same resources, you can use a command to delete the file. The nice part here is that having all of this deployed in a declarative manner allows us to clean up the resources using the same declaration and file. There is no need for manual steps to delete each object, or to know in what order they need to be deleted, or to worry about stopping some of the resources that have been allocated by these Kubernetes objects.

Deployments

Kubernetes offers several controllers that augment Pods with important capabilities. The deployment controller is specifically designed for stateless applications. We will cover some other controllers later on as well.

Theory

There are two major pieces to deployments: the specification and the controller. The deployment specification is a declarative YAML object where you describe the desired state of a stateless application. You give that to Kubernetes, where the deployment controller implements and manages it. The controller aspect is highly available and operates as a background loop, reconciling observed state with desired state.

Deployment objects, and all of their features and attributes, are defined in the apps version one workloads API. Note that the Kubernetes API is architecturally divided into smaller subgroups to make it easier to manage and navigate. The apps subgroup is where Deployment, DaemonSets, StatefulSets, and other workload-related objects are defined. We sometimes call it the workloads API.

You start with a stateless app, package it as a container, then define it in a Pod template. At this point, you could run it on Kubernetes. However, static pods like this do not self-heal, do not scale, and do not allow for easy updates and rollbacks. For these reasons, you will almost always wrap them in a deployment object.

ReplicaSets

Behind the scenes, deployments rely heavily on another object called ReplicaSet. While it is usually recommended not to manage ReplicaSets directly, deployment controllers manage them. It is important to understand the role they play.

At a high level, containers provide a way to package apps and dependencies. Pods allow containers to run on Kubernetes and enable co-scheduling and other features. ReplicaSets manage pods and bring self-healing and scaling. Deployments manage ReplicaSets and add rollouts and rollbacks.

ReplicaSets are implemented as a controller running as a background reconciliation loop, checking that the right number of Pod replicas are present on the cluster. If there are not enough, it adds more. If there are too many, it terminates some. For example, assume a scenario where the desired state is ten replicas but only eight are present. It makes no difference if this is due to a failure or if it is because an autoscaler has increased the desired state from eight to ten. Either way, this is a red alert condition for Kubernetes, so it orders the control plane to bring two more replicas.

Note that ReplicaSets are owned by the Deployment object, meaning that they are subordinated to them, and their lifecycle is tied to the Deployment object's lifecycle. When deployment configuration is updated, new ReplicaSets are created for the deployment to which the update was made. This begins the deployment of the new Pods, while the old ReplicaSet pods are being wound down.

Pods


A deployment object in Kubernetes is designed to manage a single pod template. For example, if you have an application with both a front end web service and a back end catalog, each will require its own pod template. This means you will need two separate deployment objects—one to manage the front end pods, and another to manage the back end pods. However, a single deployment can manage multiple replicas of the same pod. For instance, your front end deployment might be responsible for five identical front end pod replicas.

Rollouts

Rolling updates with deployments are a core feature of Kubernetes, enabling zero downtime updates for stateless applications. These updates are powerful, but they require your microservice applications to be loosely coupled and to maintain backward and forward compatibility. These are key characteristics of modern, cloud-native microservice architectures.

All microservices in your application should be decoupled and communicate only through well-defined APIs. This approach allows any microservice to be updated independently, without worrying about the clients or other services that interact with it. Each service exposes a documented interface and hides its internal details.

Ensuring that releases are both backward and forward compatible means you can perform independent updates without needing to know which versions of clients are consuming the service.

With these principles in place, zero downtime rollouts work as follows. Imagine you are running five replicas of a stateless web front end. As long as all clients communicate via the API and compatibility is maintained, it does not matter which replica a client connects to. To perform a rollout, Kubernetes creates a new replica running the updated version and terminates one of the old replicas. At this point, you have four replicas on the old version and one on the new. This process repeats until all five replicas are running the new version. Because the application is stateless and there are always multiple replicas available, clients experience no downtime or service interruption.

There is a lot happening behind the scenes to make this possible. Each discrete microservice is designed as its own pod. For convenience, self-healing, scaling, rolling updates, and more, you wrap each pod in a higher-level controller such as a Deployment.

Each Deployment describes several key aspects: how many pod replicas to run, which container image to use, which network ports to expose, and details about how to perform rolling updates.

When you submit the Deployment YAML file to the API server, the pods are scheduled to healthy nodes. The Deployment and ReplicaSet controllers work together to maintain the desired state. The ReplicaSet controller continuously checks to ensure the actual state matches the desired state. The Deployment object sits above the ReplicaSet, governing its configuration and managing how rollouts are performed.

Suppose you discover a vulnerability and need to roll out a new image with a fix. You update the Deployment YAML file with the new image version and resubmit it to the API server. This updates the existing Deployment object, requesting the same number of pods but all running the new image.

To achieve this, Kubernetes creates a second ReplicaSet to manage the pods with the new image. You now have two ReplicaSets: the original one for the old image, and the new one for the updated image. As Kubernetes increases the number of pods in the new ReplicaSet, it decreases the number in the old ReplicaSet. The result is a smooth, incremental rollout with zero downtime.

You can repeat this process for future updates by continuing to update the same Deployment manifest file, which should be stored in a version control system.

Kubernetes determines how to correctly roll out a given pod by using a list of labels. The Deployment controller looks for pods with specific labels when performing rollout operations. The label selector is immutable—you cannot change it once the deployment is created.

For example, imagine you have a deployment with a container image at version one point zero, and you want to deploy new pods with version two point zero. You only need to change the image version in the Deployment YAML manifest file, updating the image tag from one to two. This triggers Kubernetes to create a new ReplicaSet for the new pods, which will scale up to the target number of replicas defined in your deployment manifest.

To apply the updated deployment, you use a command that applies the deployment manifest file. This updates the deployment with the new image version.

To monitor the status of the rollout, you can use a command that shows the rollout status of the deployment. Rollouts can also be paused using a specific command. For example, you can pause the deployment immediately after applying the new manifest.

When you describe the deployment, you receive information about its current state, including whether it is paused, the number of desired and updated replicas, and the status of the old and new ReplicaSets. The deployment annotation shows the current revision, and the ReplicaSet information indicates how many replicas are running for each version.

If you want to resume the deployment, you use a command to resume the rollout. The process will continue from where it was paused, scaling up the remaining replicas to the desired state. Once all pods are running the new version, the old ReplicaSet will be gradually decommissioned along with its pods.

In the next section, we will see how to roll back to a previous ReplicaSet, such as the one with version one point zero of the image.

Rollbacks

When older ReplicaSets are wound down and no longer manage pods, their configuration still exists in the cluster. This makes them a convenient option for reverting to previous versions. The rollback process is essentially the reverse of a rollout: you scale up the old ReplicaSet while scaling down the current one.

Imagine you have updated the deployment object to a new image version, from one to two. The deployment was updated, and the old version one pods were decommissioned by the ReplicaSet, with the new ones now in place. However, the old ReplicaSet remains active, provided your deployment configuration is set to retain at least two versions of deployment history using the revision history limit.

You can view the rollout history using a command that shows the deployment’s rollout history. Revision one represents the initial deployment with the one point zero image tag, and revision two is the rolling update to version two point zero. The old ReplicaSets remain active for the previous image version, allowing you to easily revert to them. This will commission a new set of pods with the original image version.

If you list the ReplicaSets, you should see at least two ReplicaSets associated with the deployment object. One will correspond to the old version, and the other to the new version, each showing the number of desired, current, and ready pods, as well as their age. This allows you to manage rollbacks efficiently and maintain control over your application’s deployment history.


From this output, we can see some useful information. The old ReplicaSet has no active pods. This ReplicaSet was deployed with the original image, version one point zero. The new ReplicaSet, on the other hand, is using the new image, version two point zero. 

What we need to do now is commission the old ReplicaSet again and decommission the new one. This process will create new pods using the original image and remove the pods running the newer image as necessary.

It’s important to note that rollback and update are, in many ways, similar operations. The process followed during a rollback is the same as during a rollout. The only real difference is from the perspective of the person performing the action. The underlying Kubernetes infrastructure does not distinguish between a rollout and a rollback—it simply applies one set of ReplicaSets and decommissions the other.

When you run the command to undo the deployment and roll back to revision one, Kubernetes begins the rollback process. This operation is not instant. The rollback must provision a new set of pods—technically, the old set—with the original image, version one point zero, and remove the pods running version two point zero. As we have already seen, this is a gradual process. Pods with the old version are brought up, and the new ones are removed step by step.

Labels

As we have already seen, Deployments and ReplicaSets use labels and selectors to find the pods they own. In earlier versions of Kubernetes, deployments could take over management of existing static pods if they had the same label. However, recent versions use a system-generated pod template hash label, so only pods created by the Deployment or ReplicaSet will be managed.

For example, imagine you already have five pods on a cluster with the label “app equals front-end.” Later, you create a deployment that requests ten pods with the same “app equals front-end” label. In older versions of Kubernetes, the system would notice there were already five pods with that label and only create five new ones. The Deployment and ReplicaSet would then manage all ten pods.

In contrast, newer versions of Kubernetes tag all pods created by a Deployment or ReplicaSet with a unique pod-template-hash label. This prevents higher-level controllers from seizing ownership of existing static pods.

For instance, when you describe a deployment and its associated ReplicaSet, you’ll see that the ReplicaSet uses a selector that includes both the app label and the pod-template-hash. When you list the pods and show their labels, you’ll see that each pod has both the app label and the unique pod-template-hash label. This demonstrates how the different levels of objects are linked together through the pod template hash, along with the label selector.

Skeleton

The basic structure of the Deployment object is presented below. It is crucial to understand that the Deployment object technically controls many aspects of the underlying process of managing pods. This includes creating and destroying ReplicaSets and other objects.

To be clear, the Deployment object is just that—an object. The actual management happens at the kubelet level, which reads these configurations and controls the actual state of the node in the cluster.

The deployment manifest defines the API version and the kind of object, which is Deployment. It provides metadata, such as the name of the deployment, which should be a valid DNS name. The spec section is where most of the configuration happens. Anything directly under spec relates to the deployment itself, while anything nested further down refers to the behavior of the deployment object.

Within the spec, the template section defines the pod template the deployment uses to create pod replicas. In this example, the pod template defines a single-container pod. The replicas field specifies how many pod replicas the deployment should create and manage.

The selector field lists the labels that pods must have for the deployment to manage them. Notice how the deployment selector matches the labels assigned to the pod.

The revisionHistoryLimit tells Kubernetes how many older versions of ReplicaSets to keep. Keeping more gives you more rollback options, but keeping too many can bloat the object, which can be a problem on large clusters with many software releases.

The progressDeadlineSeconds field tells Kubernetes how long to wait during a rollout for each new replica to come online. In the example, a five-minute deadline is set, meaning each new replica has five minutes to become ready before Kubernetes considers the rollout stalled. The clock is reset after each new replica comes up, so each step in the rollout gets its own five-minute window.

The strategy section tells the deployment controller how to update the pods when a rollout occurs. There are some important details here. The maxUnavailable field specifies that no more than one pod below the desired state is considered valid. For example, if two pods fail and you have only eight running, the kubelet will try to scale back up to ten. The maxSurge field means that you should not have more than one pod above the desired state. If the deployment overshoots and creates twelve pods, the additional pods will be scaled down to match the desired state.

To activate the deployment configuration, you apply the deployment manifest file. To get a brief description of the deployment, you use the get command. For full details, you use the describe command.

Services

We have already seen how pods are related to containers, and how they connect to Deployments. The core levels of abstraction in Kubernetes are as follows: containers, then pods, then ReplicaSets, and then Deployments. Each of these provides different capabilities. Containers provide a way to run images. Pods manage resources and namespace the containers. ReplicaSets govern how to scale pods and containers. Deployments provide self-healing and overall control over everything below.

There is a higher level of abstraction in the Kubernetes world, called Services. Services provide reliable networking for a set of unreliable pods managed by Deployments. Since pods and containers are effectively immutable and ephemeral—they can be created and destroyed without notice—we need a way to abstract away the large number of pods that might come and go, without having to think about that process at all.

When a pod fails, it gets replaced by a new one with a new IP address. Scaling up introduces new pods with new IP addresses. Scaling down removes pods. Rolling updates also replace existing pods with completely new ones, each with a new IP address. This creates a massive churn of IP addresses and demonstrates why you should never connect directly to any particular pod.

There are three fundamental things you need to know about Kubernetes Services. 

First, when talking about Services, we are referring to the Service object in Kubernetes that provides stable networking for pods. Just like pods, ReplicaSets, and Deployments, Services are defined through a manifest YAML file and posted to the API server.

Second, every Service gets its own stable IP address, its own stable DNS name, and its own stable port.

Third, Services use labels and selectors to dynamically select the pods to send traffic to.

Theory

With a Service in place, pods can scale up and down, they can fail, and they can be updated and rolled back. Clients will continue to access them without interruption. This is because the Service observes changes and updates its list of healthy pods, but it never changes its stable IP, DNS, or port.


Think of services in Kubernetes as having two main parts: a static front end and a dynamic back end. The front end is made up of the IP address, DNS name, and port. These never change. The back end, on the other hand, is a list of healthy Pods, and this list can change constantly as Pods are created or destroyed.

Labels

Services are loosely connected to Pods using labels and selectors. This is the same mechanism that connects Deployments to Pods, and it is a key part of Kubernetes’ flexibility. For a Service to send traffic to a Pod, that Pod must have every label the Service is selecting for. The Pod can also have extra labels that the Service does not care about. However, if a Service has multiple labels in its selector, all of them must match—a logical AND is used between them. This means you need to be careful when configuring selection labels for your Pods.

It’s important to understand that Services and Deployments are orthogonal in Kubernetes. Services do not manage Deployments. Instead, both Services and Deployments work with Pods, and they exist at the same level in the Kubernetes hierarchy, just above the Pod. Services do not control Deployments; they work alongside Deployment objects to manage Pods. Deployments are responsible for managing the deployment process, while Services are meant to manage traffic and abstract away the communication between Pods and the outside world.

Let’s look at two definitions—one for a Service and one for a Deployment.

The first definition describes a Service named hello-svc. It uses the NodePort type, exposing port 8080 on the Service and mapping it to port 30001 on each node. The Service selects Pods with the label app set to hello-world.

The second definition describes a Deployment named hello-deploy. It specifies that ten replicas should be running. The Deployment uses a selector to match Pods with the label app set to hello-world. It also defines a rolling update strategy, resource limits for memory and CPU, and specifies the container image to use.

Notice that both Deployments and Services must have correct selectors configured to match the labels on the Pods. Deployments create Pods using ReplicaSets, but Services do not manage Deployments. Instead, Services manage Pods directly, just like Deployments do through ReplicaSets. This might seem odd, since the Pod is defined only in the Deployment and does not exist as a standalone object in Kubernetes.

You might also notice that the Service uses a simpler selector definition than the Deployment. The Service selector only allows for simple label matching, with no support for advanced expressions. In contrast, the Deployment selector can use matchLabels for simple matching or matchExpressions for more advanced rules.

For example, a Deployment can use matchLabels to select Pods with a specific label, or it can use matchExpressions to select Pods where the app label is either my-app or test-app. Services, on the other hand, can only match on one or more labels using an AND condition.

Endpoints

As Pods are created and destroyed, the Service dynamically updates its list of healthy matching Pods. This is done using label selection and a special object called an Endpoint. Every time you create a Service, Kubernetes automatically creates an associated Endpoint object. The Endpoint object stores a dynamic list of healthy Pods that match the Service’s label selector.

Kubernetes constantly evaluates the Service’s label selector against the healthy Pods in the cluster. As new Pods that match the selector are created, they are added to the Endpoint object. When Pods are removed, they are taken out of the Endpoint object. This means the Endpoint object is always up to date.

When you send traffic to Pods via a Service, the cluster’s internal DNS resolves the Service name to an IP address. Traffic is sent to this stable IP address, and then routed to one of the Pods in the Endpoint list. Kubernetes-native applications can also query the Endpoint API directly, bypassing DNS, and use the Service’s IP address.

Accessing Services from inside the cluster

There are several types of Services in Kubernetes. The default type is called ClusterIP. A ClusterIP Service has a stable virtual IP address that is only accessible from inside the cluster. This IP address is programmed into the network fabric and is guaranteed to be stable for the life of the Service. When we say “programmed into the network fabric,” we mean that the network is aware of it, and you do not need to worry about the details.

The ClusterIP is registered with the Service name in the cluster’s internal DNS. All Pods in the cluster are pre-configured to use the cluster DNS service, so they can resolve Service names to ClusterIP addresses. If you create a new Service called magic-sandbox, Kubernetes will dynamically assign a stable ClusterIP. Both the name and the ClusterIP are automatically registered with the cluster’s DNS service, and these are guaranteed to be long-lived and stable.

As all Pods in the cluster send service discovery requests to the internal DNS, they can resolve magic-sandbox to the actual IP address based on the ClusterIP. Iptables or IPVS rules are distributed across the cluster to ensure that traffic sent to the ClusterIP is routed to matching Pods. The result is that if a Pod knows the name of a Service, it can resolve that name to a ClusterIP address and connect to the Pods behind it. This only works for Pods and other objects inside the cluster, since it requires access to the cluster’s DNS service. It does not work outside the cluster.

Let’s look at a simple example. Imagine a Service named one and another named two, both with active Pods. If you are in a Pod that is part of Service one and want to make a call to a Pod in Service two, you would use a curl request like this:

This command sends an HTTP request from a Pod in Service one to a Pod in Service two, using the fully qualified domain name of the Service. The domain name includes the Service name, the namespace (which is default unless specified otherwise), and the cluster’s DNS suffix, which is typically svc dot cluster dot local.

Breaking down the elements of this fully qualified domain name: “two” is the name of the Service you want to call. “default” is the namespace where Service two is deployed. If the Service is in a different namespace, you would replace “default” with the appropriate namespace name. “svc dot cluster dot local” is the default domain for Services in Kubernetes. This domain is defined in the CoreDNS or kube-dns configuration, which is typically stored in a ConfigMap named coredns or kube-dns in the kube-system namespace.

Internally, Services use a special auxiliary object called EndpointSlices to manage the endpoints for the Pods they are responsible for. These objects match Pods based on the selector labels.

Types

Now, let’s talk about accessing Services from outside the cluster. Kubernetes provides two types of Services for handling requests that originate from outside the cluster: NodePort and LoadBalancer.

NodePort Services build on top of the ClusterIP type and enable external access via a dedicated port on every cluster node. This port is called the NodePort. Since the default Service type is ClusterIP, which registers a DNS name, virtual IP, and port with the cluster’s DNS, NodePort Services add a NodePort that can be used to reach the Service from outside the cluster.

Below is an example of a NodePort Service definition. This configuration exposes a Service on a specific port across all nodes, allowing external traffic to reach the Service through that port.


NodePort Service Types

NodePort service types in Kubernetes are not particularly special. While they do allow you to access a service from outside the cluster, you must know the exact IP address of a node. When you call the service, you connect directly to a node, which means you always hit the same node in the cluster.

In the provided configuration, a service named hello-svc is defined as a NodePort. It listens on port 8080 inside the cluster and exposes port 30001 on each node. This setup allows pods within the cluster to access the service by its name, magic-sandbox, on port 8080. Clients outside the cluster can send traffic to any cluster node on port 30081. However, you need to know the node’s IP address to do this.

LoadBalancer Service Types

LoadBalancer service types make external access even easier by integrating with an internet-facing load balancer provided by your cloud platform. This gives you a high-performance, highly available public IP address or DNS name to access the service. You can even register friendly DNS names, so you don’t need to know the cluster node names or IP addresses.

LoadBalancer services are tightly coupled with cloud providers and may not work in on-premises environments without extra configuration, such as using MetalLB. The main benefit is that there is a load balancer in front of the cluster nodes. Unlike NodePort, you do not connect directly to a node’s IP. Instead, you use the load balancer’s IP or domain name, which then routes traffic to one of the underlying nodes. This approach removes the need to care about node IP addresses and provides built-in traffic balancing.

In the example configuration, a service named my-loadbalancer-service is defined as a LoadBalancer. The load balancer exposes port 80 and forwards traffic to port 8080 on the pods selected by the app label my-app.

Pod Load Balancing Within a Node

If a node has multiple pods that match a given service, Kubernetes will load balance traffic between those pods on the node. The most common algorithm used is round-robin, which means each pod is hit in sequence, one after the other.

It’s important to note that these types of services—NodePort and LoadBalancer—are strictly for accessing services from outside the cluster. Communication between pods using services is handled differently, which will be explored in more detail later.

Registration

Service registration is the process where an application posts its connection details to a service registry so other applications can find and consume it. In Kubernetes, service discovery works as follows:

Kubernetes uses its internal DNS as a service registry. All Kubernetes services automatically register their details with the DNS. This is possible because Kubernetes provides a well-known internal DNS service, often called the cluster DNS. Every pod in the cluster knows where to find it. The DNS service is implemented in the kube-system namespace as a set of pods managed by a deployment called coredns. These pods are fronted by a service called kube-dns. The underlying technology is CoreDNS, which runs as a native Kubernetes application.

The registration process is divided into two parts: the front end and the back end.

The front end involves the API server receiving the request to deploy the service. Steps here include registering the service IP in the cluster DNS, creating the Service object, and other auxiliary objects.

The back end involves work on the actual node that runs the pods selected by the service. This includes configuring networking rules, such as iptables or IPVS, on the nodes.

To see the actual pods running the coredns deployment, you can use a command that lists pods in the kube-system namespace with the label k8s-app equals kube-dns. This will show the coredns pods, their status, and age.

To list the deployment object managing these pods, you can use a command that lists deployments in the kube-system namespace with the same label. This will show the coredns deployment, how many pods are ready, and their age.

To see the service fronting these pods, you can use a command that lists services in the kube-system namespace with the label k8s-app equals kube-dns. This will show the kube-dns service, its type, cluster IP, and the ports it exposes.

Service Registration Process

Here’s how the service registration process works:

First, you post a new service manifest to the API server.

Next, the request is authenticated, authorized, and subject to admission policies.

Then, the service is allocated a stable virtual IP address called ClusterIP.

An endpoints object, known as EndpointSlices, is created to hold a list of healthy pods matching the service’s label selector.

The pod network is configured to handle traffic sent to the ClusterIP.

Finally, the service name and IP are registered with the cluster DNS service.

The last step is crucial. The cluster DNS is a Kubernetes-native application that implements a controller watching the API server for new Service objects. Whenever it observes a new service, it automatically creates DNS records mapping the service name to its ClusterIP. This means applications and services do not need to perform their own service registration—the cluster DNS handles it automatically.

It’s important to understand that the name registered in the DNS for the service is the value stored in its metadata.name property. This is why service names must be valid DNS names and should not include unusual characters. The ClusterIP is dynamically assigned by Kubernetes.

In the example configuration, a service is defined with a name that must be a valid DNS name. This is essential for proper registration and discovery.

Backend Registration and EndpointSlices

Once the service front end is registered and discoverable, the backend must be built so there is something to send traffic to. This involves maintaining a list of healthy pod IPs that the service will load balance traffic to. Every service has a label selector that determines which pods it will manage.

To help with backend operations, Kubernetes creates an EndpointSlices object for every service. The kubelet agent on each node watches the API server for new EndpointSlices objects. When it sees one, it creates local networking rules to redirect ClusterIP traffic to the appropriate pod IPs. In modern Kubernetes clusters, this is done using the Linux IP virtual server, or IPVS. Older versions used iptables.

At this point, the service is fully registered and ready to be used.

Discovery

Now, let’s discuss service discovery. Kubernetes uses its internal DNS service as the center of service discovery. Service names are automatically registered with the cluster DNS, and every pod and container is pre-configured to use this DNS service. This means every pod or container can resolve any service name to its ClusterIP and connect to the pods behind it.

There is an alternative form of service discovery using environment variables. Every pod receives a set of environment variables that resolve to services present on the cluster at the time the pod was created. However, this method is limited because pods cannot learn about new services added after their creation. This is a major reason why DNS is the preferred method for service discovery.

Let’s consider an example with two microservice applications on the same Kubernetes cluster, called enterprise and cerritos. The enterprise pods are behind a service named ent, and the cerritos pods are behind a service named cer. Each service is assigned a ClusterIP, which is registered with the cluster DNS service.

For service discovery to work, applications need to know two things. First, the name of the other application they want to connect to, which is the name of the service fronting the pods. Second, they need to know how to convert the service name to an IP address that corresponds to a pod managed by the service.


Apps developers are responsible for the first step, which is quite standard. They need to write their applications so that they reference the names of other apps they want to connect to. More specifically, they must use the names of the Kubernetes Services that front those remote apps. In other words, they refer to the Services, which are abstractions over the pods running the actual applications.

Kubernetes handles the next part. It automatically converts these service names into IP addresses using the cluster’s DNS system. This is achieved by configuring every container so it can find and use the cluster DNS to resolve service names to IPs. Kubernetes does this by populating each container’s “resolv dot conf” file with the IP address of the cluster DNS service, as well as any search domains that should be appended to unqualified names.

For example, a typical “resolv dot conf” file inside a container might look like this: it lists several search domains, such as “svc dot cluster dot local,” “cluster dot local,” and “default dot svc dot cluster dot local.” It also specifies a nameserver, for instance, “one hundred ninety-two dot one sixty-eight dot two hundred dot ten,” and includes an options line, such as “ndots colon five.”

Let’s take a brief tangent to explore the structure of the “resolv dot conf” file and some basic theory.

What is an unqualified name? An unqualified name is a short name, like “ent.” When a search domain is appended, it becomes a fully qualified domain name, or FQDN, such as “ent dot default dot svc dot cluster dot local.” In the earlier example, the container is set up to send DNS queries to the cluster DNS at “one hundred ninety-two dot one sixty-eight dot two hundred dot ten.” It also lists three search domains to append to unqualified names.

A fully qualified domain name, or FQDN, is a hostname that ends with a dot, for example, “host dot.” This signals to the DNS resolver that it is a complete domain name, and no search domains should be appended. In contrast, an unqualified hostname, such as “host” without a trailing dot, is considered incomplete. In this case, the DNS resolver will append the configured search domains in order to try to resolve the name.

Let’s break down the key directives in the “resolv dot conf” file.

The “nameserver” entry is straightforward. It points to the IP address of the cluster DNS service. This is essential for resolving service names. Without it, there would be no way to map a service name to an actual Cluster IP, and eventually to an Endpoint Slice and a physical pod IP address.

The “search” entry is a bit more complex. To understand it, recall that a fully qualified domain name is one that ends with a dot. Although the dot is often omitted in practice, according to the specification, only names ending with a dot are truly FQDNs. If you use a hostname like “ent” in your app configuration to refer to the enterprise service, this is not an FQDN. Therefore, according to the “resolv dot conf” file, the resolver will try to resolve the host as “ent dot svc dot cluster dot local,” then “ent dot cluster dot local,” and finally “ent dot default dot svc dot cluster dot local,” in that order. It will query the cluster DNS nameserver for each of these names, stopping when it gets a valid IP address.

If, on the other hand, your app configuration specifies the host as “ent dot” with a trailing dot, the search domains are ignored. The resolver treats this as a fully qualified domain name and tries to resolve “ent dot” directly using the cluster DNS service. This will typically fail, since “ent dot” is not a valid FQDN in the cluster.

The “options” directive allows you to configure additional resolver behavior. In the example, “options ndots colon five” sets a threshold for the number of dots in a hostname before the resolver treats it as a fully qualified domain name. If a hostname contains at least five dots, it is considered fully qualified and the search domains are not appended.

So, how does the “resolv dot conf” file actually get used to obtain the IP address needed to establish a TCP connection? Here’s the process:

First, your application calls a function like “get address info” with a hostname, for example, “example dot com.” A simple example is the “curl” command.

Next, the resolver library, such as glibc, reads the “resolv dot conf” file to determine which nameserver and search domains to use.

Then, the resolver library sends a DNS query to the specified nameserver.

The kernel handles the network communication, such as sending TCP packets to the DNS server.

Finally, the resolver library processes the DNS response and returns the Cluster IP address of the service to your application.

However, this is not the end of the story. Having the Cluster IP alone does not help much, since it is on a different network. If a pod for the “ent” service wants to talk to a pod for the “cer” service, it needs to know the IP of at least one pod from the “cer” service. How does that happen? This is where subnet masks, gateways, IPVS, and iptables come into play.

Let’s move on to the network magic.

Cluster IPs are on a special network called the service network, and there are no direct routes to it. This means that containers send all Cluster IP traffic to their default gateway. The default gateway is the device to which traffic is sent when there is no known route. Normally, the default gateway forwards traffic to another device with a larger routing table, hoping it will have a route to the destination network.

Each device on a network has a local routing table. This table tells the device how to route outgoing traffic, specifically which gateway to use when the destination network is remote. If the destination is local, the traffic is sent directly.

Here’s a brief overview of the communication process. Imagine your device has the IP address “one hundred ninety-two dot one sixty-eight dot one dot ten.” The local network is “one hundred ninety-two dot one sixty-eight dot zero dot zero.” The default gateway is “one hundred ninety-two dot one sixty-eight dot one dot one.” The destination is “www dot google dot com,” which, for example, resolves to “one hundred forty-two dot two fifty dot one ninety dot seventy-eight.”

A typical routing table might look like this: The first entry is for the local network, with a subnet mask of “two fifty-five dot two fifty-five dot two fifty-five dot zero,” a gateway of “zero dot zero dot zero dot zero,” and interface “eth zero.” The next entries are for remote networks, such as “one seventy-two dot sixteen dot zero dot zero” with a subnet mask of “two fifty-five dot two forty dot zero dot zero” and a gateway of “one hundred ninety-two dot one sixty-eight dot one dot two fifty-three.” Another entry might be for “ten dot zero dot zero dot zero” with a subnet mask of “two fifty-five dot zero dot zero dot zero” and a gateway of “one hundred ninety-two dot one sixty-eight dot one dot two fifty-four.” The last entry is the default route, with a destination network of “zero dot zero dot zero dot zero,” a subnet mask of “zero dot zero dot zero dot zero,” and a gateway of “one hundred ninety-two dot one sixty-eight dot one dot one.”

When your device wants to transmit traffic, it needs to know where to send it. This is determined using the subnet mask. The subnet mask helps identify which network a given IP belongs to. The device applies the subnet masks from the routing table, starting from the top. For example, it takes the destination IP, “one hundred forty-two dot two fifty dot one ninety dot seventy-eight,” and applies the first subnet mask, “two fifty-five dot two fifty-five dot two fifty-five dot zero.” The resulting network is “one hundred forty-two dot two fifty dot one ninety dot zero.” It checks if this matches the destination network in the table, which is “one hundred ninety-two dot one sixty-eight dot one dot zero.” There is no match, so it moves to the next entry.

If none of the destination networks match after applying the subnet masks, the device uses the last entry, which is the default gateway. The default gateway entry always matches, since its subnet mask is “zero dot zero dot zero dot zero.” This is usually the last entry in the table. When no other subnet mask matches, the device sends the traffic to the default gateway, in this case, “one hundred ninety-two dot one sixty-eight dot one dot one.”

Here’s how the process typically goes:

First, your device wants to send data to “one hundred forty-two dot two fifty dot one ninety dot seventy-eight.”

It checks its local routing table and sees that this IP is not in the local network, which is “one hundred ninety-two dot one sixty-eight dot one dot x.”

The subnet masking and routing table mapping indicate, “Send this to the default gateway, since no other match was found in the routing table.” The default gateway is “one hundred ninety-two dot one sixty-eight dot one dot one.”

The default gateway device receives the data, checks its own routing table, and forwards it to the internet or another gateway device.

The response comes back to the router, which sends it to your device.

The key takeaway here is that subnet masking is a process that involves matching pairs of values from the routing table. When an IP is masked against a subnet mask, it produces a destination network address. That address must match the destination network in the routing table for the subnet masking to be considered a match. If there is no match, the process moves to the next entry in the IP routing table.


The container’s default gateway sends outgoing traffic to the node it is running on. However, the node itself does not have a direct route to the service network either. As a result, it forwards the traffic to its own default gateway. This process causes the traffic to be handled by the node’s kernel, and this is where the key networking logic takes place.

Every Kubernetes node runs a system service called kube-proxy. At a high level, kube-proxy is responsible for capturing traffic that is destined for a ClusterIP, and redirecting it to the IP addresses of pods that match the service’s label selector. Kube-proxy runs on each node as a Kubernetes-native application. It acts as a controller, watching the API server for new Service and Endpoint objects. In practical terms, this means it observes when pods are created or destroyed, even on other nodes.

When kube-proxy detects these changes, it creates local IPVS rules. These rules instruct the node on how to intercept traffic meant for a service’s ClusterIP, and how to proxy that traffic to the appropriate pods managed by the service and endpoint objects. This setup ensures that whenever a node’s kernel processes traffic headed for an address on the service network, a trap is triggered. The traffic is then redirected to the IP address of a healthy pod that matches the service’s label selector.

In effect, each node becomes its own load balancer for traffic between other nodes and the pods running on them. There is no separate, intermediate, physical mediator service network responsible for this traffic. Instead, the network model between pods is flat. Nodes in a cluster communicate directly with each other, and the IP mapping is handled by kube-proxy running on the node. Kube-proxy dynamically configures either iptables or, in newer versions of Kubernetes, IPVS rules.

Originally, Kubernetes used iptables to perform this traffic trapping and load balancing. However, starting with Kubernetes version one point eleven, IPVS replaced iptables for this purpose. IPVS is a high-performance, kernel-based layer four load balancer that scales better than iptables and provides improved load balancing. The previous iptables-based implementation was not ideal, since iptables is primarily designed for firewall configurations, not for load balancing traffic.

The key takeaway from all of this is that the actual load balancing between pod-to-pod communication—specifically, between pods on different nodes—happens on the nodes themselves. More precisely, it is handled by IPVS, which is the kernel-level load balancer implementation in Linux.

Network Traffic

Let’s summarize the process of how network communication works between pods on different nodes.

First, the application queries DNS with the service name to obtain the stable ClusterIP address of the service.

Next, it receives the ClusterIP address.

Then, it sends traffic to the ClusterIP address.

Since there is no direct route, the traffic is sent to the container’s default gateway.

The gateway forwards the traffic to the host node.

The node, also lacking a direct route, sends the traffic to its own default gateway.

At this point, the node’s kernel processes the traffic.

The IPVS rule intercepts the traffic.

Finally, the destination IP is rewritten to the IP address of a matching pod.

To illustrate, let’s use the example of the enterprise and cerritos applications. Suppose the enterprise app needs to send traffic to the cerritos app. First, it needs the hostname of the service running the cerritos app. This would be “cer,” as defined in the service manifest and in the enterprise app’s configuration, set by the developers.

An instance of the enterprise app, running in a container on a node, tries to send traffic to the cer service. However, networks operate with numbers, not names. So, the container hosting the enterprise app sends the name “cer” to the pre-configured DNS service, as specified in the nameserver entry of the etc slash resolv dot conf file. The DNS service resolves the name “cer” to a stable ClusterIP address, and the enterprise app sends its traffic to that address.

However, ClusterIP addresses exist on a special service network. They are not directly accessible from the nodes, and the container does not have a route to them. Therefore, the traffic is sent to the default gateway, which forwards it to the host node. The node, also lacking a route, sends it to its own default gateway. Along the way, the node’s kernel processes and intercepts the request.

By this point, the kube-proxy service running on the node has already been monitoring for new endpoints—that is, pods—on the cluster, and has configured the necessary IPVS rules. It knows all the pods and their IPs that match the target service label. The kernel uses these rules to trigger a trap, and the request is redirected to the IP address of a pod that matches the service label selector.

The IPVS implementation in the kernel acts as a load balancer. This means that subsequent requests to the same service will be directed to other pods, so the mapping will resolve to different pod IPs, possibly using a round-robin approach, depending on how IPVS is configured. The specific load balancing method is not critical for this explanation.

Namespaces

Every cluster has an address space, and Kubernetes namespaces partition this space. Cluster address spaces are based on a DNS domain, typically called the cluster domain. The domain name is usually “cluster dot local,” and objects have unique names within it. For example, a service called “ent” will have the fully qualified domain name “ent dot default dot svc dot cluster dot local.” The format is: object name, dot, namespace, dot, svc, dot, cluster, dot, local.

Namespaces allow you to partition the address space below the cluster domain level. For example, creating namespaces called “dev” and “acc” gives you two new address spaces. In the dev namespace, the format would be object name dot dev dot svc dot cluster dot local. In the acc namespace, it would be object name dot acc dot svc dot cluster dot local.

Object names must be unique within a namespace, but not across namespaces. For example, you cannot have two services with the same name in the same namespace, but you can have services with the same name in different namespaces. This is useful for parallel development and production configurations.

Objects can connect to services in the local namespace using short names, such as simply specifying the object name. This is possible thanks to service discovery, the pre-configured etc slash resolv dot conf file, and the networking mechanisms already discussed.

Imagine you have two services, each in a different namespace, with different names: svc1 in the dev namespace, and svc2 in the acc namespace. You want to access them from a pod called svc3 in a third namespace, the default one.

Here’s an interesting detail that demonstrates how namespaces function as domain name context separators. Suppose the resolv dot conf file in your container contains several search domains, including svc dot cluster dot local, cluster dot local, default dot svc dot cluster dot local, dev dot svc dot cluster dot local, and acc dot svc dot cluster dot local, along with a nameserver entry.

With this configuration, you can use the short names svc1 and svc2 without providing their fully qualified domain names. Why is this possible? The resolv dot conf file handles the search process. It takes the search configuration and attempts to resolve the host names by sending them to the DNS cluster server in order.

For example, it will try svc1 dot svc dot cluster dot local—no match, since there is no such service in the unnamed namespace. Next, it tries svc1 dot svc dot default dot svc dot cluster dot local—again, no match in the default namespace. Then, it tries svc1 dot dev dot svc dot cluster dot local, and finds a match, since svc1 is deployed in the dev namespace. The DNS resolves svc1 dot dev dot svc dot cluster dot local and returns the IP address. The rest of the process is handled by the IPVS rules and the kernel, as previously described.

It’s important to note that this is not something you should—or even can—do manually, since the etc slash resolv dot conf file is not mutable. It is maintained and managed by the Kubernetes runtime and environment. Even if you were to change it manually, the change would only apply to a single container instance and would not persist across pod or container replication.

The example above is meant to illustrate how the different networking concepts we’ve discussed are interconnected. It shows the flow from the moment an application makes a request using an unqualified domain name, down to the actual IP address and traffic routing to the target pod.

Skeleton

Now, let’s look at the general structure of a service object, as defined in a YAML manifest file. There are several important properties to understand, which determine how the service integrates with the running pods managed by a deployment object.

The manifest specifies the API version at the top, indicating which version of the Kubernetes API to use.

It defines the kind of object being created, in this case, a Service.

The metadata section includes the name of the service and any labels, such as a label indicating the chapter as “services.”

The spec section defines the type of service, which in this example is NodePort. It also lists the ports configuration, specifying the port exposed by the service, the node port, the target port on the pods, and the protocol, which is TCP.

Finally, the selector field specifies which pods the service should target, using labels to match the appropriate pods.

In summary, this YAML manifest creates a NodePort service named svc-test, which exposes port eight thousand eighty on the service, maps it to node port thirty thousand one, and forwards traffic to target port nine thousand ninety on the selected pods. The service uses the TCP protocol and selects pods with the label chapter set to services.


Let’s walk through the key concepts and commands related to Kubernetes Services and Ingress, focusing on how they work and why they matter.

First, let’s talk about the structure of a Kubernetes Service manifest.

The metadata section gives the Service a name. This name must be a valid DNS name, which means it can include alphanumeric characters, dots, and dashes. Avoid using any unusual or exotic characters.

Next is the spec section. This is where most of the configuration happens. Anything directly under spec relates to the Service itself, while anything nested further down describes the specific behavior of the Service object.

Within spec, the type field determines how the Service is exposed. In this example, it’s set to NodePort, rather than the default ClusterIP. This means the Service will be accessible on a specific port across all nodes in the cluster.

The port field specifies the port on which the Service listens.

The targetPort field indicates the port on which the application inside the container is actually listening.

The nodePort field defines the cluster-wide port that can be accessed from outside the cluster.

The protocol field, by default, uses TCP. However, you can also use UDP or other protocols, depending on your application’s needs.

To deploy the Service manifest file, you use a command that applies the configuration from a file named svc.yml. This sets up the Service in your cluster.

To see a list of all Services, you run a command that retrieves and displays them.

To check the details of a specific Service, you use a command that fetches information about that Service by name. The output will show details such as the Service name, type, cluster IP, external IP, ports, and age.

When a Service is created, Kubernetes also creates associated resources called Endpoints. These Endpoints hold information about the active, or alive, pods and their actual IP addresses. This information is used to route traffic to the correct pods when the Service’s hostname is accessed.

To get the Endpoint objects tied to a Service, you use a command that lists endpoint slices. This will show the name, address type, ports, and the IP addresses of the pods that are part of the Service.

To get more details about a specific endpoint object, you use a command that describes the endpoint slice by name. The output will include the name, namespace, labels, annotations, address type, ports, and a list of endpoints. Each endpoint entry provides the pod’s IP address, readiness status, and other metadata.

Pay attention to the Endpoints section in the output. It lists all the pods and their IP addresses, along with status information about each pod. Just as ReplicaSets are helper objects for Deployments, Endpoints are helper objects for Services.

If you want to change the Service type from NodePort to LoadBalancer, you simply update the configuration to set the type to LoadBalancer and remove the nodePort field. The rest of the setup is handled automatically by your cloud provider. Kubernetes will interface with the provider’s internal load balancer and configure everything needed to make the Service accessible over the load balancer’s hostname, which is managed by the cloud provider.

If you’re running Kubernetes on-premises, you’ll need to set up the load balancer yourself. For example, you might use MetalLB, which integrates natively with Kubernetes.

To monitor the state of the Service and see when the external IP is assigned, you can use a command that continuously watches the Service status. The external IP column will show the public address assigned by your cloud provider. Sometimes, this might be a DNS name instead of an IP address, and it may take a minute or two for the setup to complete.

It’s important to note that most cloud providers create a separate load balancer instance for each Service configured as a LoadBalancer. This provides isolation and encapsulation for each Service, but it also introduces some drawbacks. For example, if you have many internet-facing applications, you’ll need a separate cloud load balancer for each one, which can be expensive and may hit provider limits.

This brings us to the next topic: Ingress.

Ingress

Ingress is designed to allow access to multiple web applications through a single LoadBalancer Service. Before diving into Ingress, it’s helpful to have a working knowledge of Kubernetes Services.

We’ve already seen how Service objects provide stable networking for pods and how you can expose applications to external consumers using NodePort and LoadBalancer Services. However, both approaches have limitations.

NodePort Services only work on high port numbers, typically between thirty thousand and thirty-two thousand seven hundred sixty-seven. They also require you to know the node names or IP addresses.

LoadBalancer Services solve this by providing a direct mapping between an internal Service and a cloud load balancer. However, this means that if you have twenty-five internet-facing applications, you’ll need twenty-five cloud load balancers. Not only is this expensive, but cloud providers may also limit the number of load balancer instances you can provision, regardless of your budget.

Ingress addresses these issues by exposing multiple Services through a single cloud load balancer. It creates a LoadBalancer Service on port eighty or four hundred forty-three, and uses host-based and path-based routing to direct traffic to the correct backend Service.

Theory

Ingress is a stable resource in the Kubernetes API. It became generally available in Kubernetes version one point nineteen, after spending more than fifteen releases in beta. During its time in alpha and beta, service meshes became popular, and there is some overlap in functionality. If you plan to use a service mesh, you may not need Ingress.

Ingress is defined in the networking dot io API subgroup as a version one object. It is based on two main constructs: a controller, which runs in a reconciliation loop to manage state, and an object specification, which is a well-defined and versioned manifest.

The object specification defines rules that govern traffic routing, while the controller implements these rules. Unlike other API resources such as Deployments and ReplicaSets, Kubernetes clusters do not ship with a built-in Ingress controller. You usually have to install your own. However, some hosted Kubernetes clusters, like Google Kubernetes Engine, do include one by default.

Once you have an Ingress controller installed, you can deploy Ingress objects with rules that determine how traffic is routed. Ingress operates at layer seven of the OSI model, also known as the application layer. This means it can inspect HTTPS headers and forward traffic based on hostnames and paths.

For example, you can configure host-based and path-based routing so that different hostnames or URL paths are directed to different backend Services. Imagine two hostnames, shield dot mcu dot com and hydra dot mcu dot com, both pointing to the same load balancer. The Ingress object uses the hostnames in the HTTPS headers to route traffic to the appropriate backend Service. This is known as HTTP host-based routing, and the same pattern applies to path-based routing.

For this to work, DNS name resolution must point the appropriate hostnames to the public endpoint of the Ingress load balancer.

A quick side note: The OSI model is the reference model for modern networking. It consists of seven layers, numbered from one to seven. The lowest layer deals with hardware and signaling, the middle layers handle reliability through acknowledgments and retries, and the higher layers add awareness of user applications, such as HTTPS services. Ingress operates at layer seven, the application layer, and implements HTTP intelligence.

In summary, Ingress exposes multiple ClusterIP Services through a single cloud load balancer. You create and deploy Ingress objects, which contain rules for routing traffic to backend Services. The Ingress controller, which you typically install yourself, uses hostnames and paths to make intelligent routing decisions.


Let’s start with a high-level overview of what’s happening with an Ingress controller in Kubernetes.

At a broad level, the Ingress controller is unique because it is actually exposed as a Service object within the Kubernetes environment. This is different from most other controllers, which are not exposed in this way. The Ingress controller is typically exposed to the public internet through a Service of type LoadBalancer. This Service is backed by pods running an image such as NGINX, which is responsible for the actual, on-demand routing defined in the Ingress object.

There are several popular implementations of Ingress controllers. Among the most widely used are NGINX, HAProxy, AWS Application Load Balancer, and Traefik. The Ingress controller is usually deployed as a Kubernetes Deployment or as a DaemonSet, and it runs as a pod within the cluster.

Let’s break down the main components and flow:

First, the Ingress Controller Deployment. The Ingress controller is deployed as one or more pods in the cluster. It listens for incoming traffic and routes it based on the rules defined in Ingress resources.

Next, the Service for the Ingress Controller. A Service is created to expose the Ingress controller to external traffic. The type of this Service can vary. It might be a LoadBalancer, which is common for cloud providers that support external load balancers. It could also be a NodePort, which exposes the Ingress controller on specific ports of the cluster nodes. Less commonly, it might be a ClusterIP, which is used for internal-only access.

Then, there are the Ingress Rules. The Ingress resource defines rules for routing traffic to backend services. The Ingress controller reads these rules and configures itself—along with its running pods, such as NGINX or Traefik—to route traffic accordingly.

What makes the Ingress controller unique compared to other controllers?

First, it is backed by a container image. The Ingress controller is implemented as a custom application, such as NGINX or Traefik, running inside a container. This is different from most other Kubernetes controllers, which are part of the Kubernetes control plane and are not exposed to external traffic.

Second, it is exposed to the internet. The Ingress controller is typically made accessible via a Service, such as a LoadBalancer or NodePort, to handle external HTTP and HTTPS traffic. This means it is directly reachable from outside the cluster.

Third, it interfaces with external traffic. Unlike other Kubernetes controllers, the Ingress controller interacts directly with external clients, such as web browsers or APIs, to route traffic to backend services.

To summarize the core responsibilities of the Ingress controller: it is essentially a combination of a Service and pods running a reverse proxy, such as NGINX, Traefik, or HAProxy. Its main tasks are to watch for Ingress resources, configure the reverse proxy based on the rules defined in those resources, and handle traffic routing. The reverse proxy then routes incoming traffic to the correct backend services according to the configured rules.

Now, let’s talk about network traffic.

When external clients—such as browsers or other consumers—send traffic into the cluster, that traffic is first directed at the load balancer, which is running the NGINX container in this example. As we’ve discussed, the Ingress controller is implemented using Kubernetes Services and pods, which use the preferred load balancer image.

Here’s an important detail: within the cluster, pod-to-pod communication typically happens with the help of IPVS or iptables, depending on the configuration. The crucial point is that the Ingress controller pods also use this same mechanism. One might assume that the load balancing is performed solely by the proxy, but in reality, the actual traffic is load balanced by the IPVS or iptables configuration. The load balancer, such as NGINX, is responsible for creating the routing configurations and performing the actual routing.

The Ingress object defines the routing and mapping, specifying which route should be mapped to which service. This configuration is passed directly to the underlying load balancer—in this case, NGINX.

Let’s walk through the high-level steps that a client request follows to reach a pod inside the cluster:

First, the client sends an HTTP request to the Kubernetes cluster, using the cluster’s host name.

Second, the load balancing Ingress controller is deployed as a Service of type LoadBalancer.

Third, the Ingress controller, such as NGINX, receives the request and matches it against its configured routing rules, which are defined in the Ingress object.

Fourth, the Ingress controller forwards the request to the correct Kubernetes Service.

Fifth, CoreDNS resolves the name of that Service and returns the virtual ClusterIP address.

Sixth, kube-proxy has already configured the IPVS or iptables rules for that Service, since it runs in a watch loop.

Seventh, the kernel sees the request from the Ingress controller pods and matches the virtual ClusterIP of the Service to the actual physical pod IP addresses.

Finally, the traffic is sent directly to one of these real pod IP addresses, and a pod for the target Service receives the request.

If you look at the flow from step five onward, you’ll notice that these steps are identical to the process for pod-to-pod communication within the cluster. This is because, as we’ve established, the Ingress controller and the Ingress object are special types of objects, but at their core, they are just like any other Kubernetes Service and pod.

A key point to consider here is namespaces. If the Ingress object manifest specifies a namespace, the Ingress controller will create routing mappings for that namespace only. This means that only Services from that namespace will be targeted. In practice, this is achieved by configuring the resolv.conf file with a search rule to look only in the target namespace. As a result, it can resolve ClusterIP addresses only for hosts within that namespace.

Now, let’s discuss how this ties into the cluster and the public internet. How do we access a Service from the public internet on a given cluster?

As we’ve seen, the Ingress controller is deployed as either a NodePort or LoadBalancer Service, with LoadBalancer being the more common choice. A Kubernetes cluster itself is not directly exposed to the public internet, and certainly not through a single IP address. Instead, individual Services are exposed, and each Service can have its own external IP or hostname.

Here’s how the process works:

When the NGINX controller is created, it is used to create a LoadBalancer Service. The cloud provider then provisions a load balancer, such as an Elastic Load Balancer on AWS or a Load Balancer on Google Cloud Platform. This load balancer receives an external IP address or hostname, which you can use to access the Service. For example, if the load balancer’s external IP is two hundred three dot zero dot one hundred thirteen dot ten, you can access that IP address directly, and your request will reach the exposed load balancer Service—in other words, the Ingress controller Service and its pods.

Let’s consider an example involving multiple users on a cloud platform. Typically, not every user receives their own cluster, unless we’re talking about enterprise customers or high-value users. Instead, the cluster is shared among users, but each user’s resources are namespaced.

In a shared Kubernetes cluster, namespaces are the primary mechanism for isolating resources between users or teams. Namespaces provide a logical boundary for resources, ensuring that objects created by one user or team do not interfere with those created by another.

Here’s how it might look in practice:

User A creates a Kubernetes cluster and deploys an Ingress controller. The Ingress controller is exposed with an external IP address of two hundred three dot zero dot one hundred thirteen dot ten. User A configures DNS to point myapp.com to that IP address.

User B creates a separate Kubernetes cluster and deploys an Ingress controller. This controller is exposed with an external IP address of two hundred three dot zero dot one hundred thirteen dot twenty. User B configures DNS to point myapi.com to that IP address.

When a client accesses myapp.com, DNS resolves it to two hundred three dot zero dot one hundred thirteen dot ten, and the request is routed to User A’s cluster. When a client accesses myapi.com, DNS resolves it to two hundred three dot zero dot one hundred thirteen dot twenty, and the request is routed to User B’s cluster.

This is the high-level skeleton of how Ingress controllers work in Kubernetes, how they are exposed, how they route traffic, and how namespaces and external access are managed.


The skeleton of the ingress manifest file is relatively simple. However, there are a few important details to keep in mind. First, like all other Kubernetes objects, ingress resources can be namespaced. The service mapping defined in the ingress will only apply to services within that specific namespace. This means you must ensure that the service rules and the namespace match, and that the namespace already contains a service with the specified name.

For example, consider a manifest that defines an ingress object named “example-ingress” in the namespace “app-namespace.” The rules specify that traffic to “example.com” should be routed to a service called “app-service” on port eighty. This setup ensures that only services within the “app-namespace” are targeted, so it’s essential that the “app-service” exists in that namespace before applying the ingress.

Next, let’s look at an example where the ingress controller is configured to use TLS in what’s known as “pass through” mode. In this configuration, the ingress controller acts as a simple forwarder. It does not decrypt or inspect the traffic, so it cannot perform sub-path routing for services. Instead, it relies solely on the Server Name Indication, or SNI, to resolve the hostname.

In this scenario, the manifest defines an ingress object named “passthrough-ingress.” It includes an annotation to enable SSL passthrough for NGINX. The specification lists two hosts, “example.com” and “api.example.com,” under the TLS section. The rules direct traffic for each host to the appropriate backend service on port four hundred forty-three. This setup ensures that encrypted traffic is forwarded directly to the backend services without being decrypted by the ingress controller.

Let’s take a brief detour to explain what SNI, or Server Name Indication, actually is. SNI is an extension of the TLS protocol that allows a client to specify the hostname it wants to connect to during the initial handshake. This is crucial for servers that host multiple websites or services on a single IP address, such as Kubernetes clusters and cloud platform providers. When encrypted traffic cannot be decrypted by the ingress controller, SNI allows the controller to determine at least the intended hostname. This is why route or path-based routing is not possible in passthrough mode—because the relevant information is part of the HTTP headers, which remain encrypted. Without the SNI extension, even the hostname would be inaccessible.

Here’s how the SNI process works in three steps. First, when a client initiates a TLS connection, it sends a “Client Hello” message that includes the SNI extension, specifying the hostname it wants to access, such as “example.com.” Second, the server uses the SNI information to select the correct SSL or TLS certificate for the requested hostname. Third, the server responds with the appropriate certificate, and the TLS handshake completes as usual, securing the connection.

Now, let’s look at another example that uses a re-encrypt approach. In this case, the ingress controller terminates the TLS connection at its own pods, and then re-encrypts the traffic as it forwards it to the target services and their pods. This process is often called TLS edge termination. The certificate used to re-encrypt the traffic is known as the edge certificate.

In this example, the manifest defines an ingress object named “example-ingress” with an annotation to enable SSL redirection for NGINX. The TLS section specifies the host “example.com” and references a secret named “example-tls,” which contains the certificate to be used for re-encrypting the traffic to the pods. The rules section defines two paths: “slash app” and “slash api.” Requests to “slash app” are routed to the “app-service” on port eighty, while requests to “slash api” are routed to the “api-service” on port eighty. This setup allows the ingress controller to decrypt incoming traffic, inspect the request paths, and then re-encrypt the traffic before sending it to the appropriate backend service.

Storage

Storage is critical to most real-world production applications. Fortunately, Kubernetes has a mature and feature-rich storage subsystem called the persistent volume subsystem.

Kubernetes supports many storage backends, each requiring slightly different configuration. The examples provided here are designed to work with Google Kubernetes Engine clusters, but the underlying principles and theory apply to all types of Kubernetes environments.

The big picture

Kubernetes supports a wide range of storage providers, including block, file, and object storage from various external systems. These systems can be cloud-based or located in on-premise data centers. Regardless of the storage type or its origin, when it is exposed in Kubernetes, it is referred to as a volume. For example, Azure File resources and block devices from HPE are both surfaced as volumes in Kubernetes.

Modern volumes are typically based on the Container Storage Interface, or CSI. This is an open standard designed to provide a consistent storage interface for container orchestrators like Kubernetes. Before CSI, all storage plugins were implemented as part of the main Kubernetes codebase. This meant that plugins had to be open source, and all updates or bug fixes were tied to the main Kubernetes release cycle. This approach was challenging for both developers and maintainers.

With the introduction of CSI, storage vendors no longer need to open source their code. They can develop and maintain their plugins independently of the Kubernetes release cycle.

The Storage Providers

Kubernetes can use storage from a wide range of external systems. These include native cloud services such as Amazon Web Services Elastic Block Store and Azure File, as well as traditional on-premise storage arrays that provide NFS volumes and similar options. The key takeaway is that Kubernetes can leverage storage from many external systems, including enterprise-grade solutions from major data management companies.

There are some obvious restrictions. For example, you cannot use AWS storage services if your Kubernetes cluster is running on Microsoft Azure. Each provider or provisioner requires a CSI plugin to expose their storage assets to Kubernetes. These plugins typically run as a set of pods in the kube-system namespace.

The CSI — Container Storage Interface

The Container Storage Interface is a vital component of the Kubernetes storage ecosystem. It has played a key role in bringing enterprise-grade storage from traditional vendors into Kubernetes environments. Unless you are developing storage plugins, you are unlikely to interact directly with CSI itself.

CSI is an open source project that defines a standards-based interface, allowing storage to be used in a uniform way across multiple container orchestrators. For example, a storage vendor can write a single CSI plugin that works with both Kubernetes and Docker Swarm. In practice, Kubernetes is the primary focus, but Docker is also implementing support for CSI.

In the Kubernetes world, CSI is the preferred way to write plugins and drivers. This means that plugin code no longer needs to be part of the main Kubernetes codebase. CSI exposes a clean interface and hides the complexity of volume management within Kubernetes.

From a day-to-day perspective, your main interaction with CSI will be referencing the appropriate CSI plugin in your YAML manifest files and consulting its documentation to understand supported features and attributes. Sometimes, these plugins are referred to as “provisioners,” especially when discussing storage classes.

The Persistent Volume Subsystem

This is the area where you will spend most of your time configuring and interacting with storage in Kubernetes. At a high level, persistent volumes, or PVs, represent external storage assets within Kubernetes. Persistent volume claims, or PVCs, act like tickets that grant a pod access to a persistent volume. Storage classes, or SCs, make the entire process dynamic and flexible.


Let’s walk through an example of how external storage systems are integrated with Kubernetes, using two tiers of storage: fast flash or solid-state drives, and slower mechanical archive storage, such as hard drives.

In this scenario, you would expect applications running on your Kubernetes cluster to use both types of storage. To achieve this, you can create two storage classes. One storage class is mapped to the fast SSD tier, and the other to the slower mechanical tier. For instance, you might name the storage class for SSDs as “sc-fast,” and the one for mechanical drives as “sc-slow.”

With these storage classes in place, applications can dynamically create volumes by making persistent volume claims, or PVCs, that reference either storage class. Each time a PVC is created, the Container Storage Interface, or CSI plugin, specified in the storage class, instructs the external storage system to create the appropriate storage asset. This asset is then automatically mapped to a Persistent Volume, or PV, within Kubernetes. The application uses the PVC to claim and mount this volume for its own use.

Now, let’s link all the components of the persistent volume subsystem together, using examples with three different persistent volume providers: NFS, Ceph, and AWS. These are simply storage providers that allow us to store data of any type. The underlying implementation details are not important for most users. What matters is understanding how the components of the persistent volume subsystem interact with these providers within your pods.

PersistentVolume

A Persistent Volume, or PV, is a piece of storage in the cluster that has been provisioned by an administrator, or dynamically provisioned using a storage class. PVs represent the actual storage resources available in the cluster, such as a disk on AWS Elastic Block Store, a Google Cloud Persistent Disk, or a Network File System share.

Persistent Volumes are cluster-wide resources. They are not tied to a specific namespace, and they exist independently of pods and persistent volume claims. You can think of a PV as a physical hard drive or a network-attached storage device. In essence, the persistent volume represents the physical storage devices that the cloud platform exposes to the user. These could be hard drives, solid-state drives, network-attached storage, and more. They sit at the lowest level in the persistent volume system hierarchy within the Kubernetes environment.

StorageClass

A storage class is an abstraction on top of the persistent volume. The storage class is responsible for creating the link between the physical device, or persistent volume, and the PVCs and pods that will use them. This connection is made through plugins or provisioners. These plugins are custom vendor drivers that know how to interact with the underlying persistent volume. For example, you might use a plugin for Network File System, or NFS, or for Ceph, which is a storage system that provides object, block, and file storage.

These plugins must be configured outside the cluster. The storage class provides the interface for interacting with them, but the actual plugin must be installed based on the type of persistent volume provider you want to use. Think of the storage class as a way to define the system drivers for persistent volumes, enabling interaction with them. The system drivers are implemented by the plugin or provisioner, which is defined in the storage class object.

Here’s what a systems administrator or user would typically do to set up the link between a persistent volume system, such as NFS or Ceph, and the cluster, so that pods can be configured to interact with these custom storage providers. Most regular users rely on the default storage class provided by the cloud platform, but large enterprise organizations might define their own storage classes for more specialized persistent volume vendors.

In the example provided, there are three storage class definitions. The first defines a storage class for NFS, specifying the provisioner as “nfs-client” and setting a parameter to not archive on delete. The second defines a storage class for Ceph, using the “rook-ceph” provisioner and specifying parameters such as the cluster ID, pool, image format, and image features. The third defines a storage class for AWS Elastic Block Store, using the “ebs.csi.aws.com” provisioner, specifying the type as general purpose SSD, enabling encryption, and setting policies for reclaiming and binding volumes.

PersistentVolumeClaim

A Persistent Volume Claim, or PVC, is a request for storage by a user. It allows users to ask for a specific amount of storage with certain characteristics, such as size and access mode. PVCs act as a middleman between pods and persistent volumes. They let users request storage without needing to know the details of the underlying storage infrastructure.

PVCs are namespaced resources, meaning they belong to a specific namespace. When a PVC is created, Kubernetes binds it to a persistent volume that matches the requested size and access mode. You can think of a PVC as a ticket that a user creates to request storage. The ticket is then matched to an available physical storage device, or persistent volume, such as a hard drive.

In the example, there are three PVC definitions, each referencing one of the storage classes described earlier. The first PVC requests ten gigabytes of storage from the NFS storage class, with an access mode of “ReadWriteMany.” The second PVC requests ten gigabytes from the Ceph storage class, with an access mode of “ReadWriteOnce.” The third PVC requests ten gigabytes from the AWS EBS storage class, also with “ReadWriteOnce” access. These PVCs define how the storage classes, and the actual persistent volumes, will be accessed by the pods that require them.

Pod

A pod is the smallest deployable unit in Kubernetes. Pods can request storage by referencing a PVC. Pods themselves are ephemeral, meaning they can be created, deleted, and rescheduled frequently. However, the data stored in a persistent volume, accessed via a PVC, persists even if the pod is deleted.

To link a pod with a PVC, you use the “volumes” and “volumeMounts” properties in the pod’s manifest. The “volumes” section links the pod to the PVC, while the “volumeMounts” section tells the pod where to mount the PVC inside the container. This setup allows an otherwise ephemeral pod, which has no context of persistent storage, to store data persistently. If the pod dies and is recreated, it will be able to retain its persistent state.

In the example, there are three pod definitions. The first pod, named “nfs-pod,” runs an Nginx container and mounts the NFS PVC at the path “slash usr slash share slash nginx slash html.” The second pod, “ceph-pod,” does the same with the Ceph PVC. The third pod, “aws-ebs-pod,” mounts the AWS EBS PVC in the same way. This demonstrates how pods can use different storage classes and persistent volume providers to mount storage inside their own environments.

StorageClass

In Kubernetes, storage classes are resources in the “storage dot k8s dot io slash v1” API group. The resource type is “StorageClass,” and you define them in regular YAML manifest files. These files are posted to the API server for deployment. When using the “kubectl” command-line tool, you can use the short name “sc” to refer to storage classes.

This overview should help clarify how the persistent volume subsystem in Kubernetes connects external storage systems, storage classes, persistent volume claims, and pods, enabling flexible and persistent storage for your applications.


As with all Kubernetes YAML files, the “kind” field tells the API server what type of object you are defining, while the “apiVersion” field specifies which version of the schema to use when creating it. The “metadata.name” field is an arbitrary string that lets you give the object a friendly, recognizable name.

In the example provided, the configuration uses the “fast-local.provisioner” and tells Kubernetes which storage plugin to use. The “parameters” block allows you to fine-tune the storage attributes. Finally, the “allowedTopologies” property lets you specify where replicas should be placed.

Let’s discuss a few important notes.

Multiple StorageClass Objects

You can configure as many StorageClass objects as you need. However, each class can only relate to a single type of storage on a single backend. For example, if you have a Kubernetes cluster with both StorageOS and Portworx as storage backends, you will need at least two StorageClasses. This is because each backend requires its own provisioner or plugin, as they do not share the same internal workings. Therefore, different plugin implementations are necessary.

On the other hand, each backend storage system can offer multiple classes or tiers of storage. Each of these needs its own StorageClass in Kubernetes. A simple example, which will be explored later, is the distinction between a slower, standard persistent disk and a faster, SSD persistent disk tier offered by the Google Cloud backend. These are typically implemented with the following StorageClasses on Google Kubernetes Engine, or GKE: “standard-rwo” for the slower standard disk, and “premium-rwo” for the faster SSD.

Now, let’s look at a specific example. The following StorageClass defines a block storage volume on a Commvault Hedvig array that is replicated between data centers in Sunderland and New York. This configuration will only work if you have Commvault Hedvig storage systems and the appropriate replication set up on the storage system.

This YAML block defines a StorageClass named “sc-hedvig-rep.” It uses the “io.hedvig.csi” provisioner and sets parameters to specify the backend type as “hedvig-block,” enables data center-aware replication, and lists the data centers as Sunderland and New York.

Persistent Volumes

There are a few other important settings you can configure in a StorageClass, such as access mode and reclaim policy. Kubernetes supports three access modes for volumes: ReadWriteOnce, ReadWriteMany, and ReadOnlyMany.

Access Mode

The access mode type specifies how and by what a given storage class can be accessed.

ReadWriteOnce, abbreviated as RWO, defines a persistent volume that can only be bound as read-write by a single node, and by extension, a single pod. Attempts to write or read data from another pod will fail. This is ideal for stateful applications like databases, where only one app can access the data at a time to retain atomicity.

ReadWriteMany, abbreviated as RWX, defines a persistent volume that can be bound as read-write by multiple nodes, and therefore multiple pods. This behavior depends on the underlying applications, since concurrent reads and writes can be unpredictable. The applications must be able to handle this gracefully. A good use case is for apps that need shared data and can publish that shared data in real time, such as content management systems, where the data does not necessarily interfere with each other.

ReadOnlyMany, abbreviated as ROX, defines a persistent volume that can be bound as read-only by multiple nodes and pods. These are meant for read-only access and are mostly useful for app configurations or stateful, read-only data used to bootstrap apps and pods, or during the runtime of these services.

Here are some general guidelines on which type of access mode to use:

Use ReadWriteOnce for stateful applications like databases.

Use ReadOnlyMany for sharing static, immutable data across multiple applications.

Use ReadWriteMany for applications that need shared read-write access, such as file or FTP servers.

Reclaim Policy

A volume’s reclaim policy tells Kubernetes how to handle a persistent volume when its persistent volume claim, or PVC, is released. Two policies currently exist: Delete and Retain.

Delete is the most dangerous and is the default for persistent volumes. It deletes the persistent volume and the associated storage resource on the external storage system when the PVC is released. This means all data will be lost, so you should use this policy with caution.

Retain will keep the associated persistent volume object on the cluster, as well as any data stored on the associated external asset. However, other PVCs are prevented from using it in the future. The disadvantage is that it requires manual cleanup.

Skeleton

To finalize the section about StorageClass, here is a brief overview of the skeleton specification structure of the StorageClass object. This is just a summary of what it supports as a native Kubernetes object.

This YAML block defines a StorageClass named “slow.” It marks the class as the default, uses the “kubernetes.io/gce-pd” provisioner, sets the disk type to “pd-standard,” and specifies the reclaim policy as Retain.

There are a few key points to remember:

First, StorageClass objects are immutable. This means you cannot modify them after they are deployed.

Second, the metadata.name should be a meaningful name, as it is how you and other objects refer to the class.

Third, the terms “provisioner” and “plugin” are used interchangeably.

Fourth, the parameters block is for plugin-specific values, and each plugin supports its own set of values. Configuring this section requires knowledge of the storage plugin and the associated storage backend. Each provisioner usually provides documentation.

The StorageClass lets you dynamically create physical backend storage resources that get automatically mapped to persistent volumes on Kubernetes. You define StorageClasses in YAML files that reference a plugin or provisioner and tie them to a particular tier of storage backend. For example, you might use high-performance SSD storage in the AWS Mumbai region. The StorageClass needs a name, and you deploy it using the “kubectl apply” command. Once deployed, the StorageClass watches the API server for new persistent volume claim objects referencing its name. When matching PVCs appear, the StorageClass dynamically creates the required asset on the backend storage system and maps it to a persistent volume in the Kubernetes environment. Applications can then claim it with a PVC.

Application

As we have already established, StorageClasses are usually created by system administrators. On most cloud platforms, these StorageClasses are mostly generic, providing the basic capabilities needed for users and their pods to interact with a persistent storage medium.

Using Existing StorageClass

The following command lists all StorageClasses defined on a typical Google Kubernetes Engine cluster. Based on the type of cloud platform provider and cluster, these will likely be different.

This command lists all StorageClasses in the cluster, showing their names, provisioners, reclaim policies, and volume binding modes.

There is quite a lot to learn from the output. First, all three StorageClasses were automatically created when the cluster was built by the cloud provider, which in this case is Google. This is common on hosted Kubernetes platforms, but your cluster may not have any pre-created StorageClasses.

The one on the second line is listed as the default. This means it will be used by any persistent volume claim that does not explicitly specify a StorageClass. Default StorageClasses are only useful in development environments or when you do not have specific storage requirements. In production environments, you should explicitly use a named StorageClass in your PVC that meets the requirements of your application.

The “PROVISIONER” column shows that two of the StorageClasses use the Container Storage Interface, or CSI, plugin, while the other uses the legacy in-tree plugin built into the Kubernetes source tree.

The “RECLAIMPOLICY” is set to Delete for all three. This means any PVC that uses these StorageClasses will create persistent volumes and storage assets that will be deleted when the PVC is deleted. This will result in data loss. The alternative is to use the Retain policy.

Setting the “VOLUMEBINDINGMODE” to Immediate will create the volume on the external storage system as soon as the PVC is created. If you have multiple data centers or cloud regions, the volume might be created in a different data center or region than the pod that eventually consumes it.

Setting the “WaitForFirstConsumer” mode will delay creation until the pod using the PVC is created. This ensures that the volume will be created in the same data center or region as the pod, and actually on the node where the pod is running.


You can use the kubectl describe command to get more detailed information about a Kubernetes object, just as you would with any other Kubernetes resource. If you want to see the full configuration in YAML format, you can use kubectl get sc followed by the storage class name, and specify the output as YAML.

For example, describing the storage class named premium-rwo will show you details such as its name, whether it is the default class, any annotations, the provisioner being used, parameters like the disk type, whether volume expansion is allowed, the reclaim policy, the volume binding mode, and any events associated with it. In this case, the provisioner is pd.csi.storage.gke.io, the type is set to pd-ssd, and the reclaim policy is set to Delete. The volume binding mode is WaitForFirstConsumer, which means the actual persistent volume will not be provisioned until a pod requests it.

Let’s create a new volume using the premium-rwo storage class. This storage class provides basic, fast SSD storage. First, let’s check for any existing persistent volumes and persistent volume claims. Running the appropriate kubectl commands will show that there are none, which is normal. Persistent volumes and claims are user-managed objects, unlike most storage classes, which are rarely provisioned by regular users except in enterprise environments with more specialized needs.

Next, we define a persistent volume claim, or PVC, that uses the premium-rwo storage class. This claim requests ten gigabytes of storage and specifies the access mode ReadWriteOnce. This means the persistent volume created by this claim will only be accessible by a single pod at a time. The manifest for this PVC sets the storage class name to premium-rwo, requests ten gigabytes of storage, and sets the access mode to ReadWriteOnce.

Applying this manifest with kubectl creates the persistent volume claim. If you check the status of persistent volumes and claims immediately after, you’ll see that the PVC has been created but is in a pending state. No persistent volume has been created yet. This is because the premium-rwo storage class uses the WaitForFirstConsumer binding mode, so the volume will not be provisioned until a pod actually uses the claim.

To trigger the creation of the persistent volume, we need to create a pod that uses this PVC. The pod manifest defines a single volume that references the pvc-prem claim. The container in the pod uses the Ubuntu image and simply sleeps for sixty minutes. The volume is mounted at the path slash data inside the container.

When you apply this pod manifest, the pod will start, and the persistent volume will be created and bound to the claim. The volumeMounts section in the pod manifest shows that the pvc-prem claim will be mounted at slash data in the container. The volumes section defines the volumes for the pod, in this case just one, which uses the already created claim named pvc-prem.

After giving the pod some time to start, you can check the persistent volumes again. You’ll see that a new persistent volume has been created, with a capacity of ten gigabytes, access mode ReadWriteOnce, and a reclaim policy of Delete. The status will show that it is bound to the default namespace and the pvc-prem claim, and it uses the premium-rwo storage class.

To clean up the resources you’ve just created, you can delete the pod. Since the storage class’s reclaim policy is set to Delete, the persistent volume will also be deleted once nothing is bound to it anymore.

Creating a new StorageClass

Now, let’s talk about creating a custom storage class object, which can then be used to create a new volume just like in the previous examples. The storage class we’ll create is for a fast SSD device, uses regional replication, and is set to create volumes on demand with the WaitForFirstConsumer binding mode.

The manifest for this storage class sets the provisioner to pd.csi.storage.gke.io, the type to pd-ssd, and the replication type to regional-pd. The volume binding mode is WaitForFirstConsumer, and the reclaim policy is set to Retain. There are also commented-out sections for allowed topologies, which could be used to restrict the storage class to specific zones.

It’s important to remember that the parameters in the storage class manifest are specific to the provisioner being used. In this case, the provisioner is a Google-based storage plugin, and the parameters are only meaningful for that provisioner. Other provisioners, such as AWS EBS, Ceph, or NFS, use different parameter names and options. The Container Storage Interface, or CSI, standardizes the API for communication between Kubernetes and the storage backend, but it does not enforce specific parameters. Each CSI driver or provisioner defines its own set of supported parameters based on the capabilities of the storage backend.

After creating the storage class with kubectl, you can list all storage classes to see the new one. The output will show the name, provisioner, reclaim policy, and volume binding mode for each storage class.

Next, you can deploy a new persistent volume claim that uses the custom storage class. The manifest for this PVC is similar to the previous one, but it requests twenty gigabytes of storage and sets the storage class name to sc-fast-repl.

Finally, you can create a new pod that uses this PVC. The pod manifest is similar to the earlier example, with a single volume referencing the pvc2 claim, and the container mounting the volume at slash data.

Since both the pod and PVC were deployed using manifests, you can use those same manifest files to delete the resources when you’re done. Deleting the pod and the persistent volume claim will remove those resources, but if you check the persistent volumes, you’ll see that the volume still exists. This is because the storage class’s reclaim policy is set to Retain, which keeps the persistent volume and its associated backend storage even after the PVC is deleted.

ConfigMaps and Secrets

Most business applications are made up of two main parts: the application itself and its configuration. For example, a web server like NGINX or Apache HTTPD is not very useful without a configuration file. However, when you combine the application with its configuration, it becomes extremely useful.


In the past, we often bundled the application and its configuration together into a single, easy-to-deploy unit. As we transitioned into the early days of cloud-native microservices and applications, we brought this model with us. However, in the cloud-native world, this approach is considered an anti-pattern. Instead, you should decouple the application from its configuration.

The big picture

Most applications require configuration to function correctly. This fundamental need does not change in the Kubernetes world. Let’s consider a simple example: you want to deploy modern applications to Kubernetes and have three distinct environments—development, test, and production.

Developers write and update the application. Initial testing happens in the development environment. More rigorous testing is performed in the test environment, where stricter rules apply. Finally, stable components graduate to the production environment. However, each environment has subtle differences. These differences include the number of nodes, node configurations, network and security policies, different sets of credentials and certificates, and more.

Currently, you might package each application microservice with its configuration baked into the container. With this approach, you must perform several steps for every business application. You need to build three distinct images for each environment, each with its own configuration. You must store these images in three separate repositories—one for development, one for test, and one for production. Then, you run each version of the image in its specific environment.

Every time you change the configuration of an application, even for a minor fix like correcting a typo, you have to build and package an entirely new image and update the entire application. This approach, where the application and its configuration are stored as a single artifact—a container image—has several drawbacks.

Because your development, test, and production environments have different characteristics, each environment needs its own image. A development or test image will not work in production due to differences like security credentials and restrictions. This requires extra work to create and maintain three copies of each application. It complicates matters and increases the chances of mistakes, such as things working in development and test but failing in production.

How should it work, then? Ideally, you should be able to build a single version of your application that is shared across all three environments. You store a single image in a single repository and run the same version of that image in all environments. To make this possible, you build your application and images as generically as possible, with no embedded configuration. You then create and store configurations as separate objects and apply the appropriate configuration to the application at runtime.

For example, you might have a single copy of a web server that you can deploy to all three environments. When you deploy it to production, you apply the production configuration. When you run it in development, you apply the development configuration, and so on. In this model, you create and test a single version of each application image, and you can even reuse images across different applications. For instance, a hardened, stripped-down NGINX image can be used by many different applications—just load different configurations at runtime.

ConfigMaps

Kubernetes provides an object called a ConfigMap. This object lets you store configuration data outside of a Pod and allows you to dynamically inject the configuration into a Pod at runtime.

When we use the term Pod, we really mean the container, since it is ultimately the container that receives the configuration data and runs the application. However, the Pod is the logical object that the ConfigMap is bound to, as the Pod manages the containers and images being run.

ConfigMaps are first-class objects in the Kubernetes API. They are part of the core API group and are in version one of the API. ConfigMaps are stable and have been around for a while. You can operate on them with the usual kubectl commands, and they can be defined and deployed using the standard manifest YAML file format.

Premise

ConfigMaps are typically used to store non-sensitive configuration data, such as environment variables, entire configuration files, hostnames, server ports, and account names. You should not use ConfigMaps to store sensitive data like certificates and passwords. Kubernetes provides a different object, called a Secret, for storing sensitive data.

Secrets and ConfigMaps are very similar in design and implementation. The major difference is that Kubernetes takes steps to obscure the data stored in Secrets, but does not do so for data stored in ConfigMaps.

ConfigMaps are defined as a map of key-value pairs, with each key-value pair called an entry. Keys are arbitrary names that can include alphanumeric characters, dashes, dots, and underscores. Values can contain almost anything, including multiple lines with carriage returns. Keys and values are separated by a colon.

For example, you can store an entire configuration file as a value in a ConfigMap. The key might be “conf,” and the value could be a multi-line configuration for a web server, including directives, server blocks, and other settings.

Once the data is stored in a ConfigMap, it can be injected into containers at runtime using several methods. These methods include environment variables, arguments to the container’s startup command, and files or volumes. All of these methods work seamlessly with existing applications. From the application’s perspective, it simply sees its configuration data as an environment variable, a command-line argument, or a file on the filesystem. The application is unaware that the data originally came from a ConfigMap or a Secret object.

The most flexible method is using volumes, while the most limited is passing data through the startup command. A Kubernetes-native application is one that knows it is running on Kubernetes and can communicate with the Kubernetes API. Such applications can access ConfigMap data directly via the API, without needing environment variables, startup arguments, or mounted volumes. This can simplify application configuration, but it means the application will only run in a Kubernetes environment.

Definition

To create a ConfigMap directly from the command line, you can use a command that specifies the name of the ConfigMap and provides key-value pairs. For example, you might create a ConfigMap named “testmap1” with a short name and a long name. After creating it, you can describe the ConfigMap to see its contents, including the keys and their corresponding values.

Creating

The most basic example involves creating a new ConfigMap object directly from the command line, without using any manifest files. This demonstrates how ConfigMaps can be created quickly, but in real-world scenarios, the preferred method is to use a YAML manifest file.

A typical manifest file for a ConfigMap specifies the kind as ConfigMap, the API version, metadata such as the name, and a data section with key-value pairs representing the configuration data. For example, you might have a ConfigMap named “multimap” with keys for a given name and a family name.

To create the ConfigMap from the manifest, you apply the YAML file using a command. The manifest can also use special pipe characters to indicate that everything after the pipe should be treated as a literal value. This allows you to store multi-line text as the value of a key, which is useful for injecting complex configurations, such as JSON files or shell scripts, as files from the ConfigMap.

For example, you might have a ConfigMap named “test-conf” with a key called “config.” The value of this key could be a multi-line configuration, including environment settings, endpoints, character encoding, vault paths, and log sizes.

Injecting

Binding the ConfigMap to a Pod and injecting its data is the next step. There are three main methods to inject ConfigMap data, each with its own pros and cons.

Environment variables

A common way to get ConfigMap data into a container is through environment variables. You create the ConfigMap, then map its entries into environment variables in the container section of a Pod template. When the container starts, the environment variables appear as standard Linux or Windows environment variables. This is typically done using a reference to the ConfigMap and specifying the key and the source ConfigMap name.


Let’s begin by looking at how environment variables are created from ConfigMaps in Kubernetes.

When the pod is scheduled and the container starts, the environment variables FIRSTNAME and LASTNAME are created as standard Linux environment variables inside the container. Applications running in the container can use these variables just like any other environment variable.

To deploy the pod from the manifest described above, you would use a command to apply the configuration and then another command to check the environment variables inside the running container. The first command deploys the pod, and the second command executes an environment listing inside the container, filtering for variables that include the word NAME. The output shows that the environment variables HOSTNAME, FIRSTNAME, and LASTNAME are set, with values such as envpod, Nigel, and Poulton.

However, there is a drawback to using ConfigMaps with environment variables. Environment variables are static. This means that if you update the values in the ConfigMap, the changes are not reflected in containers that are already running. For example, if you update the given and family entries in the ConfigMap, the environment variables in existing containers will not see those updates. This is a major reason why environment variables are not always the best choice for dynamic configuration.

Now, let’s talk about startup arguments.

The concept here is simple. You specify a startup command for a container and then customize it with variables. In the provided pod template, a single container is started with a specific argument. If you look closely, you’ll notice that this approach also uses environment variables. The command for the container echoes the first and last name using the environment variables FIRSTNAME and LASTNAME, which are again sourced from the ConfigMap. This demonstrates how you can use environment variables to customize startup commands for containers.

Next, let’s discuss using ConfigMaps with volumes and files.

Using ConfigMaps with volumes is the most flexible option. You can reference entire configuration files and make updates to the ConfigMap, which will then be reflected in running containers. This means you can change entries in the ConfigMap after deploying a container, and those changes will become available to the application inside the container. The updates may take a minute or so to appear.

How is the ConfigMap exposed as a volume? First, you create a ConfigMap object. Then, you bind this ConfigMap as a volume to a pod and mount it into the container. The entries in the ConfigMap appear in the container as individual files.

In the example provided, the pod specification defines a volume named volmap, which is bound to the multimap ConfigMap. The container uses the nginx image and mounts the volmap volume at the path slash etc slash name. The effect is that the keys in the multimap ConfigMap, which are given and family, become filenames in the mounted directory. The values for these keys, such as Nigel and Poulton, become the contents of those files. This approach is powerful because you can define configuration for any application, and changes to the ConfigMap will be reflected in the files inside the container.

Now, let’s move on to Secrets.

Secrets in Kubernetes are almost identical to ConfigMaps. They hold application configuration data that is injected into containers at runtime. However, Secrets are designed for sensitive data, such as passwords, certificates, and OAuth tokens.

Are secrets really secure? The quick answer is no, but there is a more nuanced explanation. Despite being designed for sensitive data, Kubernetes does not encrypt Secrets by default. They are simply base sixty-four encoded, which means they can be easily decoded. Fortunately, it is possible to configure encryption at rest using EncryptionConfiguration objects, and most service meshes encrypt network traffic.

A typical workflow for a Secret is as follows. First, the secret is created and persisted to the cluster store as an unencrypted object. Next, a pod that uses the secret is scheduled to a cluster node. The secret is then transferred over the network, unencrypted, to the node. The kubelet on the node starts the pod and its containers. The secret is mounted into the container via an in-memory temporary file system, and decoded from base sixty-four to plain text. The application then consumes the secret. When the pod is deleted, the secret is removed from the node.

While it is possible to encrypt the secret in the cluster store and use a service mesh to encrypt it in transit, the secret is always mounted as plain text in the pod and container. This is necessary so the application can consume it without having to perform decryption or base sixty-four decoding. The use of in-memory temporary file systems means the secrets are never persisted to disk on the node.

In summary, secrets are not very secure by default, but you can take extra steps to improve their security. They are also limited to one megabyte in size.

A common use case for Secrets is a generic TLS termination proxy that can be used across development, test, and production environments. You create a standard image and load the appropriate TLS keys at runtime for each environment.

By default, every pod gets a secret mounted into it as a volume, which it uses to authenticate itself when talking to the API server. If you look at any running pod in your cluster and use the describe command from kubectl, you will see a secret value, such as default-token-s9nmx, mounted into the container automatically. This is a simple API token used to call the REST endpoints of the control plane.

The describe output for a pod shows the container’s mounts, including the path slash etc slash name from the volmap volume, and slash var slash run slash secrets slash kubernetes dot io slash serviceaccount from the default-token-s9nmx secret. The volumes section shows that default-token-s9nmx is a secret volume, with the secret name and other details.

Also note where this is mounted: slash var slash run slash secrets slash kubernetes dot io. This is a common way for Kubernetes to prefix its own resources. In this case, the secret is mounted under slash var slash run slash secrets, in a Kubernetes-specific folder.

If you list the secrets in your cluster, you will see output showing the currently active secrets, including the service account token used by the pod to communicate with the REST server of the control plane.

Describing a secret, such as default-token-s9nmx, will show details like the name, namespace, annotations, type, and the data it contains. The data includes the token, a certificate, and the namespace.

Next, let’s talk about how to create secrets.


As we have already seen, secrets in Kubernetes are not encrypted in the cluster store, which is called etcd. They are also not encrypted while in transit, not encrypted on the network, and not encrypted when they are surfaced inside a container. There are ways to encrypt secrets, but ultimately, the container must be able to read these secrets in plain text. This can happen either by having the container decrypt the secret itself, or by having the secret decrypted when it is delivered or mounted into the container.

By default, when we create secrets in a manifest file, the contents—specifically the data section—must contain the base64-encoded version of the secret or data. In other words, the values you provide in the data field must be encoded in base64 before being accepted by the Kubernetes API server.

For example, consider a manifest that defines a secret. In this manifest, the data section contains base64-encoded values for the username and password fields. The API server will accept these values as they are, as long as they are properly encoded. If you try to provide a value in the data field that is not base64-encoded, the API server will reject the secret, and posting the manifest will fail.

If you want to provide the raw, unencoded data in plain text, you should use the stringData field instead of data. When you use stringData, the API server will handle the base64 encoding for you.

To apply the secret manifest, you would use a command that tells Kubernetes to create the secret resource from your manifest file.

If you want to see the actual value stored in a secret, you can decode the base64-encoded value using a command-line tool. This will reveal the original plain text value.

Using secrets

The most flexible way to inject a secret into a pod, or container, is by using a special type of volume called a secret volume. For example, you can define a pod with a secret volume that is based on the secret you created earlier. In this setup, the secret is mounted into the container at a specific path, such as slash etc slash secret, and is marked as read-only.

The main difference between ConfigMap volumes and secret volumes is that secret volumes are always mounted as read-only. Additionally, the data in these volumes is automatically decoded from base64 to plain text before being mounted. This means the container receives the actual value that the base64 encoding represents.

To create a pod where the secret is mounted under a specific directory, you would use a command to apply the pod manifest.

StatefulSet

For the purposes of this discussion, a stateful application is one that creates and saves valuable data. An example might be an application that saves data about client sessions and uses it for future sessions. Other examples include databases and other data stores.

StatefulSets are the logical equivalent of Deployments in the Kubernetes world. While their specification structure is similar, there are some important differences. StatefulSets include fields like serviceName, which associates the StatefulSet with a headless service. They also support volumeClaimTemplates, which allow for the creation of persistent volume claims for each pod. StatefulSets do not support the strategy field for updates; they always use a rolling update strategy.

In contrast, Deployments include a strategy field that lets you define update strategies, such as rolling update or recreate. Deployments do not have serviceName or volumeClaimTemplates.

Theory

It is often useful to compare StatefulSets with Deployments. Both are first-class API objects and follow the typical Kubernetes controller architecture. They are both implemented as controllers that operate reconciliation loops, watching the state of the cluster via the API server and working to bring the observed state in line with the desired state. Deployments and StatefulSets both support self-healing, scaling, updates, and more.

However, there are some vital differences between StatefulSets and Deployments. StatefulSets guarantee predictable and persistent pod names, predictable and persistent DNS hostnames, and predictable and persistent volume bindings. These three properties form the state of a pod, sometimes referred to as its sticky ID. StatefulSets ensure this state or ID is persisted across failures, scaling, and other scheduling operations. This makes them ideal for applications that require unique pods that are not interchangeable.

For example, if a pod managed by a StatefulSet fails, it will be replaced by a new pod with the exact same pod name, the exact same DNS hostname, and the exact same volumes. This is true even if the replacement pod is started on a different cluster node. The same is not true for pods managed by a Deployment.

Consider a manifest that defines a StatefulSet named tkb-sts. This StatefulSet specifies two pod replicas running the latest MongoDB image. When you post this manifest to the API server, it is persisted to the cluster store. The replicas are assigned to cluster nodes, and the StatefulSet controller monitors the state of the cluster, making sure the observed state matches the desired state. That is the big picture. Now, let us take a closer look at some of the major characteristics of StatefulSet before walking through an example.

Naming

All pods managed by a StatefulSet get predictable and persistent names. These names are vital and are at the core of how pods are started, self-healed, scaled, deleted, and attached to volumes. The format of a StatefulSet pod name is the StatefulSet name, followed by a dash, and then an integer. The integer is a zero-based index, which simply means it starts from zero.

The first pod created by a StatefulSet always gets index zero, and each subsequent pod gets the next highest number. For example, if the StatefulSet is named tkb-sts, the first pod will be called tkb-sts dash zero, and the second will be called tkb-sts dash one. Note that StatefulSet names must be valid DNS names, so no exotic characters are allowed. This rule and its reasons will come into play later.

Creation

Another fundamental characteristic of StatefulSet is the controlled and ordered way it starts and stops pods. StatefulSet creates one pod at a time and always waits for the previous pod to be running and ready before creating the next. This is different from Deployments, which use a ReplicaSet controller to start all pods at the same time, potentially causing race conditions.

For example, with the previous YAML snippet, tkb-sts dash zero will be started first and must be running and ready before the StatefulSet controller starts tkb-sts dash one. The same applies to subsequent pods. Each new pod must be running and ready before the next one is created. Scaling operations follow the same ordered startup rules. For example, scaling from three to five replicas will start a new pod called tkb-sts dash three and wait for it to be running and ready before creating tkb-sts dash four.

Scaling down follows the same rules in reverse. The controller terminates the pod with the highest index first, waits for it to fully terminate, and then terminates the pod with the next highest index. Knowing the order in which pods will be scaled down, as well as knowing that pods will not be terminated in parallel, is a game changer for many stateful applications. For example, clustered applications that store data can potentially lose data if multiple replicas go down at the same time. StatefulSet guarantees this will never happen.

You can also inject additional delays using settings like terminationGracePeriodSeconds to further control the scaling down process. All in all, StatefulSets bring a lot to the table for clustered applications that create and store data.

Finally, it is worth noting that StatefulSet controllers handle their own self-healing and scaling. This is architecturally different from Deployments, which use a separate ReplicaSet helper controller for these operations. In summary, StatefulSet provides a robust, predictable, and easy-to-manage control flow over the deployment lifecycle of pods. This is crucial for services that require more fine-grained control over their own lifecycle and other dependent components.

Deleting

There are two major things to consider when deleting a StatefulSet object.


Firstly, deleting a StatefulSet does not terminate its Pods in any particular order. With this in mind, you may want to scale a StatefulSet down to zero replicas before deleting it. This approach helps ensure a more predictable and controlled shutdown process. Additionally, you can use the terminationGracePeriodSeconds setting to further control how Pods are terminated. It is common to set this value to at least ten seconds. This gives applications running inside the Pods a chance to flush local buffers and safely commit any writes that are still in progress.

Volumes

Volumes play a crucial role in the identity and state of StatefulSet Pods. When a StatefulSet Pod is created, any volumes it needs are also created at the same time. These volumes are named in a special way that connects them to the correct Pod. Each volume is created by a Persistent Volume Claim, or PVC. Volumes are decoupled from Pods through the standard Persistent Volume Claims system. This means that volumes have a separate lifecycle from Pods, allowing them to survive Pod failures and termination operations.

For example, if a StatefulSet Pod fails or is terminated, its associated volumes are unaffected. This allows replacement Pods to attach to the same storage as the Pods they are replacing. This remains true even if the replacement Pods are scheduled on different cluster nodes. The same logic applies to scaling operations. If a StatefulSet Pod is deleted as part of a scale-down operation, subsequent scale-up operations will attach new Pods to the surviving volumes that match their names. This behavior can be a lifesaver if you accidentally delete a StatefulSet Pod, especially if it is the last replica.

It is important to note the difference between Deployments and StatefulSets in their use of persistent volumes and persistent volume claims. With a Deployment, every replica is identical except for its name. In particular, every replica will share the same PersistentVolumeClaim and the same underlying PersistentVolume. In contrast, each replica of a StatefulSet gets its own PersistentVolumeClaim, assuming you use the volumeClaimTemplates field to declare the PVC. If the StatefulSet scales up, the new Pod will get a new, empty PersistentVolume. If it scales down, the PersistentVolume is preserved. If it scales up again, the previous PersistentVolume is reused.

It is also important to understand how the names of the unique PVC objects are generated when using volumeClaimTemplates. Each PVC object is created for each Pod replica. The name is generated from the name defined under volumeClaimTemplates in the StatefulSet specification, combined with the ordinal number of the Pod it is attached to.

Handling Failures

The StatefulSet controller observes the state of the cluster and tries to keep the observed state in sync with the desired state. The simplest example is a Pod failure. If you have a StatefulSet called tkb-sts with five replicas, and tkb-sts-3 fails, the controller will start a replacement Pod with the same name and attach it to the same volumes it was already using.

However, if a failed Pod recovers after Kubernetes has already replaced it, you could end up with two identical Pods trying to write to the same volume. This can result in data corruption. As a result, the StatefulSet controller is extremely careful in how it handles failures.

Possible node failures are particularly challenging. For example, if Kubernetes loses contact with a node, it cannot immediately know if the node has failed permanently, is down and will never recover, or if it is just a temporary glitch, such as a network partition, a crashed kubelet, or a simple reboot. To complicate matters further, the controller cannot even force the Pod to terminate, since the local kubelet may never receive the instruction to do so. With all of this in mind, manual intervention is often needed before Kubernetes will replace Pods on failed nodes.

Unlike Deployments, where once a Pod is considered failed it will never be restarted or reinstated in the Kubernetes environment—instead, a brand new Pod is created and the old one is completely decommissioned—in the StatefulSet world, that is not the case. A Pod might actually recover, so the StatefulSet controller has to be more conservative when managing failed Pods, unlike its Deployment and ReplicaSet counterparts.

Services

StatefulSets are designed for applications that need Pods to be predictable and long-lived. As a result, other parts of the application, as well as other applications, may need to connect directly to individual Pods. To make this possible, StatefulSets use a headless service to create predictable DNS names for every Pod replica they manage. Other applications can then query the DNS service for the full list of Pod replicas and use these details to connect directly to the Pods.

The following YAML snippet shows a headless Service called mongo-prod that is listed in the StatefulSet YAML. In summary, this configuration defines a headless Service named mongo-prod, which does not receive a virtual ClusterIP address. It also defines a StatefulSet named sts-mongo, which uses the mongo-prod Service for Pod DNS management.

Let us explain the terms headless Service and governing Service. A headless Service is simply a regular Kubernetes Service object without an IP address. You can see this because the ClusterIP is set to none. This tells Kubernetes that the Service will not receive any virtual ClusterIP address. It becomes a StatefulSet governing service when you list it in the StatefulSet manifest under spec.serviceName. When these two objects are combined, the Service will create DNS SRV records for each Pod replica that matches the label selector of the headless Service. Other Pods and applications can then find members of the StatefulSet by performing a DNS lookup against the name of the headless Service.

Network Traffic

Unlike regular services, those represented by StatefulSets will not have a virtual ClusterIP address. The headless service is what enables direct Pod-to-Pod communication using predictable DNS names.

Skeleton

The following examples represent the general structure of a deployment stage for a StatefulSet. This includes the creation and definition of all components that tie into the StatefulSet, such as PersistentVolumeClaims, StorageClasses, headless Services, and more.

First, let’s create the most basic structure: the StorageClass. This is the entry point for the PersistentVolumeClaim later. Even though most cloud providers have default StorageClass objects, it is useful to see how it all fits together. In real-world scenarios, you will most likely use the StorageClass provided by your cloud provider. The example StorageClass here defines a storage type of SSD, using the plugin pd.csi.storage.gke.io. This is specific to Google Cloud, but the approach is similar for other providers. In this case, the StorageClass represents a basic, fast storage backed by a solid-state drive.

Next, here is the headless service definition. We know it is headless because the ClusterIP is set to none. This is essentially the only difference from regular Kubernetes Service objects.

Finally, let’s look at the StatefulSet itself. This example shows some of the most prominent differences compared to the Deployment object, such as the terminationGracePeriodSeconds field. Notice that most of the Pod specification in the template section is similar to what you would see in a Deployment. The template section defines the properties of the Pod object that Kubernetes will create.

Now, pay attention to the most interesting part that differs significantly from how we define Pod templates in Deployments: the volumeClaimTemplates section. You may notice that the format and specification of this section matches that of a PersistentVolumeClaim object. That is because it is essentially the same. However, as mentioned earlier, PersistentVolumeClaims for StatefulSets must be bound to individual Pod instances, unlike with Deployment Pods. That is why the PVC is defined in the spec of the StatefulSet. For each new replica of the Pod, a new PVC will be created, and by extension, a new, independent volume. Each PVC will be bound to the name of the Pod. This is also why it is important that the name of the Pod template, specifically spec.serviceName, is DNS format compliant.


Let’s begin by discussing the StatefulSet YAML configuration.

This configuration defines a Kubernetes StatefulSet named “tkb-sts” that manages three replicas of a web application. Each replica runs an NGINX container, exposing port eighty. The StatefulSet uses a headless service called “dullahan” to manage network identity and stable storage for each pod.

Within the pod specification, there are two types of volumes. The first is a regular volume, defined in the “volumes” section, which uses a persistent volume claim named “config-data.” This is typically used for read-only configuration data that does not need to be unique or stateful for each pod. The second type is defined using “volumeClaimTemplates.” Here, a template named “nginx-data” is specified, which requests one gigabyte of storage with the “ReadWriteOnce” access mode and uses the “flash” storage class. This template ensures that each pod replica receives its own unique persistent volume claim, which is essential for stateful workloads.

It’s important to note that the “volumeClaimTemplates” mechanism does not replace the regular method of defining persistent volume claims through the “spec.volumes” section. Instead, it complements it. The regular method is still useful for mounting shared or read-only data, such as configuration files, that do not require a unique volume per pod.

When using “volumeClaimTemplates,” the persistent volume claims created for each pod will be named using the base name from the template, followed by a dash and the pod’s ordinal number. For example, with the template name “nginx-data,” the claims will be named “nginx-data-0,” “nginx-data-1,” and so on. The ordinal number corresponds to the pod’s replica index, starting from zero.

The names of the pods themselves are generated from the StatefulSet’s name, with the ordinal appended. For instance, the first pod will be “tkb-sts-0,” the second “tkb-sts-1,” and so forth. This tight coupling between StatefulSets, their pods, and the underlying storage is a key difference from Deployments, where pods are more loosely associated with their storage.

Another distinction is the relationship between the StatefulSet and its service. In StatefulSets, the service is directly referenced in the “spec.serviceName” property. This is because the service, typically a headless service, is responsible for providing stable network identities and DNS names for each pod. The DNS names for the pods follow a predictable pattern: the pod name, followed by the service name, the namespace, and the cluster domain. For example, “tkb-sts-0.dullahan.default.svc.cluster.local” would be the DNS name for the first pod.

Now, let’s move on to the topic of security in Kubernetes.

Kubernetes is designed around an API-centric model, with all operations funneled through the API server. Security is enforced at multiple stages as requests pass through the control plane. The primary mechanism for access control is Role Based Access Control, or RBAC.

Let’s review the theory behind this process.

Any entity that interacts with the Kubernetes API server—whether it’s an operator using kubectl, a pod, a kubelet, or a control plane component—follows a similar request flow. The request is made by a subject, such as a user or group, and is sent to the API server. The server then processes the request through authentication, authorization, and admission control.

Consider an example where a user named “grant” wants to create a Deployment called “hive” in the “terran” namespace. Grant issues a kubectl command, which sends a request to the API server with his credentials. Thanks to Transport Layer Security, or TLS, the connection is secure. The authentication module checks whether the request is genuinely from grant, and not an impostor. If authentication succeeds, the authorization module—using RBAC—verifies whether grant is allowed to create Deployments in the “terran” namespace. If both checks pass, admission control applies any relevant policies, and the request is accepted and executed.

This process is similar to boarding a plane. You first authenticate yourself with a photo ID, such as a passport. Then, you present a ticket that authorizes you to board and occupy a specific seat. If you pass both checks, admission control may apply additional policies, such as restrictions on hand luggage or food. Only after all these steps are you allowed to board.

Let’s focus on authentication.

Authentication, often abbreviated as “auth N,” is about proving your identity. Every request to the API server must include credentials, and the authentication layer is responsible for verifying them. If verification fails, the API server returns an HTTP four-oh-one error, and the request is denied. If it passes, the request moves on to authorization.

The authentication layer in Kubernetes is pluggable. Common modules include client certificates, webhooks, and integration with external identity management systems, such as Active Directory or cloud-based Identity and Access Management. Kubernetes does not have its own built-in identity database. Instead, it relies on external systems, which helps avoid creating yet another identity silo. Most Kubernetes clusters support client certificates by default, but in production environments, you’ll likely want to integrate with your organization’s identity management system. Many hosted Kubernetes services make this integration straightforward.

Now, let’s discuss how cluster details and credentials are managed.

Cluster details and credentials are stored in a file called “kubeconfig.” Tools like kubectl read this file to determine which cluster to connect to and which credentials to use. On Windows, the kubeconfig file is typically located at “C colon backslash Users backslash user name backslash dot kube backslash config.” On Unix systems, it’s usually at “slash home slash user name slash dot kube slash config.”

Many Kubernetes installations can automatically merge cluster endpoint details and credentials into your existing kubeconfig file. For example, Google Kubernetes Engine provides a “gcloud” command that merges the necessary details into your local kubeconfig. Running this command fetches the credentials for a specific cluster and updates your configuration.

A typical kubeconfig file defines clusters, users, contexts, and the current context. The clusters section lists one or more Kubernetes clusters, each with a friendly name, API server endpoint, and the public key of its certificate authority. The users section defines one or more users, each with a name and a token, which is often an X. five oh nine encrypted value representing the user’s identity. The contexts section combines users and clusters, specifying which user should access which cluster. The current context determines the default cluster and user for all kubectl commands.

In summary, the kubeconfig file organizes access to multiple clusters and users, making it easy to switch between environments and manage credentials securely.


Assuming the previous kubeconfig, all kubectl commands will target the prod-shield cluster and authenticate as the user named in angle brackets. The authentication module is responsible for verifying that the user is who they claim to be. If client certificates are used, the module also checks that the certificate is signed by a trusted certificate authority.

Authorization

Authorization takes place immediately after successful authentication. Sometimes, you’ll see it referred to as “auth Z.” Kubernetes authorization is pluggable, meaning you can run multiple authorization modules on a single cluster. As soon as any of these modules processes an authorization request, the process moves on to admission control.

Access control

The most common authorization module in Kubernetes is called RBAC, which stands for role-based access control. At its core, RBAC is about three things: users, actions, and resources. In other words, it determines which users can perform which actions on which resources within the cluster.

For example, imagine the following scenarios: Bao is allowed to create pods, Kalila can list deployments, and Josh is permitted to delete service accounts.

RBAC is enabled on most Kubernetes clusters and has been generally available since Kubernetes version one point eight. It follows a least-privilege, deny-by-default model. This means that all actions are denied unless you explicitly create an allow rule. In fact, Kubernetes RBAC does not support deny rules at all—only allow rules. While this might seem like a small detail, it actually makes RBAC much simpler to implement, troubleshoot, and maintain, and it also makes the system safer.

Two concepts are vital to understanding Kubernetes RBAC: Roles and RoleBindings. Roles define a set of permissions, while RoleBindings grant those permissions to users.

Here’s an example. A resource manifest defines a Role object called “read-deployments.” This role grants permissions to get, watch, and list Deployment objects in the shield namespace. There is also a RoleBinding that assigns this role to a user named sky.

In summary, the first part of the manifest creates a Role named “read-deployments” in the shield namespace, allowing the actions get, watch, and list on Deployment resources. The second part creates a RoleBinding, which assigns the “read-deployments” role to the user sky within the same namespace.

If both of these resources are deployed to a cluster, an authenticated user named sky will be able to run commands such as “kubectl get deployments” in the shield namespace.

It’s important to note that the username listed in the RoleBinding must be a string and must exactly match the username that was successfully authenticated.

Roles and Resources

The previous Role object has three key properties: apiGroups, resources, and verbs. Together, these define which actions are allowed on which objects. The apiGroups and resources fields specify the object, while verbs define the allowed actions. In the example, the role allows read access—specifically, get, watch, and list—on Deployment objects.

Let’s look at some combinations of apiGroups and resources. For instance, the core apiGroup, represented by empty double quotes, includes resources like pods and secrets. Other apiGroups, such as “storage dot k8s dot io” or “apps,” include resources like storage classes and deployments, respectively. The apiGroup is important because it namespaces different Kubernetes resources. You must specify the correct apiGroup for a given resource, unless it belongs to the default group, which is indicated by empty quotes.

Kubernetes uses a standard set of verbs to describe the actions a subject can perform on a resource. These verbs are self-explanatory and case-sensitive. For example, the verbs create, get, list, watch, update, patch, and delete correspond to HTTP methods like POST, GET, PUT, PATCH, and DELETE. Common HTTP response codes include two hundred OK, two hundred one created, and four hundred three access denied.

The verbs you use in the rules section of a Role object are the same as those listed in the Kubernetes verbs column. To see all API resources supported on your cluster, you can run a command that lists each resource, its API group, whether it is namespaced, its kind, and the supported verbs. This is a valuable resource for building rule definitions.

In summary, this command displays all API resources available in your cluster, including their short names, API versions, whether they are namespaced, their kind, and the verbs they support. It helps you understand which resources you can control with RBAC and how to refer to them in your manifests and commands.

The table output also includes a SHORTNAMES column, which provides aliases for resource names. Any resource with a short name can be referred to by that alias in manifest files, shell commands, and other contexts.

If you compare the role manifest definition from earlier with the output of this command, you’ll see that the rules, objects, verbs, and resources all match up. For example, the rules section in the manifest specifies the apps apiGroup, the deployments resource, and the verbs get, watch, and list—just as shown in the table.

In summary, understanding how RBAC roles, role bindings, apiGroups, resources, and verbs work together is essential for managing access control in Kubernetes. By carefully defining roles and assigning them to users, you can ensure that only the right people have the right permissions in your cluster.


To refer to all objects in Kubernetes, you can use the asterisk symbol. This binds the verbs to every object exposed by the Kubernetes environment. For example, a rule block that uses asterisks for API groups, resources, and verbs will grant all actions on all resources in every API group. This is essentially giving cluster administrator privileges. However, this is just for demonstration purposes—you would almost never want to do this in a real-world or production-grade Kubernetes cluster.

Cluster roles

So far, we have discussed how to create Roles and RoleBindings. However, Kubernetes actually has four RBAC objects: Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings. Roles and RoleBindings are namespaced objects, which means they can only be applied within a single namespace. In contrast, ClusterRoles and ClusterRoleBindings are cluster-wide objects and apply to all namespaces. All four are defined in the same API subgroup, and their YAML structures are almost identical.

A powerful pattern is to define roles at the cluster level and then bind them to specific namespaces using a RoleBinding. This approach lets you define common roles once and reuse them across multiple namespaces. For example, you can define a read-deployments role as a ClusterRole, which can then be reused in selected namespaces via a regular RoleBinding.

In the YAML manifest for this, the only difference from a namespaced Role is that the kind is set to ClusterRole, and there is no metadata.namespace property. This is because a cluster role is meant to be cluster-wide, so namespacing does not apply.

Existing resources

As you might have guessed, there is a set of predefined roles and bindings that grant permissions to an all-powerful user. Many people also configure kubectl to operate under the context of that user. The following example walks you through the predefined and pre-created user roles and bindings. On a Docker Desktop cluster, the names of pods and RBAC objects may differ from other clusters, but the principles remain the same. This gives you an idea of how things are set up.

Minikube runs the API server in a pod within the kube-system namespace. It uses an authorization flag to tell Kubernetes which authorization modules to use. By running a command to list the pods in the kube-system namespace, you can see the core components of the cluster, such as CoreDNS, etcd, the API server, the controller manager, the proxy, the scheduler, and the storage provisioner. Another command shows that both the Node and RBAC authorization modules are enabled for the API server.

You will not be able to interrogate the API server like this on a hosted Kubernetes cluster, because critical control plane features are hidden from you.

Minikube also updates your kubeconfig files with a cluster called minikube. The configuration file, typically located at home slash dot kube slash config, contains information about the cluster, contexts, users, and their credentials. For example, it specifies the certificate authority, the server address, the current context, and the client certificate and key for the minikube user.

This configuration allows the client—meaning kubectl in this case—to authenticate and communicate with the API server. This is especially important for non-local cluster deployments, as it provides the authorization step. With RBAC enabled and a user called minikube created, you can then look at the cluster roles and cluster role bindings that are pre-configured to grant permissions to that user.

Some of the cluster role bindings are bound to the ClusterRole named cluster-admin. For example, there are bindings called cluster-admin and minikube-rbac, both of which grant the cluster-admin role. Describing the minikube-rbac binding shows that it grants the cluster-admin ClusterRole to the default service account in the kube-system namespace.

As a result of these bindings, all commands in a default on-premise minikube installation are executed with cluster-admin permissions. This might be acceptable for development and on-premise testing environments, but it is certainly not suitable for production.

If you look at the manifest for the cluster-admin ClusterRole, you will see that it allows every action on every resource for every verb. This is normal for that type of environment, as mentioned earlier.

Authorization ensures that already authenticated users are allowed to carry out the actions they are attempting. RBAC is a popular Kubernetes authorization module that implements least privilege access based on a deny-by-default model. In this model, all actions are assumed to be denied unless a rule exists that allows them. The model is similar to a whitelist firewall, where everything is blocked and you open up access by creating allow rules.

Kubernetes RBAC uses Roles and ClusterRoles to create permissions, and it uses RoleBindings and ClusterRoleBindings to grant those permissions to users. In other words, the binding objects serve as a middleman, allowing you to create different combinations of role objects and grant them to users. This approach enables the most granular access and permission model.

Admission control

Admission control runs immediately after successful authentication and authorization. It is all about policies. There are two types of admission controllers: mutating and validating. The names are self-explanatory. Mutating controllers check for compliance and can modify requests, whereas validating controllers check for policy compliance but cannot modify requests.

Mutating controllers always run first, and both types only apply to requests that will modify state. Requests that only read state are not subjected to admission control.


Let’s walk through an example where every new or updated object in your Kubernetes cluster must have the label “env-prod.” A mutating admission controller can check for this label and add it if it’s missing. In contrast, a validating admission controller can only reject the request if the label isn’t present—it cannot add it.

To see which admission controllers are enabled on a Minikube cluster, you can run a command that describes the API server pod and filters for admission plugins. This command reveals that the API server is configured with several admission controllers, including NodeRestriction, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, and others. In real-world clusters, you’ll typically find even more admission controllers enabled.

There are many types of admission controllers. One notable example is AlwaysPullImage. This is a mutating controller that sets the image pull policy for all new pods to “always.” As a result, every time a pod is created, its container images are always pulled from the registry, rather than using any locally cached images. This approach helps prevent the use of potentially malicious cached images, ensures that other pods and processes can’t use outdated or tampered images, and forces the container runtime to present valid credentials to the registry to fetch the image.

It’s important to note that if any admission controller rejects a request, the request is immediately denied, and no further controllers are checked. If all admission controllers approve the request, it is then persisted to the cluster store. As Kubernetes evolves, admission controllers are becoming increasingly important in production environments.

General Summary

The authentication layer in Kubernetes is responsible for validating the identity of each request. Client certificates are commonly used, and integrating with Active Directory or other identity and access management services is recommended for production clusters. Kubernetes itself does not maintain its own identity database—it does not store or manage user accounts or credentials.

Once a user is authenticated, the authorization layer checks whether that user is allowed to perform the requested action. This layer is also pluggable, with Role-Based Access Control, or RBAC, being the most common module. RBAC uses four types of objects to define permissions and assign them to users.

After authentication and authorization, admission control comes into play. Admission controllers enforce policies on incoming requests. Validating admission controllers can reject requests that don’t conform to policy, while mutating admission controllers can modify incoming requests to enhance or override their structure according to certain rules.

This section will also introduce a few quick ways to obtain Kubernetes, and will cover kubectl, the Kubernetes command-line tool.

Kubernetes API

Understanding the Kubernetes API and how it works is essential for mastering Kubernetes. However, the API can be confusing if you’re new to it.

Theory

Kubernetes is fundamentally API-centric. All functionality revolves around interacting with the API server, typically through the kubectl client. Everything in Kubernetes passes through the API server.

Clients send requests to Kubernetes to create, read, update, or delete objects such as pods and services. Most of the time, you’ll use kubectl to send these requests, but you can also craft them in code or use API testing tools. Regardless of how requests are generated, they all go to the API server, where they are authenticated and authorized. If they pass these checks, the requested action is executed on the cluster. For example, if you post a create request, the object is deployed to the cluster and its serialized state is saved in the cluster store, which is typically backed by etcd.

Serialization

Kubernetes serializes objects like pods and services as JSON strings to send over HTTP. This process happens in both directions: clients like kubectl serialize objects when posting to the API server, and the API server serializes responses back to clients. The serialized state of objects is also persisted in the cluster store, usually etcd.

Serialization in Kubernetes means converting an object into a JSON string for transmission over HTTP and for storage in the cluster store. Kubernetes also supports Protobuf as a serialization format. Protobuf is faster and more efficient than JSON, and it scales better, but it’s less user-friendly for introspection and troubleshooting. As of now, Protobuf is mainly used for internal cluster traffic, while JSON is used for communication with external clients.

When clients send requests to the API server, they specify the serialization format they support using the content-type header. For example, a client that only supports JSON will include “Content-Type: application/json” in the HTTP header. Kubernetes will then respond with a serialized JSON response.

The API Server

The API server exposes the Kubernetes API over a secure, RESTful interface using HTTPS. It acts as the central hub for Kubernetes—everything communicates through the API server using REST API calls.

All kubectl commands are sent to the API server, whether you’re creating, retrieving, updating, or deleting objects. All node kubelets watch the API server for new tasks and report status back to it. All control plane services communicate with each other through the API server, not directly.

The API server is a control plane service. It typically runs as a set of pods in the kube-system namespace on the control plane nodes of your cluster. If you manage your own Kubernetes clusters, you need to ensure the control plane is highly available and that the API server has enough resources to remain responsive. In hosted Kubernetes environments, the implementation details, including performance and availability, are managed for you.

The main job of the API server is to make the API available to clients both inside and outside the cluster. It uses TLS to encrypt client connections and leverages various authentication and authorization mechanisms to ensure only valid requests are accepted and processed. All requests, whether internal or external, must pass through the same authentication and authorization checks.

The API is RESTful, which means it follows the principles of Representational State Transfer. This is the standard for modern web APIs, accepting create, read, update, and delete requests via standard HTTPS methods. Each HTTP method—such as PUT, DELETE, UPDATE, and POST—maps to a corresponding action in Kubernetes.

The API server is commonly exposed on port 443 or 6443, but you can configure it to use any port you require. To see the address and port where your Kubernetes cluster’s API server is exposed, you can run a command that displays cluster information. This command will show the control plane and core DNS endpoints, along with their addresses and ports.

RESTful Nature of the API

REST stands for Representational State Transfer, and it’s the de facto standard for communicating with web-based APIs. Systems like Kubernetes that use REST are often called RESTful. REST requests consist of a verb and a path to a resource. The verb represents the action, and the path specifies the resource.

For example, to list all pods in the “shield” namespace, you can use a kubectl command. This command translates to a REST API call that uses the GET verb and a specific path to the pods resource in the shield namespace.

To visualize this, you can start a kubectl proxy, which exposes the API on your local machine and handles authentication. Then, you can use a tool like curl to send a request to the API server through the proxy. The response will include a list of pods in the specified namespace, formatted as a JSON object.

In summary, the Kubernetes API server is the central point of communication for all cluster operations. It handles authentication, authorization, and admission control, and it exposes a RESTful interface for managing all aspects of your cluster. Understanding how to interact with the API server—whether through kubectl, code, or other tools—is fundamental to working effectively with Kubernetes.


The example returned an empty list because there are no pods in that namespace.

When you interact with the Kubernetes API server, the responses include standard HTTP response codes, a content type, and the actual payload. As you learned earlier in the chapter, Kubernetes uses JSON as its preferred content type. So, when you run a kubectl get command, the API server responds with an HTTP 200 OK status code. The content type is set to application slash json, and the payload is serialized as JSON.

For example, if you want to list all pods in the shield namespace, you can use one of the previous curl commands, but add the dash v flag for verbose output. This will show you the headers being sent and received between your client and the API server.

In this case, the curl command makes a GET request to the path slash api slash v1 slash namespaces slash shield slash pods. The response includes a 200 OK status, a content type of application slash json, and a JSON payload. The payload shows a kind of PodList, with an empty items array, indicating there are no pods in that namespace.

The API

The Kubernetes API is where all resources are defined. It is large and modular. When Kubernetes was first created, the API was monolithic, with all resources existing in a single global namespace. However, as Kubernetes grew, it became necessary to divide the API into smaller, more manageable groups. These groups include core, apps, rbac, and networking.

The core

The resources in the core group are mature objects that were created in the early days of Kubernetes, before the API was divided into groups. These are fundamental objects such as Pods, Nodes, Services, Deployments, and Secrets. They are located in the API under slash api slash v1. Now, it should be clear that slash v1 refers to the core API—the original set of resources that Kubernetes started with.

For example, the REST paths for some core resources are as follows. Pods are found at slash api slash v1 slash namespaces slash curly brace namespace curly brace slash pods. Services are at slash api slash v1 slash namespaces slash curly brace namespace curly brace slash services. Nodes are at slash api slash v1 slash nodes. Namespaces are at slash api slash v1 slash namespaces.

Notice that some objects are namespaced and some are not. Namespaced objects have longer REST paths because you must include the namespace segment. For example, listing all pods in the shield namespace uses the path slash api slash v1 slash namespaces slash shield slash pods.

When you make read requests to these paths, you can expect HTTP response codes of either 200 for success, or 401 for unauthorized access.

On the topic of these REST paths, GVR stands for group, version, resource. This is a helpful way to remember the structure of Kubernetes API paths.

Named Groups

Named API groups represent the future of the Kubernetes API. All new resources are added to named groups, which are sometimes called sub-groups. Each named group is a collection of related resources. For example, the apps group contains resources that manage application workloads, such as Deployments, ReplicaSets, DaemonSets, and StatefulSets. The networking dot k8s dot io group contains resources like Ingresses, Ingress Classes, and Network Policies.

There are some exceptions to this pattern. Older resources in the core group, like Pods, would probably be placed in the apps group if they were invented today. Similarly, Services might belong in the networking dot k8s dot io group.

Resources in named groups are found under the path slash apis slash curly brace group name curly brace slash curly brace version curly brace slash resource path. For example, Ingress resources are at slash apis slash networking dot k8s dot io slash v1 slash namespaces slash curly brace namespace curly brace slash ingresses. RoleBindings are at slash apis slash rbac dot authorization dot k8s dot io slash v1 slash namespaces slash curly brace namespace curly brace slash rolebindings. ClusterRoles are at slash apis slash rbac dot authorization dot k8s dot io slash v1 slash clusterroles. StorageClasses are at slash apis slash storage dot k8s dot io slash v1 slash storageclasses.

Notice how the URI paths for named groups start with slash apis and include the group name. This is different from the core group, which starts with slash api in the singular and does not include a group name. In some places, you will see the core API group referred to by empty double quotes. This is because, when the API was first designed, there was no concept of groups—everything was just in the API.

Dividing the API into smaller groups makes it more scalable and easier to navigate. It also makes it easier to extend.

To see API-related information for your cluster, you can use the following commands.

The command kubectl api-resources shows which resources are available on your cluster and which API groups they belong to. It also displays resource short names and whether objects are namespaced or cluster-scoped.

The command kubectl api-versions shows which API versions are supported on your cluster. It does not list which resources belong to which API, but it is useful for finding out whether you have features such as alpha APIs enabled.

For example, running kubectl api-resources will display a table of resources, their short names, API versions, whether they are namespaced, and their kind. This helps you quickly see what is available in your cluster and how resources are organized.

In summary, understanding the structure of the Kubernetes API, including core and named groups, as well as how to discover available resources and versions, is essential for working effectively with Kubernetes.


Let’s begin by looking at how you can inspect the different kinds and versions of API resources supported on your Kubernetes cluster.

When you run the command to list API versions using kubectl, you receive a list of all the API groups and their versions that are available in your cluster. This helps you understand which APIs your cluster supports.

Next, there’s a command that loops through all the resource kinds in your cluster and uses kubectl explain to show their kind and version. The output highlights the kind of resource, such as Namespace or Node, and the API version it belongs to, like v1 or autoscaling/v1. This is useful for quickly mapping resources to their API versions, which can help with debugging or understanding the source of each resource.

These commands can be run on both self-hosted and cloud-hosted Kubernetes clusters. They are especially helpful for troubleshooting or gathering information about resources and their associated APIs.

Thread modeling

Thread modeling is the process of identifying vulnerabilities so you can put measures in place to prevent and mitigate them. In this section, we’ll introduce the popular STRIDE model and show how it can be applied to Kubernetes.

STRIDE defines six categories of potential threats. The word STRIDE is an abbreviation, standing for Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, and Elevation of privilege.

While the model is useful, it’s important to remember that it’s just a model. It doesn’t guarantee coverage of every possible threat, but it does provide a structured way to analyze security risks. For the rest of this section, we’ll look at each of the six threat categories in turn, and discuss how to prevent and mitigate them.

In twenty nineteen, the Cloud Native Computing Foundation, or CNCF, commissioned a third-party security audit of Kubernetes. The audit included threat modeling, manual code reviews, dynamic penetration testing, and cryptography review. All findings were assigned a difficulty and severity level, with high-severity issues receiving special attention.

Spoofing

Spoofing is when someone pretends to be somebody else in order to gain extra privileges on a system. Kubernetes is made up of many small components that work together. These include control plane services such as the API server, controller manager, scheduler, and cluster store, as well as node components like the kubelet and container runtime. Each component has its own set of privileges that allow it to interact with and modify the cluster. Even though Kubernetes implements a least-privilege model, spoofing the identity of any component can cause serious problems.

If you’ve read the sections on RBAC and API security, you’ll know that Kubernetes requires all components to authenticate using cryptographically signed certificates. This is a good practice, and Kubernetes makes it easy to auto-rotate certificates. However, there are some important considerations.

First, a typical Kubernetes installation will auto-generate a self-signed certificate authority, or CA. This CA issues certificates to all cluster components. While this is better than nothing, it may not be sufficient for production environments.

Second, mutual TLS is only as secure as the CA that issues the certificates. If the CA is compromised, the entire mutual TLS layer becomes ineffective. Therefore, it’s critical to keep the CA secure.

A good practice is to ensure that certificates issued by the internal Kubernetes CA are only used and trusted within the Kubernetes cluster. This means you should carefully approve certificate signing requests, and make sure the Kubernetes CA is not added as a trusted CA for any system outside of Kubernetes.

As mentioned earlier, all internal and external requests to the API server are subject to authentication and authorization checks. The API server needs a way to authenticate both internal and external sources. A good approach is to use two trusted key pairs: one for authenticating internal systems, and another for authenticating external systems. In this model, you use the cluster’s self-signed CA to issue keys to internal systems, and configure Kubernetes to trust one or more third-party CAs for external systems.

Besides spoofing access to the cluster, there’s also the risk of spoofing in app-to-app communications. This is when one pod pretends to be another. Fortunately, you can use Kubernetes Secrets to mount certificates into pods, which are then used to authenticate pod identity.

Every pod in Kubernetes has an associated service account that provides its identity within the cluster. This is done by automatically mounting a service account token into every pod as a secret. There are two important points to note here. First, the service account token allows access to the API server. Second, most pods probably do not need to access the API server.

With these points in mind, it’s often recommended to set the automountServiceAccountToken option to false for pods that do not need to communicate with the API server. This can be done in the pod manifest by specifying the service account name and setting automountServiceAccountToken to false. This configuration prevents the service account token from being automatically mounted into the pod.

If a pod does need to talk to the API server, there are some non-default configurations worth exploring. For example, you can set expirationSeconds and audience. These options let you specify when the token will expire and restrict which entities the token works with. In the example inspired by the official Kubernetes documentation, a pod is configured to mount a service account token with an expiry period of one hour, and the token is restricted to the “vault” audience. This is done using a projected volume in the pod specification.

Tampering

Tampering is the act of maliciously changing something to cause denial of service or elevation of privilege. Tampering can be hard to avoid, so a common countermeasure is to make it obvious when something has been tampered with. For example, over-the-counter drugs are often packaged with tamper-proof seals, making it easy to see if the product has been altered.

Let’s look at some of the cluster components that can be tampered with. All of the following Kubernetes components, if tampered with, can cause harm or issues: etcd, configuration files, container runtime binaries, container images, Kubernetes binaries, and more.

Generally, tampering happens either in transit or at rest. In transit refers to data being transmitted over the network, while at rest refers to data stored in memory or on disk. TLS is a great tool for protecting against tampering in transit, as it provides built-in integrity guarantees and will warn you if data has been altered.

To prevent tampering with data at rest in Kubernetes, consider the following recommendations. Restrict access to the servers running Kubernetes components, especially the control plane. Limit access to repositories that store Kubernetes configuration files. Only perform remote bootstrapping over secure shell, or SSH. Restrict access to your image repository and associated repositories.

This is not an exhaustive list, but implementing these measures will greatly reduce the chances of your data being tampered with while at rest. Additionally, it’s good production hygiene to configure auditing and alerting for important binaries and configuration files. If set up and monitored correctly, these can help detect potential tampering attacks.

For example, you can use the Linux audit daemon to monitor access to the Docker binary and to audit attempts to change the binary file’s attributes. This is done by configuring the audit daemon to watch the Docker binary for write, execute, and attribute changes, and to tag these events with a specific key for easy identification.

Kubernetes applications

Let’s continue by looking at how tampering can affect Kubernetes applications.


As well as infrastructure components, application components are also potential targets for tampering. One effective way to prevent a live Pod from being tampered with is to set its filesystems to read-only. This approach guarantees filesystem immutability and can be accomplished either through a Pod Security Policy or by using the securityContext section of a Pod manifest file.

You can make a container’s root filesystem read-only by setting the readOnlyFilesystem property. As mentioned earlier, this can be configured via a PodSecurityPolicy object or directly in Pod manifest files. The same principle applies to other filesystems that are mounted into containers, which can be controlled using the allowedHostPaths property.

For example, a YAML manifest can use both settings in a Pod specification. The allowedHostPaths section ensures that anything mounted beneath the directory slash test will be read-only. There are two ways to represent this functionality: one is directly in the Pod manifest, and the other is as a separate object using PodSecurityPolicy. PodSecurityPolicy is relatively new in the Kubernetes specification, which is why it is not under the stable version one API, but rather under a new beta version path, policy slash v1beta1. In the future, when the specification is finalized, it will move to the policy slash v1 API version.

In summary, the first YAML block defines a Pod with a read-only root filesystem and restricts host path mounts under slash test to be read-only. The second block defines a PodSecurityPolicy object with the same restrictions, but uses the beta API version.

Repudiation

At a very high level, repudiation means creating doubt about something, while non-repudiation is about providing proof. In the context of information security, non-repudiation is the ability to prove that certain actions were carried out by specific individuals.

Looking deeper, non-repudiation includes the ability to provide answers to several key questions: What happened, when it happened, who made it happen, where it happened, why it happened, and how it happened. Answering the last two questions—why and how—usually requires correlating several events over a period of time. Fortunately, auditing Kubernetes API server events can usually help answer these questions.

For example, an API server audit event might include details such as the kind of event, the API version, metadata like the creation timestamp, the level of detail, the timestamp, a unique audit ID, the stage of the request, the request URI, the verb used, the user’s identity, source IP addresses, object references, and timestamps for when the request was received and processed.

This audit event shows who performed an action, what resource was affected, and when it occurred. You may need to manually enable auditing on your API server to collect such events.

Although the API server is central to most operations in Kubernetes, it is not the only component that requires auditing for non-repudiation. At a minimum, you should collect audit logs from container runtimes, kubelets, and the applications running on your cluster. This is in addition to network firewalls and similar infrastructure.

Once you start auditing multiple components, you quickly need a centralized location to store and correlate events. A common way to achieve this is by deploying an agent to all nodes using a DaemonSet. The agent collects logs from the runtime, kubelet, and applications. If you take this approach, it is vital that the centralized log store is secure. If the security of the log store is compromised, you can no longer trust the logs, and their contents can be repudiated.

To provide non-repudiation related to tampering with binaries and configuration files, it can be useful to use an audit daemon that watches for write actions on certain files and directories on your Kubernetes masters and nodes. For example, earlier in the chapter, there was an example that enabled auditing of changes to the Docker binary. With this enabled, starting a new container with the Docker run command will generate an audit event that records details such as the system call, process IDs, user IDs, the command executed, and the file paths involved.

When audit logs like these are combined and correlated with Kubernetes audit features, they create a comprehensive and trustworthy picture that cannot be repudiated.

Information Disclosure

Information disclosure occurs when sensitive data is leaked. There are many ways this can happen, including compromised data stores and APIs that unintentionally expose sensitive data.

Protecting cluster data

In Kubernetes, the entire configuration of the cluster is stored in the cluster store, which is usually etcd. This includes network and storage configuration, as well as passwords and other sensitive data stored in Secrets. For obvious reasons, the cluster store is a prime target for information disclosure attacks.

At a minimum, you should limit and audit access to the nodes hosting the cluster store. As will be discussed in the next paragraph, gaining access to a cluster node can allow a logged-on user to bypass some of the security layers.

Kubernetes version one point seven introduced encryption of Secrets, but this is not enabled by default. Even when encryption becomes the default, the data encryption key, or DEK, is stored on the same node as the Secret. This means that gaining access to a node allows an attacker to bypass encryption completely. This is especially concerning on nodes that host the cluster store, such as etcd nodes.

Fortunately, Kubernetes version one point eleven enabled a beta feature that lets you store key encryption keys, or KEKs, outside of your Kubernetes cluster. These keys are used to encrypt and decrypt data encryption keys and should be safely guarded. You should seriously consider using hardware security modules, or HSMs, or cloud-based key management stores, or KMS, for storing your encryption keys. Keep an eye on upcoming versions of Kubernetes for further improvements to the handling of Secrets.

Protecting data in Pods

As previously mentioned, Kubernetes has an API resource called a Secret, which is the preferred way to store and share sensitive data such as passwords. For example, a front-end container accessing an encrypted back-end database can have the key to decrypt the database mounted as a Secret. This is a much better solution than storing decryption keys in a plain text file or environment variable.

It is also common to store data and configuration information outside of Pods and containers in persistent volumes and config maps. If the data on these resources is encrypted, the keys for decrypting them should also be stored in Secrets.

With all of this in mind, it is vital that you consider the caveats outlined in the previous section regarding Secrets and how their encryption keys are stored. You do not want to do the hard work of locking the house, but then leave the key in the door.

Denial of Service

Denial of Service, or DoS, is all about making something unavailable. There are many types of DoS attacks, but a well-known variation is overloading a system to the point where it can no longer service requests.

In the Kubernetes world, a potential attack might be to overload the API server so that cluster operations grind to a halt. Even essential system services have to communicate via the API server.

Protecting cluster resources


It is a time-honored best practice to replicate essential control plane services across multiple nodes to achieve high availability. Kubernetes follows this principle as well. For your production environments, you should run multiple master nodes in a high-availability configuration. This approach prevents a single master node from becoming a single point of failure.

When it comes to certain types of denial of service attacks, having multiple masters means an attacker would need to compromise more than one master node to have a significant impact. Additionally, you should consider distributing control plane nodes across different availability zones. This strategy can help prevent a denial of service attack targeting the network of a specific availability zone from taking down your entire control plane.

The same principle applies to worker nodes. Having multiple worker nodes allows the scheduler to distribute your application across several nodes and availability zones. This setup can also make denial of service attacks on a single node or zone less effective, or even ineffective.

You should also configure appropriate limits for key resources, including memory, processor, storage, and Kubernetes objects. Setting these limits helps prevent critical system resources from being exhausted, which in turn can help prevent potential denial of service attacks. Limiting Kubernetes objects involves restricting the number of items such as ReplicaSets, Pods, Services, Secrets, and ConfigMaps within a particular namespace.

For example, you can use a manifest file to limit the number of Pod objects in the namespace called “skippy” to one hundred. This is done by defining a ResourceQuota with a hard limit on pods.

There is another feature called podPidsLimit, which restricts the number of processes a Pod can create. Imagine a scenario where a Pod is targeted by a fork bomb attack—a specialized attack where a rogue process creates as many new processes as possible to consume all system resources and bring the system to a halt. By placing a limit on the number of processes a Pod can create, you prevent the Pod from exhausting the node’s resources and confine the impact of the attack to that Pod. Once the podPidsLimit is reached, the Pod will typically be restarted.

Protecting the API server

The API server exposes a RESTful interface over a TCP socket, making it susceptible to botnet-based denial of service attacks. The following measures can help prevent or mitigate such attacks.

First, run highly available masters by having multiple API server replicas on different nodes and across multiple availability zones. Second, monitor and set up alerts for API server requests based on reasonable thresholds. Third, use firewalls or similar network controls to limit the API server’s exposure to the internet.

In addition to botnet denial of service attacks, an attacker might attempt to spoof a user or another control plane service to cause an overload. Fortunately, Kubernetes provides robust authentication and authorization controls to prevent spoofing. However, even with a strong role-based access control model, it is crucial to safeguard access to accounts with high privileges.

Protecting the cluster store

Cluster configuration is stored in etcd, making it essential that etcd remains available and secure. The following recommendations help achieve this.

Configure a highly available etcd cluster with either three or five nodes. Set up monitoring and alerting for requests to etcd. Isolate etcd at the network level so that only control plane members can interact with it.

By default, Kubernetes installs etcd on the same servers as the rest of the control plane. This setup is usually acceptable for development and testing. However, large production clusters should seriously consider running a dedicated etcd cluster. This approach provides better performance and greater resilience.

From a performance perspective, etcd is often the most common bottleneck for large Kubernetes clusters. Therefore, you should test to ensure that the infrastructure running etcd can sustain performance at scale. A poorly performing etcd can be just as problematic as an etcd cluster under sustained attack.

Operating a dedicated etcd cluster also adds resilience by protecting it from other parts of the control plane that might be compromised. Monitoring and alerting for etcd should be based on reasonable thresholds, and a good starting point is to monitor etcd log entries.

Protecting application components

Most Pods expose their main service on the network. Without additional controls, anyone with network access can perform a denial of service attack on the Pod. Fortunately, Kubernetes provides Pod resource requests and limits to prevent such attacks from exhausting Pod and Node resources. In addition to these, the following practices are helpful.

Define Kubernetes Network Policies to restrict Pod-to-Pod and Pod-to-external communications. Use mutual TLS and API token-based authentication for application-level authentication, and reject any unauthenticated requests.

Elevation of privilege

Privilege escalation refers to gaining higher access than what is normally granted, often to cause damage or gain unauthorized access. Let’s look at a few ways to prevent this in a Kubernetes environment.

Protecting the API server

Kubernetes offers several authorization modes to safeguard access to the API server. These include role-based access control, webhook authorization, and node authorization. You should run multiple authorizers at the same time. For example, a common best practice is to always have both role-based access control and node authorization enabled.

Role-based access control allows you to restrict API operations to specific subsets of users. These users can be regular user accounts or system services. The idea is that all requests to the API server must be both authenticated and authorized. Authentication ensures that requests come from a validated user, while authorization ensures that the validated user is allowed to perform the requested operation.

For example, consider the question: Can Lily create Pods? In this scenario, Lily is the user, “create” is the operation, and Pods is the resource. Authentication verifies that it is really Lily making the request, and authorization determines if she is allowed to create Pods.

Webhook mode allows you to offload authorization to an external REST-based policy engine. However, this requires additional effort to build and maintain the external engine. It also introduces the risk of making the external engine a single point of failure for every request to the API server. For instance, if the external webhook system becomes unavailable, you may not be able to make any requests to the API server. Therefore, you should be rigorous in vetting and implementing any webhook authorization service.

Protecting Pods

The next sections will discuss several technologies that help reduce the risk of privilege escalation attacks against Pods and containers. We will look at preventing processes from running as root, dropping capabilities, and filtering system calls.

Root processes

The root user is the most powerful user on a Linux system and always has user ID zero. Running application processes as root is almost always a bad idea, as it grants the application process full access to the container. This risk is made even worse by the fact that the root user in a container often has unrestricted root access on the host system as well.

Fortunately, Kubernetes allows you to force container processes to run as unprivileged, non-root users. For example, you can configure a Pod so that all containers within it run processes as user ID one thousand. If the Pod has multiple containers, all processes in all containers will run as user ID one thousand.

The runAsUser setting is one of many options that can be configured as part of what is known as PodSecurityPolicy. It is possible for two or more Pods to be configured with the same runAsUser value. When this happens, containers from both Pods will run with the same security context and may have access to the same resources. This might be acceptable if they are replicas of the same Pod or container. However, it can cause problems if they are different containers. For example, two different containers with read and write access to the same host directory or volume can cause data corruption if both write to the same dataset without coordinating their operations. Shared security contexts also increase the risk of a compromised container tampering with data it should not have access to.

With this in mind, it is possible to use the securityContext.runAsUser property at the container level instead of the Pod level.


This example sets the user ID, or UID, to one thousand at the pod level, but then overrides it at the container level so that processes in one particular container run as UID two thousand. Unless otherwise specified, all other containers in the pod will use UID one thousand.

A couple of other strategies that might help address the issue of multiple pods and containers using the same UID include enabling user namespaces and maintaining a map of UID usage.

User namespaces are a Linux kernel technology that allows a process to run as root within a container, but as a different user outside of the container. For example, a process can run as UID zero, which is the root user, inside the container, but be mapped to UID one thousand on the host. This can be a good solution for processes that need to run as root inside the container. However, you should check that your version of Kubernetes and your container runtime fully support user namespaces.

Maintaining a map of UID usage is a more manual and less elegant way to prevent multiple different pods and containers from using overlapping UIDs. It is a bit of a hack and requires strict adherence to a gated release process for deploying pods into production.

Drop capabilities

While user namespaces allow container processes to run as root inside the container but not on the host machine, the reality is that most processes do not really need to run as full root inside the container. However, it is also true that many processes require more privileges than a typical non-root user. What is needed is a way to grant the exact set of privileges a process requires in order to run. This is where capabilities come in.

Let’s take a moment for some theory and background. The root user is the most powerful user on a Linux system, but its power is actually a combination of many small privileges called capabilities. For example, the SYS_TIME capability allows a user to set the system clock, while the NET_ADMIN capability allows a user to perform network-related operations, such as modifying the local routing table and configuring local interfaces. The root user holds every capability and is therefore extremely powerful.

Having a modular set of capabilities like this allows you to be extremely granular when granting permissions. Instead of an all-or-nothing approach—root or non-root—you can grant a process the exact set of capabilities it needs to run.

There are currently over thirty capabilities, and choosing the right ones can be daunting. With this in mind, the default Docker runtime drops over half of them by default. This is a sensible default designed to allow most processes to run, without leaving the keys in the front door.

While these sensible defaults are better than nothing, they are often not good enough for many production environments. A common way to find the absolute minimum set of capabilities an app requires is to run it in a test environment with all capabilities dropped. This will cause the app to fail and log messages about the missing permissions. You then map those permissions to capabilities, add them to the app’s pod specification, and run the app again. You repeat this process until the app runs properly with the minimum set of capabilities.

As good as this approach is, there are a few things to consider. First, you must perform extensive testing of your app. The last thing you want is a production edge case that you had not accounted for in your test environment, which could crash your app in production. Second, every fix and update to your app requires the same extensive testing against the capability set. With these considerations in mind, it is vital that you have testing procedures and a production release process that can handle all of this.

By default, Kubernetes implements the default set of capabilities provided by your chosen container runtime. However, you can override this in a pod security policy or as part of the container’s security context.

In the next example, a pod manifest is shown where the container’s security context adds the NET_ADMIN and CHOWN capabilities. This means the container will be able to perform network administration tasks and change file ownership, in addition to the default capabilities.

Filter syscalls

Seccomp, which stands for secure computing, is similar in concept to capabilities but works by filtering system calls, or syscalls, rather than capabilities. The way a Linux process asks the kernel to perform an operation is by issuing a syscall. Seccomp lets you control which syscalls a particular container can make to the host kernel.

As with capabilities, a least-privilege model is preferred, where the only syscalls a container is allowed to make are those it needs in order to function and run properly.

Seccomp became generally available in Kubernetes version one point nineteen and can be used in different ways based on the following seccomp profiles.

First, there is the non-blocking profile. This allows a pod to run and records every syscall it makes to an audit log, which you can use to create a custom profile. The idea is to run your app or pod in a development or test environment and make it do everything it is designed to do. When you are done, you will have a log file listing every syscall the pod needs in order to run. You then use this to create a custom profile that only allows the syscalls the app needs, following the least-privilege principle.

Second, there is the blocking profile. This blocks all syscalls. It is extremely secure, but prevents the pod from doing anything useful.

Third, there is the runtime profile. This forces a pod to use the seccomp profile defined by its container runtime. This is a common place to start if you have not created a custom profile yet. Profiles that ship with container runtimes, like Docker and containerd, are not the most secure in the world, but they are not wide open either. They are usually designed to be a balance of usable and secure, and they are thoroughly tested.

Fourth, there is the custom profile. This is a profile that only allows the syscalls your app needs in order to run. Everything else is blocked. It is common to extensively test your app in development or test environments with a non-blocking profile that records all syscalls to a log. You then use this log to identify the syscalls your app makes and build the customized profile. The danger with this approach is that your app may have some edge case that you miss during testing. If this happens, your app can fail in production when it hits an edge case and uses a syscall not captured in the logs during testing.

Privilege escalation

The only way to create a new process in Linux is for one process to clone itself and then load new instructions into the new process. This is a simplified explanation, but the original process is called the parent process and the copy is the child. This is called process forking.

By default, Linux allows a child process to claim more privileges than its parent. This is usually a bad idea. In fact, you will often want a child process to have the same or fewer privileges than its parent. This is especially true for containers, as their security configurations are defined against their initial configuration and not against potentially escalated privileges.

Fortunately, it is possible to prevent privilege escalation through a pod security policy or the security context property of an individual container.

Pod security policies

As you have seen throughout these sections, you can enable security settings on a per-pod basis by setting security context attributes in individual pod manifest files. However, this approach does not scale well. It requires developers and operators to remember to do this for every pod manifest specification, and it is prone to errors.

Pod security policies offer a better way. Pod security policies allow you to define security settings at the cluster level. You can then apply them to a targeted set of pods as part of the deployment process. This approach scales better, requires less effort from developers and admins, and is less prone to error. It also lends itself to situations where you have a team dedicated to securing apps in production.

Pod security policies are implemented as an admission controller. In order to use them, a pod’s service account must be authorized to use the policy. Once this is done, policies are applied to new requests to create pods as they pass through the API admission chain.

Real world security

In the previous sections, we described how to threat model Kubernetes security using the STRIDE model. In this section, we will cover common security-related challenges that are likely to be encountered in the real world when implementing a Kubernetes cluster.


While every Kubernetes deployment is unique, there are many common patterns and practices. The examples discussed here will apply to most Kubernetes environments, whether they are large or small.

Rather than providing step-by-step, cookbook-style solutions, this discussion takes a high-level perspective—similar to the approach a security architect would use. The section is organized into several key areas: the CI and CD pipeline, infrastructure and networking, identity and access management, and finally, security monitoring and auditing.

Let’s begin with the CI and CD pipeline.

Containers have revolutionized how applications are packaged and run. When it comes to packaging, containers bundle an application’s code together with its dependencies into a single image. These images also include the commands needed to launch the application. This approach has greatly simplified building, sharing, and running applications, and it has helped solve the classic problem of “it worked on my laptop, but not in production.”

However, containers also make it easier than ever to run potentially dangerous code. With this in mind, let’s explore some strategies for securing the flow of application code as it moves from a developer’s laptop to production servers.

Now, let’s look at image repositories.


