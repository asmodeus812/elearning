Introduction

Containers are transforming the way everyone in the IT industry works. At first, containers appeared on developers’ laptops, helping them build applications more quickly than they could with virtual machines or by configuring their laptop’s operating system. As containers became more common in development environments, their use expanded. What started on laptops and in small development labs soon moved into the enterprise. Within just a few years, containers reached the point where they now power massive production workloads, such as those at GitHub.

Container platform

A container platform is an application platform that uses containers to build, deploy, serve, and orchestrate the applications running inside it. OpenShift relies on two main tools to serve applications in containers. First, it uses a container runtime to create containers on Linux. Second, it uses an orchestration engine to manage a cluster of servers, also called nodes. These servers can be physical machines, virtual machines, or even Internet of Things devices, all running containers.

Containers in OpenShift

A container runtime operates on a Linux server to create and manage containers. To understand this, let’s look at how containers function on a Linux system. In later sections, we’ll explore in detail how containers isolate applications in OpenShift. For now, think of containers as discrete, portable, and scalable units for applications. Each container holds everything required for the application inside it to function. Every time a container is deployed, it includes all the libraries and code needed for its application to work properly.

Applications running inside a container can only access the resources within that container. They are isolated from anything running in other containers or on the host system. Containers isolate five main types of resources: mounted filesystems, shared memory resources, hostnames and domain names, network resources such as IP addresses and MAC addresses, and process counters. We’ll examine each of these in detail in the following sections.

In OpenShift, the service responsible for creating and managing containers is Docker. Docker is a large, active open source project started by Docker Incorporated. The resources Docker uses to isolate processes in containers are all part of the Linux kernel. These include technologies like SELinux, Linux namespaces, and control groups, also known as cgroups. We’ll cover these in detail later.

Docker has made these resources much easier to use and has added several features that have contributed to its popularity and growth. Some of the main benefits of Docker as a container runtime include portability, image reuse, an application-centric API, and a thriving ecosystem. Earlier attempts at container formats were not portable between hosts running different operating systems, but Docker’s container format is now standardized as part of the Open Container Initiative. Any container image can be reused as the base for other images. The API and command line tools allow developers to quickly create, update, and delete containers. This is reflected in both the Docker engine API and the Kubernetes API. Docker Incorporated also maintains a free public hosting environment for container images, which now contains several hundred thousand images.

Orchestrating Containers

While the Docker engine manages containers by using Linux kernel resources, it is limited to a single host operating system. Running containers on a single server is interesting, but it’s not enough to create robust applications. To deploy highly available and scalable applications, you need to run application containers across multiple servers.

To orchestrate containers across multiple servers, you need a container orchestration engine. This is an application that manages a container runtime across a cluster of hosts, providing a scalable application platform. OpenShift uses Kubernetes as its container orchestration engine. Kubernetes is an open source project started by Google and donated to the Cloud Native Computing Foundation in twenty fifteen.

Kubernetes uses a master and node architecture. The master servers maintain information about the server cluster, while the nodes run the actual application workloads. Kubernetes is a vibrant open source project with a rapidly growing and active community. It is consistently one of the most active projects on GitHub.

However, to unlock the full power of a container platform, you need a few additional components. This is where OpenShift comes in. It uses Docker or Containerd and Kubernetes as its foundation, but adds more tools to provide a better experience for users.

Examining the architecture

OpenShift builds on the Kubernetes master and node architecture. From there, it adds additional services that a good application platform should include out of the box.

Integrating container images

In a container platform like OpenShift, container images are created when applications are deployed or updated. For this to be effective, the container image must be available quickly on all the application nodes in a cluster. OpenShift includes an integrated image registry as part of its default configuration. An image registry is a central location that can serve container images to multiple locations. In OpenShift, the integrated registry itself runs in a container.

In addition to providing tightly integrated image access, OpenShift also works to make access to applications more efficient.

Accessing applications

In Kubernetes, containers are created on nodes using components called pods. There are some distinctions, which we’ll discuss in more depth later, but they are often similar. When an application consists of more than one pod, access to the application is managed through a component called a service. A service acts as a proxy, connecting multiple pods and mapping them to an IP address on one or more nodes in the cluster.

Managing IP addresses can be challenging, especially when they are behind a firewall. OpenShift addresses this by providing an integrated routing layer. This routing layer is a software load balancer. When an application is deployed in OpenShift, a DNS entry is created for it automatically. That DNS record is added to the load balancer, which then interfaces with the Kubernetes service to efficiently handle connections between the deployed applications and their users.

With applications running in pods across multiple nodes, and management coming from the master node, there is a lot of communication between servers in an OpenShift cluster. It’s important to ensure that traffic is properly encrypted and can be separated when needed.

Handling network traffic

OpenShift uses a software-defined networking solution to encrypt and shape network traffic within a cluster. The default solution is Open vSwitch, but other software-defined networking solutions are also supported. We’ll explore this in more detail in future sections.

Now that you have a good understanding of how OpenShift is designed, let’s look at the life cycle of an application in an OpenShift cluster.

Examining an application

OpenShift provides workflows to help you manage your applications through all phases of their lifecycle: building, deploying, upgrading, and retiring.

Building an application

The primary way to build an application is to use a builder image. This is the default workflow in OpenShift, and it’s what you’ll use in the next section to deploy your first application. A builder image is a special container image that includes the applications and libraries needed for a specific programming language.

For example, in the next section, you’ll deploy a PHP web application. The builder image for this deployment includes the Apache web server and the PHP language libraries—everything needed to run this type of application. The build process takes the source code for your application and combines it with the builder image to create a custom application image. This custom image is stored in the integrated registry, ready to be deployed and served to your application’s users.

Deploying and serving applications


In the default workflow for OpenShift applications, deployment is automatically triggered after the container image is built and available. The deployment process takes this newly created application image and deploys it across one or more nodes. Along with the application pods, a service is also created, as well as a DNS route in the routing layer. Once all these components are in place, users can access the new application through the routing layer.

When it comes to application upgrades, the same workflow applies. An upgrade triggers the creation of a new container image, and the new version of the application is deployed. There are several upgrade processes available, which we will explore in future sections.

That’s a high-level overview of how OpenShift works. We’ll dive much deeper into all of these components and mechanisms as we move forward. Now that we have a foundational understanding of OpenShift, let’s discuss some of the strengths and limitations of container platforms.

Use case for platforms

The technology behind OpenShift is impressive, but it’s important to connect any new technology to tangible benefits for your organization. In this section, we’ll look at some of the advantages that OpenShift can offer.

Technology use cases

If you pause to consider the history of major IT innovations, you’ll notice a pattern: each step forward has been about achieving more efficient process isolation. It began with mainframes, then moved to the client-server model and the x86 revolution, which improved application isolation. This was followed by the virtualization revolution, where multiple virtual machines could run on a single physical server. This allowed administrators to increase density in their data centers while still keeping processes isolated from one another.

With virtual machines, each process is isolated in its own virtual machine. However, each virtual machine requires a full operating system and kernel, along with all the necessary file systems. This means each virtual machine must be patched, managed, and treated like traditional infrastructure.

Containers represent the next step in this evolution. An application container includes everything the application needs to run: the source code, libraries, configurations, and information about connecting to shared data sources. What containers do not include is just as important. Unlike virtual machines, all containers on a host share a single kernel. Application isolation is achieved using components within the kernel itself.

Because containers don’t need to include a full operating system and kernel, they are much smaller than virtual machines. For example, while a typical virtual machine might require a disk size of ten gigabytes or more, a container image could be as small as one hundred megabytes.

This smaller size brings several advantages. First, portability is improved. Moving a one hundred megabyte container from one server to another is much easier than transferring multi-gigabyte virtual machine images. Second, container startup is much faster, since it doesn’t involve booting an entire operating system. Starting a container typically takes milliseconds, compared to seconds or even minutes for virtual machines.

Businesses use cases

Modern business solutions must deliver time or resource savings as part of their design. Today’s solutions need to use both human and computer resources more efficiently than in the past. The ability of containers to enable these savings is a major reason for their rapid adoption.

If you compare a server using virtual machines for process isolation to one using containers, you’ll notice some key differences. Containers use server resources more efficiently. Since all containers on a host share a single kernel, rather than each virtual machine running its own, more of the server’s resources are dedicated to running applications instead of supporting platform overhead.

Application density also increases with containers. Because containers are much smaller than virtual machines, more applications can fit on a single server. This means you can run more applications with fewer servers.

Invalid use cases

While an ever-increasing number of workloads are a good fit for containers, there are still situations where containers are not the best solution. The container revolution began with web applications, but now includes command-line tools, desktop tools, and even relational databases. However, if you have a complex legacy application, you should be cautious about breaking it down and converting it to containers. For example, if an application will only be around for eighteen months, and it would take nine months to properly containerize it, it may be better to leave it as is.

Containers were designed for enterprise IT environments and work well with most enterprise-grade storage and network solutions, but not all. Some applications are inherently large and resource-intensive, such as software for human resources departments or very large relational databases. If a single application requires multiple servers on its own, running it in a container that is designed to share resources with other applications doesn’t make sense.

Container storage

Containers are a revolutionary technology, but they can’t do everything. Storage is one area where containers need to be paired with other solutions to support production-ready applications. This is because the storage created when a container is deployed is ephemeral. If a container is destroyed or replaced, its internal storage is not reused. This design allows containers to be stateless by default. If something goes wrong, a container can be removed from your environment entirely, and a new one can be started instantly.

While stateless application containers are useful, most applications need to share data across multiple containers and preserve state. For example, you might need to share uploaded images across containers in a web application, maintain state information so users can resume long-running transactions, or store information in relational or non-relational databases.

In all these cases, you need persistent storage for your containers. This storage should be defined as part of your application deployment and accessible from all nodes in your OpenShift cluster. Fortunately, OpenShift offers several ways to address this need. In future sections, we’ll configure an external network storage service and set it up to work with OpenShift, so applications can dynamically allocate and use persistent storage volumes.

When you successfully integrate shared storage with your application containers, you can approach scalability in new ways.

Scaling applications

For stateless applications, scaling up and down is straightforward. Since there are no dependencies beyond what’s inside the application container, and transactions are atomic by design, scaling simply involves deploying more instances and load balancing them together.

OpenShift makes this process even easier by proxying connections to each application through a built-in load balancer called HAProxy. This allows applications to scale up and down without changing how users connect to them.

If your applications are stateful—meaning they need to store or retrieve shared data, such as databases or user-uploaded files—you must provide persistent storage. This storage needs to scale automatically with your application in OpenShift. For stateful applications, persistent storage is a critical component that must be tightly integrated into your design. Ultimately, stateful pods are how users get data in and out of your application.

Integrating stateful and stateless apps

This is where the text ends. In the next sections, we’ll explore how to effectively combine stateful and stateless applications within OpenShift, ensuring both scalability and reliability for your workloads.


As you begin separating traditional monolithic applications into smaller services that work effectively in containers, you will start to view your data needs in a different way. This process is often called designing applications as microservices.

For any application, you will have some services that need to be stateful, and others that can be stateless. For example, the service that provides static web content can be stateless, while the service that processes user authentication needs to write information to persistent storage. All of these services work together to form your application.

Because each service runs in its own container, you can scale the services up and down independently. Instead of scaling your entire codebase with containers, you only need to scale the services in your application that require additional processing power. Additionally, since only the containers that need access to persistent storage have it, your data is more secure.

That brings us to the end of our initial walkthrough. The benefits provided by OpenShift save time for people and use server resources more efficiently. The way containers work also provides improved scalability and faster deployment compared to virtual machines. All of this combines to create a powerful application platform that you will continue to work with throughout this reading.

Starting

There are three main ways to interact with OpenShift: the command line, the web interface, and the REST API. This chapter focuses on deploying applications using the command line, because the command line exposes more of the process used to create containerized applications in OpenShift. In other sections, examples may use the web interface or even the API. Our goal is to give you the most real-world examples of using OpenShift, and to show you the best tools for the job. We will also try not to make you repeat yourself unnecessarily.

Almost every action in OpenShift can be performed using all three access methods. If something is limited, we will let you know. We want you to have the best experience possible with OpenShift. With that said, in this section we are going to repeat ourselves, but for a good reason. The most common task in OpenShift is deploying an application. Because this is so common, we need to introduce it as early as possible using both the command line and the web interface. So please bear with us if this section seems a little repetitive.

Cluster runtimes

Before you can start using OpenShift, you have to deploy it. There are a few options: you can install OpenShift locally on your machine, or on cloud providers. We are going to focus on tools like Minishift or Red Hat’s CodeReady Containers, also known as CRC.

Logging in to OpenShift is required, as every action needs authentication. This allows every action to be governed by the security and access rules set up for all users in an OpenShift cluster. We will discuss the various methods of managing authentication in the next section, but by default, your OpenShift cluster’s initial configuration is set to allow any user and password. The “allow all” identity provider creates a user account the first time a user logs in. Each username is unique, and the password can be anything except an empty field. This configuration is safe and recommended only for lab and development OpenShift instances, like the one we are setting up.

The oc command line tool is a user-facing program used to interact with the REST server of the running OpenShift cluster. In this case, the actual OpenShift server is backed by the Kubernetes REST API server under the hood, but those are implementation details. OpenShift builds on top of the Kubernetes REST API server to provide more flexibility and robustness. What you need to remember is that you must first use the login command to perform the login action before any other command can be executed with the oc tool.

The login command sets the user and password, and points the oc tool to a valid running cluster REST API server. These three fields are mandatory; otherwise, the tool would not know who to authenticate, or how and where to do so.

To obtain the list of configured credentials on the cluster locally, you can use a command that lists every user that is configured, along with the password, in a ready-to-use login command. This makes it easier to log in.

Cluster installation

To install an OpenShift cluster, there are really two major options: Minishift and CRC. CRC, which stands for CodeReady Containers, is the more modern option and is provided directly by Red Hat. CRC supports more modern versions of OpenShift, specifically any version four point double X and above.

The runtime first needs a container runtime on your host machine. This is usually Docker, Podman, or Containerd. One of these needs to be installed in order for you to use Minishift. However, for the CRC runtime, the process is a bit different. While Minishift uses native container technologies to mimic the OpenShift cluster runtime, CRC uses virtualization. This means you need to have virtualization enabled on your host.

For any modern Windows system, nothing more is required, as Hyper-V will be used to run the CRC runtime. For Linux systems, you are required to install KVM, which is a virtualization platform. For Linux, there are a couple more dependencies that need to be installed in order to run the CRC runtime. These include Virtsh, which is a general-purpose virtual network daemon used to bridge the connection between the CRC runtime running in the virtual machine and your host, and KVM, which is the core component required to run the CRC runtime. Once these are installed, you can proceed further.

After having all dependencies installed on your platform, simply download the CRC runtime from the official Red Hat website. The download page can be found at redhat dot com slash en slash blog slash codeready containers. Once you visit this page, follow the instructions to set up the orchestration runtime.

Cluster login

To enable SSH login into the virtual machine itself, if you are using the CRC distribution running the OpenShift cluster, you need to add a configuration into your SSH config file located in your home directory under dot SSH slash config. This will allow you to perform a simple SSH login command, such as SSH CRC. Also, make sure that the certificate files specified in the configuration exist in the specified locations. They should be present for a local installation of CRC, but if you are using another local OpenShift cluster distribution, replace the paths to point to the locally installed certificate credentials on your machine.

The Minishift distribution does not require any special SSH setup, since it provides a command line option to directly log in to the cluster using the Minishift SSH command.

The set of examples below will use CRC and OpenShift version four point double X, which is supported by CRC.

Creating projects can now be done using the tool. In OpenShift, projects are the fundamental way applications are organized. Projects let users collect their applications into logical groups. They also serve other useful roles around security, which we will discuss in future sections. For now, think of a project as a collection of related applications. You will create your first project and then use it to house a handful of applications that you will deploy, modify, redeploy, and perform various actions on over the next few sections.

The default project and working with multiple projects

The oc tool’s default action is to execute the command you run using the current working project. If you create a new project, it automatically becomes your working project. The oc project command changes the current working project from one to another. To specify a command to be executed against a specific project, regardless of your current working project, use the dash n parameter with the project name you want the command to run against. This is a helpful option when you are writing scripts that use oc and act on multiple projects.

There is a quick and simple way to configure your local SSH client to work with the cluster node, allowing you to quickly log in to the node by simply providing the name of the host, as shown in the SSH config file. This will send the correct identity material from your host machine to the cluster, and allow you to directly log in to the cluster node itself. This is very useful, as you will need to interact with the cluster node to configure it in the future.


Let’s begin with a look at how to connect to your cluster nodes.

The SSH configuration block at the start of this section sets up a host called “crc” for SSH access. It specifies the local loopback address, port twenty-two twenty-two, and uses the “core” user. It also points to a specific identity file for authentication and disables strict host key checking. This setup allows you to SSH into the CodeReady Containers node easily.

In contrast, Minishift provides a much simpler approach. You can use the “minishift ssh” command directly from the command line. This command connects you to the cluster node without any extra configuration or manual steps.

Cluster configuration

When starting your cluster, there are several configuration options you should consider. These settings help ensure your cluster performs well and can even determine whether it starts successfully. While most default configurations are acceptable, the default CPU and memory allocations for both CodeReady Containers and Minishift are often insufficient. This can prevent clusters from running smoothly or even starting at all.

It’s important to remember that enabling more features will require more resources. There’s no precise formula for calculating the exact resources needed for a local deployment, but a good rule of thumb is to allocate at least sixteen gigabytes of memory for Minishift and twenty-four gigabytes for CodeReady Containers. For CPU, start with at least eight cores for Minishift and ten for CodeReady Containers.

To adjust these settings globally, you can use configuration commands. For Minishift, you can set the disk size to one hundred gigabytes, increase memory to sixteen gigabytes, and allocate eight CPU cores. For CodeReady Containers, you can set the disk size to one hundred gigabytes, memory to twenty-four thousand five hundred twelve megabytes, and CPUs to ten. These changes require a cluster restart if applied while the cluster is running.

Beyond these resource settings, there are additional configuration options that enable features like monitoring, which will be discussed later.

There’s another level of configuration available that isn’t tied directly to the cluster tool itself. This is known as the patch approach. It allows you to enable or disable native OpenShift features by patching the default “clusterversion” configuration object. For example, you can remove an entry from the overrides section of the clusterversion object, which changes the default behavior. Before making modifications, you can inspect the object using the “oc describe clusterversion/version” command. The patch command uses a JSON path to remove the first entry from the “spec.overrides” array.

In future sections, you’ll see that the cluster node contains several files under the “slash etc” directory. These files can be edited directly to configure the cluster in a more hands-on way. However, you should only modify these files if you fully understand the changes you’re making, as they provide significant flexibility but also carry risk.

Cluster debugging

If you ever need to access the OpenShift cluster API from the node itself using the system user, you can do so by setting the “KUBECONFIG” environment variable to point to the appropriate configuration file. This file contains the necessary certificate material to log in without providing credentials, allowing you to act as the special “system:node:crc” user. Once the variable is set, any “oc” command you run will execute in the context of this system user.

For Minikube, the configuration file is located in your home directory under “dot kube slash config.” You can export the variable in your shell to use the “oc” tool as a system user. On Windows, use the “SET” command instead of “export,” but the overall approach remains the same.

To log in as an administrator and access the default password for the cluster’s root user, you can enable the emergency login configuration. This isn’t always necessary, but it’s useful to know. By default, you can assume sudo rights on the host by running “sudo dash i” in the shell, which typically doesn’t require a password.

To enable emergency login, configure the cluster accordingly. This generates a password file in the “dot crc slash machines slash crc” directory on your host machine. The file contains the password for the “core” user in the CodeReady Containers cluster node. After enabling this setting and restarting the cluster, locate the password file and use it as needed.

Cluster software

You can install software on your cluster using the “dnf” or “yum” package managers, which are standard for the CentOS distribution underlying the OpenShift cluster node.

The process involves several steps. First, SSH into the cluster node. Then, switch to the root user for easier command execution. Remount the “slash usr” file system as read-write, since it’s read-only by default. Next, modify the “dnf” configuration file to relax certain restrictions that might interfere with software installation. You’ll need to register the node with Red Hat using your credentials, refresh the subscription, clean up any leftover artifacts, and update the repository list. Finally, you can install utility applications such as the Apache HTTP server, IP tables services, NFS utilities, Go, GCC, Python three, and Python pip.

First project

To create your first project, use the “oc new-project” command and provide a project name. For this example, use “image-uploader” as the project name. In OpenShift, a project is equivalent to a namespace in Kubernetes. Projects help separate different application contexts and make it easier to control permissions by restricting user access to specific namespaces.

When you run the command to create a new project, your context will immediately switch to that project.

Application components

Applications in OpenShift are not monolithic. Instead, they consist of several components within a project that work together to deploy, update, and maintain your app throughout its lifecycle. These components include container images, image streams, application pods, build configurations, deployment configurations, deployments, and services.

Let’s start by looking at container images.


Each app deployment in OpenShift creates a custom container image to serve your application. This image is built using your app’s source code, along with a custom base image known as a builder image. For example, the PHP builder image includes the Apache web server and the core PHP language libraries. The image build process takes the builder image you select, integrates your source code, and produces a custom container image that will be used for your app deployment.

Once these images are created, both the custom container images and the builder images are stored in the OpenShift integrated container registry. This registry was discussed in the first section. The component responsible for controlling the creation of your app containers is called the build config.

Build configs

A build config contains all the information needed to build an app from its source code. This includes details such as the URL for the app’s source code, the name of the builder image to use, the name of the app container image that will be created, and the events that can trigger a new build.

After the build config completes its job, it triggers the deployment config that is created for your newly built app.

Deployment configs

If an app is never deployed, it can never do its job. The deployment and upgrading of the app are handled by the deployment config component. Deployment configs track several important pieces of information about an app, including the currently deployed version, the number of replicas to maintain, and the trigger events that can cause a redeployment. By default, configuration changes to the deployment or changes to the container image will automatically trigger an app redeployment. The default upgrade strategy used by app-cli is a rolling upgrade.

App deployments

A key feature of apps running in OpenShift is that they are horizontally scalable. This is represented in the deployment config by the number of replicas. The number of replicas specified in a deployment config is passed into a Kubernetes object called a replication controller. This is a special type of Kubernetes pod that allows for multiple replicas, or copies, of the app pod to be kept running at all times. By default, all pods in OpenShift are deployed by replication controllers.

Another feature managed by a deployment config is the automation of app upgrades. Each deployment for an app is monitored and made available to the deployment config component using deployments.

Pod lifecycle

In OpenShift, a pod can exist in one of five phases at any given time in its lifecycle. These phases are described in detail in the Kubernetes documentation, but here is a brief summary.

The Pending phase means the pod has been accepted by OpenShift but is not yet scheduled on one of the app nodes.

The Running phase means the pod is scheduled on a node and is confirmed to be up and running.

The Succeeded phase means all containers in a pod have terminated successfully and will not be restarted.

The Unknown phase means something has gone wrong and OpenShift cannot obtain a more accurate status for the pod.

The Failed and Succeeded phases are considered terminal states for a pod in its lifecycle. Once a pod reaches one of these states, it will not be restarted.

You can see the current phase for each pod in a project by running a command that lists the pods. This is important when you begin creating project quotas.

Each time a new version of an app is created by its build config, a new deployment is created and tracked by the deployment config. A deployment represents a unique version of an app. Each deployment references a version of the app image that was created and creates the replication controller to create and maintain the pod that serves the app.

New deployments can be created automatically in OpenShift by managing how apps are upgraded, which is also tracked by the deployment config. The default app upgrade method in OpenShift is to perform a rolling upgrade. Rolling upgrades create new versions of an app, allowing new connections to access only the new version. As traffic increases to the new deployment, the pods for the old deployment are removed from the system.

New app deployments can be automatically triggered by events such as configuration changes to your app or the availability of a new version of a container image. These trigger events are monitored by image streams in OpenShift.

Image stream

Image streams are used to automate actions in OpenShift. They consist of links to one or more container images. Using image streams, you can monitor apps and trigger new deployments when their components are updated.

Deploying an app

Apps are deployed using a command that creates a new app. When you run this command to deploy the image uploader app into the image-uploader project, you need to provide three pieces of information.

First, you specify the type of image stream you want to use. OpenShift ships with multiple container images, called builder images, that you can use as a starting point for apps.

Second, you provide a name for your app. In this example, the name app-cli is used because this version of your app will be deployed from the command line.

Third, you provide the location of your app’s source code. OpenShift will take the source code and combine it with the PHP builder image to create a custom container image for your app deployment.

The code block here shows a command that deploys a new app from a GitHub repository using the PHP builder image. It specifies the image stream, the source code location, and the app name.

After you run the command to create a new app, you will see a long list of output. This output shows OpenShift building the image from all the components needed to make your app work properly.

If you visit the web console, you will be able to browse the project structure and see that several new objects have been created for the new app in the new project. When you trigger a new app deployment using this command, various new objects are created in the cluster for the current project. In real-world scenarios, each of these objects would typically be manually defined in manifest files by developers, allowing for fine-tuning of their properties.

However, for demonstration purposes and ease of use, OpenShift provides users with quick and simple ways to deploy apps. This is very useful for development, when you just want to get your app running and avoid the hassle of manual configuration.

The next code block summarizes commands that list some of the objects automatically created by OpenShift, such as services, pods, and deployments. The output shows multiple pods, including a build pod that is used to build the image from the source and upload it to the internal OpenShift registry. It also shows the actual app pod in a running state, as well as a new service created for the app.

Providing access to apps

In future sections, we will explore multiple ways to force OpenShift to redeploy app pods. In the course of a normal day, this happens frequently for many reasons, such as scaling apps up and down, app pods not responding correctly, node reboots, hardware issues, or human error.

Although pods may come and go, there needs to be a consistent presence for your app in OpenShift. This is the role of a service. A service uses labels applied to application pods when they are created to keep track of all pods associated with a given app. This allows a service to act as an internal proxy for your app.

You can see information about the service for app-cli by running a command that describes the service. This will provide details about how the service is configured and which pods it is currently routing traffic to.


Let’s begin by looking at the details of the app-cli service in the image-uploader namespace.

This service is labeled with several identifiers, including app equals app-cli, and app dot kubernetes dot io slash component equals app-cli. It is of type ClusterIP, which means it is only accessible within the OpenShift cluster. The service has a single IPv4 address, ten dot two hundred seventeen dot four dot one sixty-two, and exposes two TCP ports: eighty eighty and eighty four forty-three. These ports are mapped to the corresponding ports on the application pods. The service does not use session affinity, and its internal traffic policy is set to Cluster.

The key point here is that these IP addresses are virtual and only routable within the OpenShift cluster. They are not accessible from outside the cluster. The service maintains information about its own IP address and the TCP ports that connect to the pods running your application.

Most components in OpenShift have a shorthand notation for use on the command line. For example, you can use “svc slash app-cli” to get information about the app-cli service. Similarly, build configurations can be accessed with “bc slash” followed by the application name.

Services provide a consistent gateway into your application deployment. However, the service IP addresses are only available within your OpenShift cluster. To connect users to your app and make DNS work properly, you need to expose your application externally. This is done by creating a route.

Exposing application services

When you install your OpenShift cluster, one of the services created is the HAProxy service. HAProxy is an open-source software load balancer that runs in a container on OpenShift. We will explore this service in more detail in later sections.

To create a route for the app-cli service, you run a command that exposes the service as a route. This command tells OpenShift to create a new route resource, which will allow external traffic to reach your application through the cluster’s load balancer.

As discussed earlier, OpenShift uses projects to organize applications. When you create an application route, the project name is included in the generated URL. The default format for an application URL is the application name, followed by the project name, then a dot, and finally the cluster application domain. In Kubernetes, the namespace is used in place of the project name, but in OpenShift, projects are implemented as enhanced Kubernetes namespaces.

When you deployed OpenShift, you specified the application domain. By default, all applications in OpenShift are served using the HTTP protocol. Putting this together, the URL for the app-cli application should look like this: “http colon slash slash app-cli dash image-uploader dot cluster-app dot domain.”

You can get more information about the route you just created by running a command that describes the route. This command provides details such as the route’s name, namespace, creation time, labels, annotations, the requested host, the path, TLS termination settings, insecure policy, endpoint port, associated service, weight, and endpoints.

The output tells you that the host configuration has been added to HAProxy, and it shows the service associated with the route, as well as the endpoints for the service. This information is important for understanding how requests are handled for the route.

Now that you have created the route to your application, you can verify it by visiting the generated URL in a web browser.

Focusing on the components that deploy and deliver the app-cli application, you can see the relationship between the service, the newly created route, and the end users. The route is tied to the app-cli service, and users access the application pod through the route. We will cover this relationship in more depth in later sections.

This chapter is about relationships. In OpenShift, multiple components work together to build, deploy, and manage applications. Understanding these relationships is fundamental to knowing how container platforms operate.

Containers

In the previous sections, you deployed your first application in OpenShift. In this chapter, we will look deeper into your OpenShift cluster and investigate how containers isolate their processes on the application node.

Understanding how containers work in a platform like OpenShift is some of the most powerful information in information technology today. This knowledge informs how systems are designed and how issues are analyzed when they inevitably occur.

This section is challenging, not because of complex configuration, but because we are discussing the fundamental layers of abstraction that make a container a container in the modern Linux kernel.

Defining containers

If you ask five different container experts to define what a container is, you are likely to get five different answers. Here are some of our personal favorites, all of which are correct from a certain perspective.

First, a container can be described as a transportable unit to move applications around. This is a typical developer’s answer.

Second, a container can be seen as a fancy Linux process.

Third, it can be described as a more effective way to isolate processes on a Linux system. This is a more operations-centered answer.

All of these definitions are correct, depending on your point of view.

Earlier, we talked about how OpenShift uses Kubernetes and Docker to orchestrate and deploy applications in containers within your cluster. However, we have not discussed which application component is created by each of these services. Before moving forward, it is important to understand these responsibilities as you begin interacting with application components directly.

OpenShift component interaction

When you deploy an application in OpenShift, the request starts in the OpenShift API server. To really understand how containers isolate the processes within them, we need to take a more detailed look at how these services work together to deploy your application.

The relationship between OpenShift, Kubernetes, Docker, and ultimately the Linux kernel is a chain of dependencies. When you deploy an application in OpenShift, the process starts with the OpenShift services.

OpenShift manages deployments

Deploying an application begins with application components that are unique to OpenShift. The process is as follows.

First, OpenShift creates a custom container image using your source code and the builder image template you specified.

Next, this image is uploaded to the OpenShift container image registry.

Then, OpenShift creates a build configuration to document how your application is built. This includes which image was created, the builder image used, the location of the source code, and other relevant information.

After that, OpenShift creates a deployment configuration to control deployments and manage updates to your application. Information in deployment configurations includes the number of replicas, the upgrade method, application-specific variables, and mounted volumes.

OpenShift then creates a deployment, which represents a single deployed version of an application. Each unique application deployment is associated with your application’s deployment configuration component.

The OpenShift internal load balancer is updated with an entry for the DNS record for the application. This entry is linked to a component created by Kubernetes, which we will discuss shortly.

Finally, OpenShift creates an image stream component. In OpenShift, an image stream monitors the builder image, deployment configuration, and other components for changes. If a change is detected, image streams can trigger application redeployments to reflect those changes.


The build configuration creates a custom container image for your application. It uses a specified builder image and your source code. Once built, this image is stored in the OpenShift image registry.

The deployment configuration then creates a deployment that is unique for each version of your application. An image stream is also set up. This image stream monitors for changes to both the deployment configuration and the related images in the internal registry. Additionally, a DNS route is created and linked to the relevant Kubernetes object.

Kubernetes schedules applications

Kubernetes acts as the orchestration engine at the heart of OpenShift. In many ways, an OpenShift cluster is essentially a Kubernetes cluster. When you first deploy an application using the command-line interface, Kubernetes creates several key components for your application.

First, there is the replication controller. This component is responsible for scaling the application as needed. It ensures that the desired number of replicas, as specified in the deployment configuration, is always maintained.

Next, there is the service. A Kubernetes service exposes your application. It provides a single IP address that is used to access all the active pods for a given deployment. When you scale your application up or down, the number of pods changes, but they are all accessed through this single service proxy.

Pods are the smallest scalable unit in OpenShift. The replication controller determines how many pods are created for an initial deployment, and these pods are linked to the OpenShift deployment component. The service is also linked to the pod components. It represents all the pods deployed by a replication controller and provides a single IP address in OpenShift to access your application, even as it scales up and down across different nodes in your cluster. This service uses an internal IP address, which is referenced in the route created by the OpenShift load balancer.

Docker creates containers

Docker is the container runtime. A container runtime is the application on a server that creates, maintains, and removes containers. While a container runtime can be used as a standalone tool on a laptop or single server, it is most powerful when orchestrated across a cluster by a tool like Kubernetes.

Kubernetes controls Docker to create containers that house your application. These containers use the custom base image as the starting point for the files that are visible to the application inside the container. Finally, the Docker container is associated with a Kubernetes pod.

To isolate the libraries and application in the container image, along with other server resources, Docker uses Linux kernel components. These kernel-level resources are what isolate your application in the container from everything else on the application node.

Let us check them out.

Linux isolates resources

At the core of what makes a container a container in OpenShift—and in Linux—are three key Linux kernel components. Docker uses these components to isolate the application running in containers and to limit their access to resources on the host machine or cluster node.

First, Linux namespaces provide isolation for the resources running in the container. Although the term “namespace” is also used in Kubernetes, here we are referring specifically to Linux namespaces, which are a different concept. In this section, when we mention namespaces, we mean Linux namespaces.

Second, control groups, or cgroups, provide guaranteed access to CPU and memory limits on the application node or cluster node.

Third, SELinux contexts prevent the container application from improperly accessing resources on the host or in other containers. An SELinux context is a unique label applied to container resources on the application node. This label ensures that the container cannot access anything on the host that does not have a matching label.

The Docker daemon creates these kernel resources dynamically when the container is created. These resources are then associated with the application that is launched for the corresponding container. At this point, your application is running in a container.

Applications in OpenShift are run and associated with these kernel components. They provide the isolation that you experience from inside a container. In upcoming sections, we will discuss how you can investigate a container from the application node. From the perspective of the container, an application only has access to the resources that have been allowed and allocated to it.

Working with the cluster

To extract resources from the cluster, you first need to be able to log in to the cluster. This means you want to interact with the virtual machine that is created locally when you set up your OpenShift cluster. Logging in to the cluster is done using SSH, following a specific command template.

The SSH command sets up a secure connection to the cluster. It uses a valid private key that the cluster trusts. By default, the port is set to twenty-two twenty-two, and the host is set to one twenty-seven dot zero dot zero dot one for a local cluster. However, the same SSH command can be used to log in to a provided cluster, as long as you have a locally set up private key that is trusted by the cluster’s SSH agent.

For example, you might use a command that logs you into the CodeReady Containers virtual machine, which represents the actual single cluster node being simulated locally. You can refer to the beginning of your documentation to set up a permanent SSH host configuration for CodeReady Containers.

To extract the process ID of a running container, you need to take a few more steps. First, you can list all running processes on the cluster using a command that shows all pods along with their IDs, names, and additional information about the running pods, containers, and images. From this output, you need to look for the app-cli container information and get the container ID from that row in the table. The table provides columns such as container ID, image, pod ID, creation date, and more.

Once you have identified the container for the app-cli pod, you can inspect it to obtain its process ID. Running the appropriate command will show you detailed information about the container, including a property called “pid,” which stands for process ID. This is the identifier you need.

Listing kernel components

With the process ID of the current app-cli container, you can begin to analyze how containers isolate process resources using Linux namespaces. As discussed earlier, kernel namespaces are used to isolate the application in a container from other processes on the host. Docker creates a unique set of namespaces for each container to isolate its resources.

Cgroups and SELinux are both configured to include information for a newly created container, but those kernel resources are shared among all containers running on the application node.

To get a list of the namespaces that were created for the app-cli container, you can use the lsns command. You will need the process ID for the application to pass as a parameter to this command. This allows you to see the specific namespaces associated with your container.


OpenShift and Linux Namespaces

OpenShift uses five Linux namespaces to isolate processes and resources on application nodes. Defining exactly what a namespace does can be tricky, but two analogies help clarify their most important properties.

First, namespaces are like paper walls in the Linux kernel. They are lightweight and easy to set up or tear down, yet they provide enough privacy when in place.

Second, namespaces are similar to two-way mirrors. From within a container, only the resources inside that namespace are visible. However, with the right tools, you can observe what’s inside a namespace from the host system.

Listing Namespaces for a Container

To see all namespaces associated with a process, you can use the lsns command. This command requires the process ID, so you must first extract the correct process ID for the running container. The process for obtaining this ID is described in detail elsewhere, but in summary, you log into the cluster and use a tool like crictl to inspect the container and retrieve its process ID.

Once you have the process ID, running lsns with the -p flag lists all namespaces for that process. For example, if the process ID is seventy-eight twenty-five, the output will show several namespaces attached to this process. The type column in the output indicates the presence of mount, cgroup, network, UTS, and other namespaces created for the process.

The Five Namespaces Used by OpenShift

OpenShift uses five key namespaces to isolate applications:

Mount namespace ensures that only the correct content is available to applications inside the container.

Network namespace gives each container its own isolated network stack.

PID namespace provides each container with its own set of process ID counters.

IPC namespace offers shared memory isolation for each container.

UTS namespace gives each container its own hostname and domain name.

Additional Linux Namespaces

There are two more namespaces in the Linux kernel that OpenShift does not use by default.

The cgroup namespace is used as a shared resource on the OpenShift node, so it is not required for effective isolation.

The user namespace can map a user inside a container to a different user on the host. For example, a user with ID zero in the container could be mapped to user ID five thousand on the host. This feature can be enabled in OpenShift, but it may introduce performance and configuration challenges that are beyond the scope of this example. For more information on enabling user namespaces with Docker and OpenShift, see the article “Hardening Docker Hosts with User Namespaces” by Chris Binnie.

Mount Namespace

The mount namespace isolates file system content, ensuring that only the content assigned to the container by OpenShift is available to the process running inside. For example, the mount namespace for the app-cli container allows the application to access only the content in the custom app-cli container image and any data stored on the persistent volume associated with its persistent volume claim.

Persistent storage is essential for applications. It allows data to persist even when a pod is removed from the cluster and enables data sharing between multiple pods when needed. You will learn how to configure and use persistent storage on an NFS server with OpenShift in a future section.

Container Storage and Logical Volumes

The root file system based on the app-cli container image can be more challenging to uncover. When you configured OpenShift, you specified a block device for Docker to use for container storage. OpenShift uses logical volume management on this device, giving each container its own logical volume when created. This storage solution is fast and scales well for large production clusters.

To view all logical volumes created by Docker on your host, you can use the lsblk command. This command lists all block devices and logical volumes on your host, confirming that Docker has been creating logical volumes for containers.

Listing Active Pods and Inspecting Containers

To list all active pods, you can use the crictl ps command. This command shows the container ID, the image, the state, the name, and the pod it is connected to. This information allows you to inspect the container by its ID.

When you inspect a container, you can see details such as mount targets. Some are generic, like the etc hosts file, while others are specific to the application. The inspect output also reveals the process ID of the container, which is typically process ID one. This process ID is unique and ties all information on the host to the container. The host system does not see pod or container IDs; it sees the process ID.

Mount Points and File System Layers

The inspect output also shows how and where parts of the root file system are mounted. The root file system of a container is built from layers, but certain parts may be mounted to different locations outside the container. These mount points are similar to shared directories on a network. Each mount point refers to a host path and a container path, which are self-explanatory.

For example, the etc hosts file inside the container may be mounted from a specific location on the host. Similarly, the termination log and service account secrets are mounted from host paths into the container. These mount points ensure that the container has access only to the necessary files and directories, maintaining isolation and security.

In summary, OpenShift leverages Linux namespaces and logical volume management to provide strong isolation and efficient storage for containers, ensuring that applications run securely and reliably within the cluster.


Here, having the process ID, or PID, we can list the namespaces associated with the container process for our application. This allows us to see the many different namespaces that the process is part of. In this case, we want to inspect the mount namespace.

When you run the command to list namespaces for a specific PID, you get a table showing each namespace type, the number of processes in it, the PID, the user, and the command being run. For example, you might see entries for time, user, mount, PID, cgroup, UTS, IPC, and network namespaces, each with their own unique identifiers. The mount namespace, in particular, is associated with the process running the HTTP daemon in the container.

Next, we can enter the mount namespace of the container. By doing this, if you execute a command like listing the root directory, you will see the contents of the container itself. This gives you direct access to the root file system of the container. To exit the mount namespace, you simply use the exit command.

Now, let’s see where this file system is mounted on the host. Using the process ID, you can check where the root file system is mounted on the host, which in this context is the cluster node itself. The output will show overlay paths, which reference locations on the host’s root file system, typically under the var directory.

The command to check this uses the process ID to look up mount information. The output reveals that the root file system is mounted using OverlayFS, with several key parameters. These include lower directories, which are the read-only layers, an upper directory, which is the writable layer, and a work directory used by OverlayFS.

What are these layers referring to? The file system in a container runtime is made up of layers. This means it is not a single chunk of data, but rather a stack of layers. For example, the base image itself is one layer, and any subsequent changes to the file system in the container generate new layers that stack on top of the previous ones. Actions like adding a new file, creating directories, copying, or removing data from the container all add new layers. This is similar to how source control systems work, where you have commits based on previous commits. If you follow the chain of commits, you can move between different states of the repository. The idea is very much the same here: the container’s file system is layered in a similar fashion, and you can think of layers as being like commits in a source control system.

Above, we have shown how you can enter the mount namespace, detect where the root file system of a container is mounted on the host, and even explore and operate within the mount namespace. This effectively allows you to modify or work with the root file system of the container—tasks that are usually hidden behind commands like exec. Under the hood, exec does the same thing: it enters the namespace of the process, allowing you to execute commands in the context of the container’s namespace.

Each image is built from read-only layers, also known as copy-on-write layers. This works similarly to source control systems like Git. Each image is constructed from read-only layers, and if you look at a Dockerfile, each command creates a new read-only layer. For example, consider a Dockerfile that starts from an Ubuntu base image, runs an update command, copies in an application file, and sets a command to run. Each of these steps creates a new layer: the base layer, the update layer, and the copy layer. These layers are stacked on top of each other, so the final layer is based on the previous one, just like commits in a source control system.

When you run a container, the read-only layers from the image are mounted as lower directories using OverlayFS. A new writable layer, called the upper directory, is added on top. The container sees a merged view of the upper directory and the lower directories. Changes are handled as follows: if you modify a file from a read-only layer, the file is copied from the lower directory to the upper directory, and changes are applied to the copy in the upper directory. New files are written directly to the upper directory. If you delete a file, a whiteout file is created in the upper directory to hide the file in the lower layers. The file is not actually deleted from the lower layers, but it is obscured.

The read-only layers are shared between many different images and containers. However, the writable layers are unique to each container and are ephemeral. When the container is deleted, so are its writable layers. The merged view is what the container sees, combining the read-only layers and its own writable layer. Each modification or change in the container file system generates a new writable layer. The read-only layers are only created during the creation of the container image.

The key implication of this model is efficiency. Multiple containers can share the same read-only base layers, which saves disk space. Since many images are based on the same base layers, such as CentOS or Alpine, these layers are shared. Performance is also good because of copy-on-write: only the modified files are copied into new layers. The writable layer isolates changes in the container, and it is ephemeral by default, deleted when the container is deleted. To persist data across containers, you should use volumes. Volumes live outside of the file system layers and are simply mount points in the container, meaning they are not part of the file system layers at all.

UTS namespace

UTS stands for UNIX Time Sharing in the Linux kernel. The UTS namespace allows each container to have its own hostname and domain name. It can be confusing because time sharing here does not refer to managing the system clock. Originally, time sharing meant multiple users sharing system time simultaneously, a novel idea when it was introduced in the nineteen seventies. The UTS data structure in the Linux kernel originated from that era. This structure retains the hostname, domain name, and other system information.

If you want to see all the information in that structure, you can run the uname dash a command on a Linux server. That command queries the same data structure. The easiest way to view the hostname for a server is to run the hostname command.

You could use nsenter to enter the UTS namespace for the app-cli container, just as you entered the mount namespace earlier. However, there are additional tools that will execute a command in the namespace for a running container. One of those tools is the docker exec command. To get the hostname value for a running container, pass docker exec the container’s short ID and the hostname command you want to run in the container. Docker executes the specified command in the container’s namespaces and returns the value. The hostname for each OpenShift container is its pod name.

Each container has its own hostname because of its unique UTS namespace. If you scale up app-cli, the container in each pod will have a unique hostname as well. This is valuable for identifying data coming from each container in a scaled-up system.

To confirm that each container has a unique hostname, log into your cluster as your developer user. Use the oc login command with your username, password, and cluster URL. The oc command line tool has functionality similar to docker exec. Instead of passing in the short ID for the container, you can pass it the pod in which you want to execute the command.

After logging in to your oc client, you can scale the app-cli application to two pods with a command that sets the number of replicas to two. In this context, dc stands for deployment config, which is the shorthand name for the object type. Deployment config is the old version three object, which was renamed to deployment in version four. The command in newer versions looks a little different, but the idea is the same: you scale the deployment to the desired number of replicas.


This will cause an update to your app-cli deployment configuration and trigger the creation of a new app-cli pod. To find the name of the new pod, you can run a command that lists all pods and filters for those in the “Running” state. This filtering ensures you only see active pods, not those that have already completed. In OpenShift, the container’s hostname matches its pod name, so you can easily identify which pod you’re working with, similar to how you would with Docker.

First, you need to scale up the deployment of your app. This involves altering the deployment, which prompts OpenShift to create two new pods. Once these new pods are ready, the old ones will be removed. After scaling, you can list the running pods to confirm that you now have two active pods, as expected. The pod names will look something like “app-cli-5b9c58956d-p7jpn” and “app-cli-5b9c58956d-p8jpn,” and both will be in the Running state.

To get the hostname from your new pod, use the “oc exec” command, targeting the new pod by name. This command works similarly to “docker exec,” but instead of using a container’s short ID, you specify the pod name. The hostname inside the pod will always match the pod’s name, just as it does in Docker and Kubernetes. When you run the command to print the hostname, the output will be the full pod ID you used to call the exec command.

Remember, in Docker, Kubernetes, and OpenShift, the container’s hostname is essentially the unique identifier provided by the orchestrator or Docker itself.

PID namespace

Process IDs, or PIDs, are how one application sends signals and information to other applications. Isolating visible PIDs within a container is an important security feature, and this is accomplished using PID namespaces.

On a Linux server, the “ps” command shows all running processes and their associated PIDs on the host. On a busy system, this can produce a lot of output. To limit the output to a single PID and its child processes, you can use the “--ppid” option. From your app node, run the “ps” command with the “--ppid” option and include the PID you obtained for your app-cli container. This will show you the process tree for that specific parent process. For example, you might see that the process with PID 7,825 is “httpd,” and it has spawned several other processes.

To see the processes from within the container, use “oc exec” to run the “ps” command inside the app-cli pod. Inside the container, you do not use the “--ppid” option, because you want to see all PIDs visible from within the app-cli container. The output will show the initial command, such as “httpd,” as PID 1, along with its child processes. The PIDs inside the container will be different from those on the host.

There are three main differences in the output when comparing the host and the container. First, the initial “httpd” command is listed in the output. Second, the “ps” command itself appears in the list. Third, the PIDs are completely different.

Each container has its own unique PID namespace. This means that, from inside the container, the initial command that started the container is always viewed as PID 1. This is the parent process, and all other processes in the container are forked from it. Any applications created by a process already inside a container automatically inherit the container’s namespace. This makes it easier for applications within the container to communicate with each other.

So far, we have discussed how filesystems, hostnames, and PIDs are isolated in a container. Next, let’s take a quick look at how shared memory resources are isolated.

Memory namespace

Applications can be designed to share memory resources. For example, one application can write a value into a special shared section of system memory, and another application can read and use that value. In OpenShift, the following shared memory resources are isolated for each container: POSIX message queue interfaces in the directory “slash proc slash sys slash fs slash mqueue,” IPC interfaces in “slash proc slash sysvipc,” and memory parameters such as “msgmax,” “msgmnb,” and “msgmni.”

If a container is destroyed, its shared memory resources are destroyed as well. Because these resources are application-specific, you will work with them more in later sections. When you deploy a stateful application, the last namespace to discuss is the network namespace.

Networking namespace

The fifth kernel namespace used by Docker to isolate containers in OpenShift is the network namespace. The networking namespace isolates network resources and traffic within a container. In this context, “resources” means the entire TCP/IP stack used by applications in the container.

Future chapters will go into detail about OpenShift’s software-defined networking. For now, it’s important to understand that the view from within the container is very different from the view from the host.

The PHP builder image you used to create “app-cli” and “app-gui” does not have the “ip” utility installed. You could install it into the running container using “yum,” but a faster way is to use the “nsenter” tool. Earlier, you used “nsenter” to enter the mount namespace of the Docker process so you could view the root filesystem for app-cli.

It would be helpful to discuss the OSI model here, but that is outside the scope for now. In short, the OSI model describes how data travels in a TCP/IP network. There are seven layers, and you will often hear about “layer three devices” or a “layer two switch.” When someone refers to a specific layer, they are talking about the layer of the OSI model on which a particular device operates. The OSI model is a great tool for understanding how data moves through any system or application. If you haven’t read about the OSI model before, it is worth your time to look up the article titled “The OSI model explained: how to understand and remember the seven-layer network model.”

When you run “nsenter” and include a command as the last argument, instead of opening an interactive session in that namespace, the command is executed in the specified namespace and returns the result. Using this tool, you can run the “ip” command from your server’s default namespace in the network namespace of your app-cli container.

If you compare this to the output from running the “ip” command on your host, the differences are obvious. Your app node will have ten or more active network interfaces, representing the physical and software-defined devices that make OpenShift function securely. But in the app-cli container, you have a container-specific loopback interface device and a single network interface with a unique MAC and IP address.

Running the “ip a” command on the host or cluster node will yield a large output, showing all network interfaces and their configurations.


Let’s begin by comparing the network devices present on a cluster node versus those inside a container.

On the cluster node, there are many more software and hardware network devices active compared to what you find in a container. For example, the node has a loopback interface, labeled as “lo,” which is up and running. It also has two Ethernet interfaces, “eth0” and “eth1,” each with their own MAC addresses and both supporting broadcast and multicast traffic. These interfaces have assigned IPv4 and IPv6 addresses, and their states are reported as up.

In addition to these, the node has a “docker0” bridge interface, which is used for Docker networking. This bridge has its own MAC address and IP address, and it is also up. There are several virtual Ethernet, or “veth,” interfaces, each connected to different network namespaces. These veth interfaces are used to connect containers to the bridge, allowing network traffic to flow between containers and the outside world. Each veth device has its own MAC address and link to a specific network namespace, and they all support IPv6 link-local addresses.

Now, let’s look at what happens when you use the “ip” command inside a container’s network namespace. This is done by running a command on the host that enters the container’s namespace using “nsenter,” and then executes the “ip” command. The key point here is that the same “ip” binary from the host is used, but it operates in the context of the container’s network namespace.

When you run this command, the output is much simpler. Inside the container, you typically see only two network interfaces. The first is the loopback interface, “lo,” which is up and has both IPv4 and IPv6 addresses assigned for local communication. The second is a single Ethernet interface, usually named “eth0.” This interface is connected to the host via a veth pair, and it has its own MAC address and an IP address assigned from a private range. The interface also supports IPv6 link-local addressing.

This difference in the number of network devices is expected. Containers are designed to be isolated, so they only have access to the network interfaces they need—usually just the loopback and one Ethernet interface.

Summary

The network namespace is the first component in the OpenShift networking solution. We’ll discuss how network traffic gets in and out of containers in the next sections, when we cover OpenShift networking in depth.

In OpenShift, isolating processes does not happen in the application space or even in user space on the application node. This is a key difference between OpenShift and other types of software clusters, as well as some other container-based solutions. In OpenShift, isolation and resource limits are enforced in the Linux kernel on the application nodes. Isolation with kernel namespaces provides a much smaller attack surface. For an exploit to allow someone to break out from a container, it would have to exist in the container runtime or the kernel itself.

With OpenShift, as we’ll discuss in depth in the next section on security principles, the configurations of the kernel and the container runtime are tightly controlled. The last point to emphasize in this section is the importance of understanding how containers work and how they use the Linux kernel. This knowledge is invaluable when managing your cluster or troubleshooting issues. It allows you to think about containers in terms of what they are doing all the way down to the Linux kernel, making it easier to solve issues and create stable configurations.

Before moving on, make sure to clean up to a single replica.

Cloud Native Apps

Cloud native is the approach used to create the next generation of applications. In this part of the book, we’ll discuss the technologies in OpenShift that enable continuously deploying, self-healing, and auto-scaling behaviors—features we all expect in a cloud native app.

Previous chapters focused on working with and modifying services in OpenShift. This section also walks you through creating problems for your app to ensure that it’s always functioning correctly.

Testing App Resiliency

When you deployed the Image Uploader app in previous sections, one pod was created for each deployment. If that pod crashed, the app would be temporarily unavailable until a new pod was created to replace it. If your app became more popular, you would not be able to support new users beyond the capacity of a single pod.

To solve this problem and provide scalable apps, OpenShift deploys each app with the ability to scale up and down. The component that handles scaling app pods is called the replication controller.

Replication Controller

The main function of the replication controller is to ensure that the desired number of identical pods is running at all times. If a pod exits or fails, the replication controller deploys a new one to ensure a healthy app is always available. In other words, OpenShift takes care of maintaining the desired state, as configured by developers or app managers.

You can think of the replication controller as a pod monitoring agent that ensures certain requirements are met across the entire OpenShift cluster. To check the current status of the replication controller for the app-cli deployment, you can use the “oc describe” command. Note that you specify the individual deployment, not just the name of the app. The information tracked about the replication controller helps establish its relationships to other components that make up the app.

The command to scale a deployment in OpenShift uses the “oc scale” command. This command creates or updates a replication controller instance by setting the desired number of replicas for a deployment. The replication controller is an evolution of the replica controller, but it essentially represents the actual replicas being managed by the orchestrator.

When you run this command, OpenShift scales the deployment to the specified number of replicas.

The replication controller tracks several key pieces of information. It records the name of the replication set or replication controller, which matches the name of the associated deployment. It also tracks the image name used to create pods for the replication controller, as well as the labels and selectors for the controller. The current and desired number of pod replicas running in the replication controller are monitored, along with historical pod status information. This includes how many pods are waiting to be started or have failed since the creation of the replication controller.


The labels and selectors in the next listing are key-value pairs that are associated with all OpenShift components. These pairs are essential for creating and maintaining the relationships and interactions between applications. We will discuss them in more depth in the following sections.

It is important to note that the command shown below will return a list of replication controllers only if there have ever been any replicas created. If the application’s pods were never replicated—meaning there was only ever one pod for the deployment configuration—then no replication controller object will be created. Make sure that you have created replicas for the given deployment; otherwise, you might not receive the expected result from the command that lists replica sets.

The command in question describes a replica set for the app-cli application. It provides detailed information about the replica set, including its name, namespace, selectors, labels, annotations, the number of current and desired replicas, the status of the pods, and the configuration of the containers and volumes. This information helps you understand how the replica set is managing the pods and what metadata is associated with it.

It is also worth noting that more recent versions of OpenShift will, by default, use the ReplicaSet object instead of the older ReplicationController object. This change is because the new set object offers more features, and the old ReplicationController is being phased out. Ultimately, they serve the same purpose, but the newer object includes additional features in its manifest and specification.

Labels and selectors

As we move forward, it is important to understand how labels and selectors are used in OpenShift.

When an application is deployed in OpenShift, every object that is created is assigned a collection of labels. These labels are unique per project, just like application names. For example, in the Image Uploader project, only one application can be named app-cli.

Labels applied to an object are attributes that can be used to create relationships in OpenShift. However, relationships are two-way streets. If something can have a label, something else must be able to state a need for a resource with that label. The other side of this relationship exists in the form of label selectors.

Label selectors are used to define the labels that are required when work needs to happen. In other words, selectors specify which labeled resources should be targeted for a particular operation.

Let’s examine this in more depth using the app-cli application you deployed earlier. In the next example, you will remove a label from a deployed pod. This is one of the few times you will intentionally break an application. Removing a label from a pod will break its relationship with the replication controller and other application components. The purpose of this example is to demonstrate the replication controller in action—and to do that, you need to create a condition it needs to remedy.

As mentioned earlier, selectors in an application component are the labels it uses to interact with other components and link up to them. They are not simply meant for tagging human-readable information to application components.

The app-cli replication controller will create and monitor application pods with specific labels. For example, the deployment object is tagged with many labels, and the selector in this case matches the labels in the pod template. Most of the labels for the deployment are Kubernetes-specific, and only one is really user-defined, which is app equals app-cli.

The selector section in the deployment specifies which pods it should manage by matching their labels. In this case, the pod contains only one label, which matches the selector in the deployment. This is how OpenShift knows which pods belong to which deployments. The same rule applies to any other OpenShift object, and this is how relationships between them are built in Kubernetes and OpenShift.

The fastest way to delete the pods for the app-cli deployment is through the command line. This process should not be part of your normal application workflow, but it can be useful when troubleshooting issues in an active cluster.

The command to delete all pods with a matching selector for app-cli will remove all pods that have the label app equals app-cli. This label was automatically attached when the app-cli application was created by OpenShift, but in real-world scenarios, you would typically decide how to name the key and value pair of the label tag.

It is important to note that labels are a fundamental part of how OpenShift and Kubernetes work. If we make an analogy, labels are like table relations in a relational database.

There is also another command that can be used to delete and clean up all resources related to and attached to a given label. This command will delete all resources that are attached, related, or associated with the specified label.

Returning to the pod from which we removed or detached the label, you might wonder whether the abandoned pod will still receive traffic from users. The service object, which is responsible for network traffic, also works on the concept of labels and selectors. To determine whether the abandoned pod would have served traffic, you need to look at the selector field in the service object.

You can get this selector information about the app-cli service by running a command that describes the service and filters for the selector. This will print out the selector label for the service, which might look like app equals app-cli and deploymentconfig equals app-cli. As you can see, more than one selector can be applied to an OpenShift object. The key and value are usually enough to know how that label or selector will be used. In this example, there is a generic top-level selector, app equals app-cli, which links the service object to all app-cli objects, effectively tagging them with a namespace. There is also a specific selector that links the service to the deployment configuration, which may not be relevant in this case.

Because we have deleted the selector label from the original pod, its labels no longer match the app-cli service. Since there is no other pod, this means that no traffic will be routed to the original pod. Selectors would no longer receive traffic requests.

Kubernetes was born out of many lessons learned at Google from running containers at scale for over ten years. The main two orchestration engines internally at Google during this time were Borg and its predecessor, Omega. One of the primary lessons learned from these two systems was that control loops of decoupled API objects were far preferable to large, centralized, stateful orchestration. This type of design is often called control through choreography.

Here are just a few of the ways this is implemented in Kubernetes. The system uses decoupled API components, avoids stateful information, and loops through control loops against various microservices.


By running control loops instead of maintaining a large state diagram, the resiliency of the system is significantly improved. If a controller crashes, it simply reruns the loop when it restarts. In contrast, a state machine can become brittle when errors occur or as the system grows in complexity.

In our specific examples, this principle holds true because the replication controller, or RC, loops through pods with the labels specified in its selector field, rather than maintaining a static list of pods it supervises. Replication controllers ensure that properly configured application pods are always available in the correct number. Additionally, the desired replica counts can be modified either manually or automatically.

Next, let’s discuss how to scale application deployments.

Scaling applications

An application can consist of many different pods, all working together to serve users. Because different pods may need to be scaled independently, each collection of identical pods is represented by a service component, as we discussed earlier. More complex applications can include multiple services, each with independently scaled pods.

A standard application design uses three tiers to separate concerns within the app. The first tier is the presentation layer, which provides the user interface, styling, and workflows for the user. This is typically where the website resides. The second tier is the login layer, which handles all required data processing for the application. This is often referred to as the middleware layer. The third tier is the storage layer, which provides persistent storage for application data. This can include databases, filesystems, or a combination of both.

Essentially, the application code runs in the pods for each app layer. That code is accessed directly from the routing layer. Each application service communicates with the routing layer and the pods it manages. This design results in the fewest network hops between the user and the application.

This architecture also allows each layer in the three-tier design to be independently scaled to handle its workload effectively, without requiring changes to the overall application configuration. Changes in any of the application layers do not affect the other layers they interact with. For example, scaling or modifying the middleware layer will not impact the frontend tier.

Application Health

In most situations, application pods encounter issues because the code inside the pod stops responding. The presentation layer is typically where the website lives. The login layer handles all required data processing and is often called the middleware layer. The storage layer provides persistent storage for data, which can be databases, filesystems, or both. The application code runs in the pods for each layer, and that code is accessed directly from the routing layer, with each service communicating with the routing layer.

The first step in building a resilient application is to run automated health and status checks on your pods, restarting them when necessary without manual intervention. OpenShift has built-in support for creating probes to run the necessary checks on applications to ensure they are healthy.

The first type of probe we will look at is the liveness probe. In OpenShift, you define a liveness probe as a parameter for specific containers in the deployment configuration. The liveness probe configuration then propagates down to individual containers created in pods that are part of the deployment. A service on each node running the container is responsible for executing the liveness probe defined in the deployment configuration.

If the liveness probe is implemented as a script, it runs inside the container. If it is an HTTP or TCP socket-based probe, it is run by the node connecting to the container. If a liveness probe fails for a container, the pod is restarted. The service that executes liveness probe checks is called the kubelet service. This is the primary service running on each application node in the OpenShift cluster, and it is responsible for many other tasks, including monitoring the liveness status of the node.

Creating liveness probes

In OpenShift, the liveness probe component is a simple yet powerful concept that checks to ensure an application pod is running and healthy. Liveness probes can check container health in three ways.

First, an HTTP check verifies if a given URL endpoint served by the container is responding, and evaluates the HTTP status response code.

Second, a container execution check runs a command, typically a script, at intervals to verify that the container is behaving as expected. If the command returns a non-zero exit code, the liveness check fails.

Third, a TCP socket check verifies that a TCP connection can be established on a specific port in the application pod.

The HTTP response code is a three-digit number supplied by the server as part of the HTTP response headers in a web request. A two hundred series response indicates a successful connection, while a three hundred series response indicates an HTTP redirect. You can find more information about response codes on the IETF website.

As a best practice, always create a liveness probe unless your application is intelligent enough to exit when it reaches an unhealthy state. Create liveness probes that not only check the internal components of the application, but also isolate problems from external service dependencies. For example, a container should not fail its liveness probe simply because another service it depends on is not functional. Modern applications should have code to gracefully handle missing service dependencies.

If you need an application to wait for a missing service dependency, you can use readiness probes, which are covered later in this section. For legacy applications that require an ordered startup sequence of replicated pods, you can use a concept called stateful sets, which we will discuss later.

To make creating probes easier, OpenShift includes a health check wizard in its web interface. Using the wizard helps you avoid formatting issues that can arise from creating the raw YAML template by hand.

The example command shown here updates the deployment configuration for an application called app-cli by adding a liveness probe that checks the root URL on port eighty eighty, with an initial delay of five seconds. This ensures the application is checked for health shortly after it starts.

After adding the liveness probe, you can verify how it has been reflected in the final deployment configuration by inspecting the deployment object. You will notice a new line representing the liveness probe of type HTTP, which checks the root context path of the application. Other important parameters include the timeout, which is the time to wait for a response from the endpoint, the period, which specifies how often the check is performed, the success threshold, which means one successful response indicates the container is alive, and the failure threshold, which means that after three failures, the pod and container will be restarted.

The next command inspects and displays the liveness probe that was recently configured. Most of the deployment data is omitted, focusing only on the probe information. This output shows the deployment name, the container image, the ports, and the liveness probe configuration, including the HTTP endpoint, delay, timeout, period, and thresholds.

If you were to configure the liveness probe manually in a YAML manifest file, the configuration would include the HTTP GET path, port, scheme, timeout in seconds, period in seconds, success threshold, and failure threshold. This information matches what you would observe in the describe command output.

Creating readiness probes

Many applications need to perform a combination of tasks before they are able to receive traffic, which increases the time before an application is ready to do work. Some common tasks include loading classes into memory, initializing datasets and databases, performing internal checks, establishing connections to other containers or external services, and finishing a startup sequence or other workflow.


Fortunately, OpenShift supports readiness probes. These probes ensure that a container is truly ready to receive traffic before the pod is marked as active. Like the liveness probe, a readiness probe operates at the container level within a pod. It supports the same types of checks as the liveness probe, including HTTP, container execution, and TCP socket-based checks.

However, there is a key difference. If a readiness check fails, OpenShift does not deploy a new pod. Instead, the pod remains running, but it does not receive any traffic. This is in contrast to the liveness probe, where a failure can trigger a pod restart.

Let’s walk through an example of adding a readiness probe to the app-cli application using the command line. In this scenario, you will instruct OpenShift to check for a non-existent endpoint. This simulates a situation where the container is running, but the application inside is still starting up and not yet ready. By configuring the readiness probe to look for a URL that does not exist in your app-cli deployment, the probe will fail. This exercise demonstrates how OpenShift handles a probe that encounters an undesired condition.

Until a deployment passes its readiness probe, it will not receive user requests. If it never passes, as in this example, the deployment will fail and never become available to users.

To create the readiness probe, you use a command that updates the deployment. This command tells OpenShift to check a specific URL for readiness, and sets an initial delay before the first check. When you run this command, OpenShift deletes the old pods and spins up new ones to reflect the updated configuration.

The output confirms that the deployment was updated. Just like with a liveness probe, creating a readiness probe triggers a new deployment for app-cli.

To check whether the new pods were deployed, you can run a command that lists all pods. The output will show the current pods, their readiness status, and their age. In this example, you might see that some pods are running and ready, while a new pod is running but not yet ready.

Examining the pod details, you will notice that the liveness probe is functioning correctly and reporting no failures. However, the readiness probe is failing, which is expected since it is configured to check a non-existent endpoint.

The new pod is running, but it will never be marked as ready because the probe cannot find the specified URL. As a result, it is not ready to receive traffic. The previous pod remains running and continues to handle incoming requests.

Eventually, the readiness probe will fail multiple times, reaching the failure threshold, which is set to three by default. The failure threshold determines how many times a probe can fail before it is considered a failure.

It is important to note that the readiness probe will take about ten minutes to trigger a failed deployment. When this happens, the pod is deleted, and the deployment rolls back to the previous working configuration. This results in a new pod without the problematic readiness probe. You can adjust the default timeout parameters by changing the timeoutSeconds setting in the deployment object’s strategy parameters. Deployment strategies are discussed in more detail in later chapters.

Once all three failures occur, the deployment is marked as failed, and OpenShift automatically reverts to the previous deployment. The reason for the failure is displayed in the event view in OpenShift, which can be accessed from the command line or the web console. The web console provides a more user-friendly way to review these events. By expanding the events panel, you can see that the deployment failed and the specific reason for the failure.

To restore normal operation, you should update the readiness probe to point to a valid endpoint. This will cause the old pods to be dropped and new, healthy pods to be created, ensuring that both the liveness and readiness probes are satisfied.

Auto-scaling with metrics

In the previous section, we explored how to monitor the health and status of an application. OpenShift deployments use replication controllers to ensure that a specific number of pods are always running. This means the desired state, as configured by you, matches the actual state of the cluster.

Readiness and liveness probes help ensure that running pods start and behave as expected. The number of pods serving a workload can be easily changed to a new static number with a single command or a click of a button. This deployment model offers much better resource utilization compared to traditional virtual machines, but it is not a complete solution for operational efficiency.

One major challenge with virtual machines is resource utilization. Developers often request more CPU and memory than necessary, and making changes to these resources can be difficult. Many developers are unsure of the exact resources their applications need. Even at large companies like Google and Netflix, predicting application workload demand is so challenging that automated tools are used to scale applications as needed.

Determining expected workloads

Imagine you deploy a new application and it suddenly becomes extremely popular. External monitoring tools alert you that more pods are needed to handle the increased demand. Without historical data, it is difficult to predict how many pods will be needed in the future.

A great example is the mobile game Pokémon GO, which runs on Kubernetes. Within minutes of its release, demand skyrocketed, and over the opening weekend, it became an international sensation. Without the ability to dynamically provision pods on demand, the game could have crashed as millions of users overloaded the system.

In OpenShift, the process of automatically scaling pods up or down based on demand is called autoscaling. Developers can set limits and objective measures for scaling, while administrators can restrict the number of pods within a defined range. OpenShift uses pod metrics, such as CPU and memory usage, to determine if more or fewer pods are needed.

However, these pod metrics are not available by default. To use metrics in OpenShift, administrators must deploy the OpenShift metrics stack. This stack includes several popular open source technologies, such as Prometheus, Hawkular, Heapster, and Apache Cassandra. Once the metrics stack is installed, OpenShift autoscaling has the data it needs to scale pods automatically. The metrics stack can also be deployed during the initial OpenShift installation using the advanced installation option.

Recent versions of OpenShift offer the option to deploy Prometheus, a widely used open source monitoring and alerting solution, to provide and visualize cluster metrics. In the future, Prometheus may become the default metrics solution.

Installing OpenShift metrics

Installing the OpenShift metrics stack is straightforward. By default, the pods used to collect and process metrics run in the OpenShift infrastructure project, which is created during installation.

To switch to the OpenShift infrastructure project from the command line, you must first ensure you are logged in as an administrator. Regular users do not have access to system infrastructure projects and will not be able to see them.

The command to log in as an administrator requires your cluster’s IP address, the administrator username, and password. This step is necessary before you can proceed with installing or managing the metrics stack.


This is the important bit. Here, we are changing the default cluster version object, which controls certain flags and switches. These determine which features are enabled or not. In this case, we remove the first entry from the default overrides. This action restores the default behavior and enables metrics collection.

The first command describes the current cluster version object, showing its configuration and any overrides in place.

The next command patches the cluster version object. It removes the first override entry, which effectively reverts any custom behavior and brings back the default settings, including enabling metrics.

After making this change, the following command lists the pods in the openshift-monitoring namespace. It shows the status of various monitoring components, such as alertmanager, kube-state-metrics, metrics-server, node-exporter, prometheus, and others. Some pods are running, while others are pending, indicating their current state after the configuration change.

Understanding the metrics

In the previous section, you successfully deployed the OpenShift metrics stack. Three types of pods were deployed to make this happen, each with a different purpose, using technologies like Prometheus. However, none of these pods generate metrics themselves. Instead, the metrics come from kubelets.

A kubelet is a process that runs on each OpenShift node. It coordinates which tasks the node should execute with the OpenShift master. For example, when a replication controller requests that a pod be started, the OpenShift scheduler, which runs on a master node, eventually tasks an OpenShift node to start the pod. The command to start the pod is passed to the kubelet process running on the assigned node.

One of the additional responsibilities of the kubelet is to expose local metrics available to the Linux kernel through an HTTPS endpoint. The OpenShift metrics pods use the metrics exposed by the kubelet on each node as their data source.

Although the kubelet exposes metrics for individual nodes through HTTPS, there are no built-in tools to aggregate this information and present a cluster-wide view. This is where Prometheus comes in. Prometheus acts as the backend for metrics deployment. It queries the API server for the list of nodes, then queries each individual node to get metrics for the entire cluster. Prometheus stores these metrics in its internal data store.

On the frontend, the Prometheus pod processes the metrics. All metrics are exposed in the cluster through a common REST API, which allows metrics to be pulled into the OpenShift console. This API can also be used for integration with third-party tools or other monitoring solutions.

Using pod metrics and autoscaling

To implement pod autoscaling based on metrics, you need a couple of simple things. First, you need a metrics stack to pull and aggregate metrics from the entire cluster and make those metrics easily available. So far, so good.

Second, you need an object to monitor the metrics and trigger scaling up or down. This object is called a Horizontal Pod Autoscaler, or HPA. Remember that abbreviation. The main job of the HPA is to define when OpenShift should change the number of replicas in an application deployment.

Creating the HPA object

OpenShift provides a shortcut from the command line interface to create the HPA object. This shortcut is available through the oc autoscale command.

The following command creates an HPA object for the deployment named app-cli. It sets the minimum number of replicas to two, the maximum to five, and specifies that scaling should occur when CPU usage reaches seventy-five percent.

When you run this command, a couple of things happen. First, you trigger an automatic scale-up to two app-cli pods by setting the minimum number of pods to two. You can verify the number of app-cli pods by running a command to list all pods. You should see at least two running pods in the ready state, even if you previously had only one or none.

Second, the HPA object is created for you. By default, it has the same name as the deployment object, which in this case is app-cli. You can list all HPA objects to confirm its creation. In this example, only one deployment has an HPA object, and there is only one deployment anyway.

Describing the HPA object provides detailed information, such as its name, namespace, creation timestamp, reference to the deployment, metrics being monitored, minimum and maximum replicas, and the current number of deployment pods. It also shows the conditions and any recent events.

In the events section, you may see some errors, especially around reporting the fact that CPU utilization was not computed. You might notice the unknown state in the table when listing the HPA object. The message states that the HPA failed to get CPU utilization because there is a missing request for CPU in the container app-cli of a specific pod. This means that the pods themselves were not configured to have any CPU resource limits.

Remember, configuring the autoscaler only tells OpenShift when to scale the pods. However, if the pods have no resource restrictions, there is no way to compute any percentage of CPU utilization. Therefore, you must set a CPU request as well.

In OpenShift, a resource request is a threshold you can set that affects scheduling and quality of service. It essentially provides the minimum amount of resources guaranteed to the pod. For example, a user can set a CPU request of four-tenths of a core, written as four hundred milli-cores, or four hundred m. This tells OpenShift to schedule the pod on nodes that can guarantee there will always be at least four hundred m of CPU time.

CPU is measured in units called milli-cores. By default, pods do not get individual cores. Instead, they get time slices of CPU, sharing the cores on the node with other pods. If a particular node has four CPUs assigned to it, then four thousand m are available to all the running pods on that node.

Resource requests can also be combined with a resource limit, which is similar to a request but sets the maximum amount of resources guaranteed to the pod.

Setting requests and limits also allows the user to set a quality of service level by default. There are three levels:

BestEffort, where neither a resource request nor a limit is specified. This is for low-priority applications that can live with very low amounts of processing and memory resources.

Burstable, where a request is set, indicating a minimum amount of resources allocated to the pod.

Guaranteed, where a request and a limit are both set to the same number. This is for the highest-priority applications that need the most consistent amount of computing power.


Setting a lower quality of service gives the scheduler more flexibility, allowing it to place more pods throughout the cluster. In contrast, setting a higher quality of service limits this flexibility, but provides applications with more consistent resources. Since choosing the right quality of service is about finding reasonable defaults, most applications should use the Burstable setting.

To set a CPU request, you can use a command that updates the deployment’s resource requests. For example, you might set the CPU request for a deployment named app-cli to four hundred millicores. When you make this change, a new deployment configuration object is created. This triggers the creation of new pods, which will replace the current ones once the new pods are ready.

After making these changes, you can list the Horizontal Pod Autoscaler, or HPA, objects again to check for any issues. Once the setup is complete, you can proceed to test the autoscaling.

Testing the autoscaling setup

To demonstrate that autoscaling works as expected, you need to trigger the CPU threshold that was previously set. To help reach this threshold, you can use the Apache benchmark tool, which comes pre-installed with CentOS and is already available in your system path.

Before running the benchmarking test, make sure you are logged into the OpenShift console in another window. This allows you to watch as pods are spun up and down in real time. Navigate to the overview page for the image-uploader project, then run a command that sends a large number of requests—such as one hundred thousand requests with one thousand concurrent connections—to the application’s endpoint. This will generate overwhelming CPU usage and force OpenShift, along with its monitoring service, to scale up more pods.

If the CPU threshold is reached, you can describe the HPA object and observe events indicating that the number of pods was automatically increased, for example, from three to four, based on CPU utilization requirements. The actual number of pods will depend on your local system’s capabilities. Eventually, these pods will be scaled down, but this process takes time. This delay is intentional and helps avoid a problem known as thrashing, which is discussed in the next section.

Avoiding thrashing

When the Apache benchmark test is started, the OpenShift autoscaler detects very high CPU usage on the deployed pods, which violates the HPA constraints. This causes new pods to be spun up on demand. Behind the scenes, the deployment is modified to create a new number of replicas. After the tests are completed and CPU usage drops, the new pods will eventually be spun down, but this can take several minutes.

By default, the HPA synchronizes with Prometheus metrics every thirty seconds. You can modify this synchronization period in the master configuration file. This time window is designed to avoid thrashing, which is the constant starting and stopping of pods unnecessarily. Thrashing wastes resources, since deploying new pods consumes resources to schedule and deploy them, often including loading applications and libraries into memory.

After OpenShift triggers an initial scale, there is a forbidden window of time to prevent thrashing. The rationale is that if there is a need to constantly scale up and down within a few minutes, it is probably less expensive to keep the pods running than to continuously trigger scaling changes. In OpenShift versions up to three point six, the forbidden window is hard-coded at five minutes for scaling down and three minutes for scaling up. In later versions, these default values remain the same, but they can be modified using controller manager arguments and the horizontal pod autoscaler downscale delay.

Continuous integration and deployment

Deploying software into production is difficult. One major challenge is adequately testing applications before they reach production. Adequate testing requires consistent environments, which has been a longstanding challenge in information technology. For many organizations, setting up new environments for development, testing, and quality assurance is time-consuming. When these environments are finally in place, they are often inconsistent.

These inconsistencies develop over time due to poor configuration management, partial fixes, and direct patches made in production environments. Inconsistent environments can lead to unpredictable software behavior. To eliminate this risk, organizations often schedule maintenance windows during software deployments and hope for the best.

Over the past fifteen years, there have been many attempts to improve software processes. The most notable has been the industry-wide shift from the waterfall method of deploying software to more flexible approaches, such as Agile. Agile aims to reduce risk by performing many small, iterative deployments instead of massive software rollouts. However, Agile falls short in several areas because it focuses on software development and does not address the efficiency of other stakeholders in the organization. For example, code may reach operations quickly, but a bottleneck can occur if operations must deploy code more frequently.

Many organizations are addressing these problems with a modern DevOps approach. DevOps brings together all stakeholders to work jointly throughout the software development lifecycle. DevOps is now almost synonymous with automation, continuous integration, and continuous deployment—often referred to as CI and CD. The delivery mechanism for implementing this is called a software deployment pipeline, which is closely tied to container technology.

Container images are the centerpiece

From a technology perspective, containers have become the most important component in the software deployment pipeline. Developers can build applications and services without needing to design or even consider the underlying infrastructure. Operations teams can spend less time designing application installations. Applications and services can be easily moved between environments, such as quality assurance and testing, as well as between on-premises and public cloud environments like Amazon Web Services, Microsoft Azure, and Google Compute Platform.

When applications need to be modified, developers package new container images that include the application, configuration, and runtime dependencies. The container then moves through the software deployment pipeline, undergoing automated testing and processing. Using container images in the deployment pipeline reduces risk because the exact same binary runs in every environment. If a change is needed, it begins in the sandbox or development environment, and the entire deployment process starts over.

Since running containers are created from container images, there is no way to fix things directly in production. If someone tries to patch a running container in production, the change will not persist. The change must be made to the underlying container image. By making the container the centerpiece of the deployment pipeline, system stability and application resiliency are greatly increased.

When failures occur, identifying issues and rolling back software is faster because the container can be rolled back to a previous version. This is a significant improvement over previous approaches, where entire application servers and databases might need to be reconfigured in parallel to roll back the entire system.

Additionally, containers allow developers to run more meaningful tests earlier in the development cycle, since they have environments that closely mimic production on their laptops. A developer can reasonably simulate production load and performance testing on the container during development. The result is higher quality, more reliable software updates. Better and more efficient testing also leads to less work in progress and fewer bottlenecks, which means faster updates.

Promoting images


In this section, you will build a complete pipeline in OpenShift. To fulfill the goal of using the same binary in every environment, you will build your image only once in your development environment. After that, you will use image tagging to indicate when the image is ready to be promoted to other projects. To make this process easier, you will use Jenkins, along with some additional OpenShift concepts that you will learn about as you progress.

Jenkins is an open source automation server that is widely used as the backbone for continuous integration and continuous delivery pipelines. It is popular because it offers many plugins for existing tools and technologies. Jenkins often becomes a Swiss army knife, integrating different technologies into a single pipeline.

CI/CD: Creating an environment

The first part of any continuous integration and continuous delivery pipeline is the development environment. In this environment, container images are built and tested. If they pass their tests, they are tagged for promotion. All container builds happen here.

You will use a pre-built template to quickly set up a simple application that runs on Python and uses MongoDB as its database. The template also provides an open source Git repository called Gogs, which comes pre-installed with the application. PostgreSQL is also provided as a database for Gogs.

This section will make extensive use of OpenShift templates to install applications. An OpenShift template is essentially a collection of objects that can be parameterized and spun up on demand. In most cases, the API objects created as part of the template are all part of the same application, but that is not a strict requirement.

Using OpenShift templates offers several advantages over manually importing objects. You can provide parameterized values at creation time. Values can be generated dynamically, such as randomly generated database passwords. Messages can be displayed to the user in the console or on the command line interface, typically including information on how to use the application. You can also create labels that are applied to all objects in the template. Additionally, part of the OpenShift API allows templates to be instantiated programmatically, even without a local copy of the template.

OpenShift comes with many templates out of the box. You can view them through the service catalog or by running a command to list templates in the OpenShift namespace. To see the raw template files, you can navigate to the examples directory on your system.

The previous code block shows a command that lists the templates installed in the default OpenShift distribution. This list is only a portion of the default templates. There are even more templates available on platforms like GitHub, distributed by the community, and proprietary ones also exist. The output includes template names, descriptions, the number of parameters, and the number of objects each template creates. For example, there are templates for Jenkins, MariaDB, MySQL, Nginx, Node.js with PostgreSQL, Java applications, PostgreSQL, Rails with PostgreSQL, React web applications, and Redis, among others.

Now, let's create your development environment. The following steps will create a new project and set up the necessary template for your continuous integration and delivery pipeline.

First, you create a new project called "dev" with a display name of "ToDo App - DEV." Next, you configure the template by applying a file that contains the template definition. Finally, you instantiate the template, which creates a new application using all the objects defined in the template. The benefit of using a template is that you can easily reuse it to duplicate applications or replicate common setups and configurations.

Deployment strategies

So far, you have learned how to build a container image and automate its promotion across different environments using OpenShift's native automation and Jenkins integration. However, we have not yet discussed the exact sequence for how a new version of the application is rolled out. The method you use to update your application in OpenShift is called a deployment strategy. This is a critical component for supporting a wide variety of applications on the platform.

OpenShift supports several deployment strategies.

The rolling strategy is the default. When pods with the new image become ready by passing their readiness checks, they gradually replace the old pods one by one. You set this deployment strategy in the deployment configuration object.

The re-create strategy scales down all pods with the old image to zero, then begins deploying the new pods. This approach results in a brief downtime while waiting for the new pods to start. Like the rolling strategy, you set this in the deployment configuration object.

The blue-green strategy focuses on reducing risk by standing up pods with the new image while the old pods remain running. This allows you to test your code in a production environment. Once the code has been fully tested, all new requests are sent to the new deployment. OpenShift implements this strategy using routes.

The canary strategy adds checkpoints to the blue-green approach by rolling out a fraction of the new pods at a time, then pausing. This allows you to test the application before rolling out more pods. Like blue-green deployments, this strategy is implemented using OpenShift routes.

Finally, the dark launch strategy rolls out new code but does not make it available to users. This allows you to test how the new code works in production. Later, you can enable the features for users when it is determined to be safe. This strategy has become popular at companies like Facebook and Google. To accomplish dark launches, the application code must check for certain environment variables that enable the new features. In OpenShift, you can take advantage of this by toggling the new features on or off by setting the appropriate environment variables for the application deployment.


There are many factors to consider when choosing a deployment strategy for your application. The rolling strategy is the fastest way to upgrade your application without causing downtime. However, it does this by running your old code alongside your new code during the update. For many stateful applications, such as clustered applications and databases, this can create problems.

Imagine your new deployment introduces a new database schema, has a long-running transaction, shares persistent storage among all the pods, or uses clustering to dynamically discover other pods. In these situations, it is better to use a re-create strategy instead of a rolling update. Databases, in particular, almost always use the re-create strategy.

You can check which strategy is being used by inspecting the deployment object for your target application. Stateless applications are a good fit for rolling upgrades, while databases are better suited to the re-create strategy.

Both the rolling and re-create strategies offer extensible options. You can adjust parameters to control the timing of rollouts, and both provide lifecycle hooks that let you inject code during the deployment process.

Many users also enhance their deployment strategies by adding blue-green or canary style deployments. This is done by combining OpenShift routes with either rolling update or re-create strategies. For applications using rolling deployment, adding blue-green or canary deployments reduces risk by allowing a more controlled rollout with checkpoints. For applications using the re-create strategy, these features help avoid downtime.

Both blue-green and canary deployments use OpenShift routes to manage traffic across multiple services. To implement these strategies, you create a complete copy of the application with the new code. This copy includes all the necessary objects, such as deployments, services, replication controllers, and pods. Once you have tested the new code, you update the OpenShift route to point to the service running the new code.

A blue-green deployment allows you to test code in production. Since the old code is still running, you can quickly roll back if something goes wrong. The downside is that blue-green deployments require more infrastructure, because you need to run both versions of the application at the same time.

A canary deployment is similar to blue-green, but instead of switching all traffic at once, it uses weights to control what percentage of traffic goes to the new and old services. You can adjust these weights during the rollout using the OpenShift command line tool or the console.

Here is a brief summary of the topics we have covered so far. Image streams enable automation and consistency for container images. OpenShift triggers allow event-based image builds and deployments. DNS can be used for service discovery, and environment variables can also be used for service discovery if dependencies are installed first. Image tagging automates the promotion of images between environments. Secrets are used to mask sensitive data that should be separated from an image. Config maps provide startup arguments, environment variables, or files that are mounted in an image. OpenShift offers a Jenkins instance or template with many useful plugins pre-installed, allowing you to run and monitor Jenkinsfile pipelines from the OpenShift console. Finally, OpenShift supports many types of deployment strategies for a wide variety of applications.

Stateful applications

The first application we deployed in OpenShift was a PHP web app called the image uploader. This application has a few simple features. When you click an image, it shows you the full-size version. It displays images as thumbnails on the main page. You can upload images from your workstation, and it verifies that the files you upload are standard image formats.

While it is not a complex application, its simplicity makes it easy to edit and manage in OpenShift. If you have not already done so, try out the image app and upload a few images. After uploading, you should see your images displayed as thumbnails on the home page.

When you deploy an application in OpenShift, you can specify the minimum number of replica instances to keep running at all times. If you do not specify a number, OpenShift will always keep at least one instance running. We discussed this earlier and used it to scale the image uploader to more than one replica. However, none of these replicas had persistent storage. If an application pod was deleted or scaled down, any data it had written would be lost.

Let’s test this scenario.

Container storage

After logging into your OpenShift cluster from the command line using the oc tool, you can list all your running pods. The output will show that the app-cli has a few running pods and a few completed ones, which are the build pods. The running pods are your actual application.

Next, try deleting one or both of the application pods. OpenShift will automatically create new pods to maintain the desired number of replicas. If you list the pods again, you will see that the new pod has a recent age, showing that OpenShift quickly replaced the deleted pod.

The code snippet in the original text demonstrates logging in to the cluster, listing pods, deleting a pod, and listing pods again to observe the changes.

At first glance, this might seem like a complete solution—applications restart themselves when they go down. However, if you check your application’s web page after deleting a pod, you will notice that the images you uploaded earlier are gone. This is because the storage used by a pod’s file system in OpenShift is ephemeral. It does not persist across pod restarts or replacements.

When your application needs to store data permanently, you need to set up persistent storage in OpenShift.

Handling permanency

In OpenShift, persistent storage is available for data that needs to be shared between pods or must persist beyond the lifetime of any single pod. This is managed using persistent volumes, or PVs. Persistent volumes in OpenShift use industry-standard, network-based storage solutions to manage data.

OpenShift supports a wide range of storage solutions for creating persistent volumes. Some of the main options include the following.

First, the Container Storage Interface, or CSI, defines a standard way for container orchestration systems like Kubernetes to expose different storage systems to their workloads. Once a CSI-compatible volume driver is deployed on your cluster, you can use the CSI volume type to attach or mount volumes. A pod can reference a persistent volume claim, which in turn references a persistent volume that specifies the CSI driver to use.

Second, NFS, or Network File System, volumes allow an existing NFS share to be mounted into a pod. Unlike temporary storage, the contents of an NFS volume are preserved when a pod is removed—the volume is simply unmounted. This means you can pre-populate an NFS volume with data and share it between multiple pods. NFS supports simultaneous access by multiple writers. You must have your own NFS server running with the share exported before you can use it. NFS mount options must be specified in the persistent volume definition, not in the pod specification.

Third, HostPath volumes allow you to mount local directories from the OpenShift nodes themselves into your pods. These directories are not ephemeral. HostPath is not commonly needed by most pods, but it provides a powerful option for certain applications that require direct access to the node’s file system.

These are just a few of the persistent storage options available in OpenShift. Each has its own use cases and requirements, and choosing the right one depends on your application’s needs.


Local Volumes and HostPath

A local volume in Kubernetes represents a mounted local storage device, such as a disk partition or a directory. Local volumes can only be used as statically created persistent volume objects. Unlike some other storage types, dynamic provisioning is not supported for local volumes. Compared to hostPath volumes, local volumes are designed to be more durable and portable, and you do not need to manually schedule pods to specific nodes. The system is aware of the volume’s node constraints by checking the node affinity on the persistent volume. However, local volumes are still tied to the availability of the underlying node. This means they are not suitable for all applications, especially those that require high availability. If the node goes down, the pod using the local volume cannot run. Applications using local volumes must be able to tolerate this reduced availability and the potential for data loss, depending on the durability of the underlying disk.

It is important to note that using the hostPath volume type presents significant security risks. If possible, you should avoid using hostPath volumes. Instead, define a local persistent volume and use that. If you must restrict access to specific directories on the node using admission-time validation, that restriction is only effective if you also require that any mounts of that hostPath volume are read-only. Allowing a read-write mount of any host path by an untrusted pod can let containers in that pod subvert the read-write host mount.

Be cautious when using hostPath volumes, whether they are mounted as read-only or read-write. Access to the host file system can expose privileged system credentials, such as those for the kubelet or privileged API keys. Pods with identical configurations, such as those created from a pod template, may behave differently on different nodes because the files on the nodes can differ. HostPath volume usage is not treated as ephemeral storage usage, so you need to monitor disk usage yourself. Excessive hostPath disk usage can lead to disk pressure on the node.

In the next chapter, we will configure a persistent volume in OpenShift using the network file system.

Creating Resources

In OpenShift, network storage types are used to make storage available across all nodes in a cluster. For the examples in the following sections, you will use a persistent volume built with NFS storage. The first step is to export an NFS volume on your OpenShift master node.

As discussed earlier, your OpenShift cluster is currently configured to allow any user to log in as long as their password is not empty. Each new username is added to a local database at first login. You previously created a user named “dev” and used that user to create a project and deploy the app-cli. The dev user can create projects and deploy apps, but does not have the permissions needed to make cluster-wide changes, such as attaching a persistent volume. We will explore user management in OpenShift in more detail later, but for now, to create a persistent volume, you need an admin-level user in the OpenShift cluster. Fortunately, you can obtain that access.

The following steps summarize how to prepare your environment for NFS-based persistent volumes:

First, you switch to the root user to avoid permission issues.

Next, you list the available disks to identify the main partition. In this example, the main partition is labeled vda4, and the rest are system storage partitions for the virtual machine, such as the boot partition.

You then remount the /var directory as read-write to ensure you can make changes, and create a directory called /var/nfs-share to store the network file system data.

After that, you check the block identifier for your storage, confirming that vda4 is the correct partition.

You also check the firewall rules to ensure that the NFS server port, which is port 2049, is open. If it is not, you add a rule to allow traffic on that port.

NFS relies on several services: rpcbind, which handles the RPC protocol for data transfer; nfs-server, which is the main NFS server service; nfs-lock, which manages file locking for NFS volumes; and nfs-idmap, which handles user and group mapping for NFS volumes. You enable and start all these services to ensure NFS operates correctly.

Creating Storage

Now that you have created the persistent volumes, it is time to use them in your OpenShift application. Applications consume persistent storage using persistent volume claims, or PVCs. A persistent volume claim can be added to an application as a volume, either through the command line or the web interface. Let’s create a PVC on the command line and add it to the application.

To understand how persistent volume claims match up with persistent volumes, remember that in OpenShift, persistent volumes represent the available storage, while persistent volume claims represent an application’s need for that storage. When you create a PVC, OpenShift looks for the best fit among the available persistent volumes and reserves it for use by the PVC.

In this environment, matches are based on two criteria. First, persistent volume size: OpenShift tries to use resources efficiently, so when a PVC is created, it reserves the smallest persistent volume available that meets its needs. Second, data access mode: when a PVC is created, OpenShift looks for an available persistent volume with at least the required level of access. If an exact match is not available, it reserves a persistent volume with more privileges that still satisfies the requirements. For example, if a PVC requests a persistent volume with read-write access, OpenShift will use a persistent volume with read-write-many access if one is available.

Because all the persistent volumes in your environment are the same size, matching them to a PVC will be straightforward. Next, you will create a PVC for your application to use.

Before that, you need to enable NFS capabilities on the server. This means creating and enabling the mount paths that will be used in the persistent volume. The following script automates this process. It creates five directories, which can be used by five different persistent volume claims. These directories are named from pv zero one to pv zero five, and are located under the directory /var/nfs-share.

The script defines a variable for the number of persistent volumes to create, and a prefix for naming. It then loops over the number of persistent volumes, creating a new directory for each one under the specified path. It also appends an export entry for each directory to the /etc/exports file, which is used by NFS to define shared directories.

After creating the directories, the script sets their permissions to be very permissive, ensuring that containers can read from and write to them. It also changes the ownership to the nfsnobody user and group, which is standard for NFS shares. Finally, it displays the exported directories, which will be referenced later in the manifest files for storage.

To use the script, you make it executable and run it as sudo. After running the script, you can check the /etc/exports file to see all the exported volumes. You should see entries for each of the five directories, each configured with read-write access, synchronous writes, and no root squashing.

Logging in as kubeadmin

Next, you will log in as the kubeadmin user to perform administrative tasks in OpenShift.


When an OpenShift cluster is installed, it automatically creates a configuration file for a special administrative user. This user is typically named either kubeadmin or system:admin, depending on the specific OpenShift distribution you are using.

The admin user is authenticated using an SSL certificate, regardless of which authentication provider is configured for the cluster. This user has full administrative privileges over the entire OpenShift cluster. The key certificate for the admin user is placed in the Kubernetes configuration files, which are usually found in the .kube directory in your home folder. This setup makes it easier to run commands as the admin user.

You can also list the credentials of users directly from the command line, or you might see the credentials displayed in your terminal when the cluster is being started for the first time. For example, when starting a cluster using the CodeReady Containers, or CRC, deployment method, you might see output that tells you the web console address, as well as the usernames and passwords for both the administrator and the developer accounts. The output will show the web console URL, the admin username as kubeadmin, and a generated password. It will also show the developer username and password, which are often set to developer by default.

This example is specific to the CRC deployment method, but you will see similar output if you use the Minishift deployment method. It is important to take note of the cluster’s IP address, as this will be your point of entry to both the API server and the user interface.

If you are unable to locate the password for the admin or developer accounts, and you are using either CRC or Minishift, check your home directory for folders named .crc or .minishift. Inside these folders, you will find files that store the credential details. For CRC, the admin password is typically stored in a file called kubeadmin-password, located in the .crc directory under machines and then crc. For Minishift, look for a similar path under .minishift.

Physical volume

To create a resource from a YAML template, use the oc create or oc apply command, along with the dash f parameter, which specifies the template file you want to apply. For example, to create a persistent volume, you would save the template to a file and then run a command that applies it. This command would look like oc apply dash f, followed by the path to your YAML file.

Let’s discuss some of the options used in the persistent volume template. The access mode of the persistent volume determines how different pods can access the volume. This is important for managing race conditions and resource locks that might occur when data is being accessed, whether for reading or writing.

There are three main access modes:

Read Write Once, or RWO, allows the volume to be mounted as read-write by a single node in the OpenShift cluster. This is useful for workloads where a single application pod will be writing data, such as a relational database server.

Read Only Many, or ROX, allows the volume to be mounted as read-only by multiple OpenShift nodes. This is helpful for horizontally scalable web applications that need access to the same static content, like images.

Read Write Many, or RWX, allows multiple nodes to mount the volume and both read from and write to it. This is the access mode you will use for the persistent volume in this section. For example, if you have an image uploader application that you plan to scale up, multiple nodes will need to read and write to the persistent storage.

A reclaim policy determines how the persistent volume handles storage after a claim is no longer needed. There are two main options:

Retain, which keeps all data in the volume and requires manual intervention to reclaim space.

Recycle, which automatically removes data when the claim is deleted. This is the policy we will use for the persistent volume in this section.

Creating volumes

OpenShift relies heavily on configuration files written in YAML. YAML is a human-readable language commonly used for configuration files and data serialization. It is the default way to push data into Kubernetes and, by extension, into OpenShift.

Previously, we discussed OpenShift resources that are created when an application is built and deployed. These resources have documented YAML templates, making it easy to create and manage them. When you create a new application using the new-app command, OpenShift automatically generates and stores these templates.

In this section, you will use a template to create a persistent volume. This template acts as a specification and includes a version number, which tells Kubernetes which revision of the specification to use. Different versions may have differences in layout and structure.

The YAML template for a persistent volume in this example specifies several key details. It sets the API version to version one, and the kind of resource to PersistentVolume. The metadata section gives the resource a name, in this case, pv zero one. The spec section defines the storage capacity as two gigabytes, sets the access mode to Read Write Many, and specifies the NFS server and path for the volume. The persistent volume reclaim policy is set to Recycle.

After applying this template, you can use the oc get pv command to see the newly created persistent volume. This command lists the persistent volumes, showing their names, capacities, access modes, reclaim policies, and statuses. In this example, you would see a volume named pv zero one, with a capacity of two gigabytes, access mode set to RWX, reclaim policy set to Recycle, and status as Available.

So, what type of information does this template contain? At the very start, it specifies the version of the object, which is v one. It then defines the type of resource to create, which is a persistent volume. The template gives the resource a unique name, pv zero one, which is required by Kubernetes. It sets the storage capacity to two gigabytes. The access mode is specified, and the NFS path and server are provided. If you used a different IP address for your master or another server, you would need to edit this value. Finally, the template sets the recycle policy for the persistent volume, which determines how data is disposed of once it is no longer needed.

Creating claims

Now, you will do something similar for the persistent volume claim, or PVC. You will create it from a YAML template, filling in the necessary fields to attach the PVC to your application. This will also match the PVC with the persistent volume you already created.

There are a few important parameters to note. First, the name of the PVC must be unique, as is required for all Kubernetes and OpenShift objects. Second, the access mode of the PVC should match the access mode of the persistent volume. In this example, the PVC will request Read Write Many, or RWX, which aligns with the persistent volumes created earlier. Third, the size of the storage requested should match the size of the persistent volume, which in this example is two gigabytes.


Persistent Volume Claim Example

Let’s start by looking at a sample Persistent Volume Claim, or PVC, configuration. In this example, the claim is named “pvc01.” The configuration is set up to bind specifically to a persistent volume called “pv01.” This explicit binding is not always necessary, but here it ensures that the claim uses the correct persistent volume that was already created. The storage class name is set to empty, which is required in this context to avoid errors when applying the configuration. The access mode is set to allow multiple pods to read and write to the volume at the same time, and the requested storage size is two gigabytes.

Applying the PVC Configuration

To apply this configuration, first save it into a file. Then, use the OpenShift command-line tool to apply it. The command to do this is “oc apply -f pv01 dot y m l.” Unlike persistent volumes, which are typically created by administrators, persistent volume claims are created by developers for use by pods. When a new deployment configuration is created, the pod refers to the volume claim by name. Developers should create the claim before the pod is deployed. It’s important to remember that the PVC must be created in the same project as the application that will use it.

When the PVC is created, it queries OpenShift for available persistent volumes. It uses the specified criteria to find the best match and then reserves that persistent volume. Depending on the size of your cluster, it may take a minute or so for the PVC to become available for use as persistent storage in an application.

Checking PVC Status

You can use the “oc” command-line tool to get information about active persistent volume claims in your OpenShift project. For example, you can run a command to list all PVCs in the current project. If you want to ensure you’re working in the correct project, either use the “dash n” flag with your command or set the active project context with “oc project” followed by the project name.

When a persistent volume claim is first created, it is lazily initialized. This means it will remain in a pending status until at least one pod is actually using it. You might see an event message indicating that the system is waiting for the first consumer to be created before binding the claim.

A PVC represents reserved storage available to applications in your project, but it is not yet mounted into an active application. To use the storage, you need to mount the newly created PVC into an application as a volume.

Modifying the Deployment to Use the PVC

In OpenShift, a volume is any file system or data mounted into an application pod to provide persistent data. Here, we are focusing on persistent storage volumes. Volumes can also be used for encrypted data, application configuration, and other types of data.

To add a volume, use the “oc set volume” command. In the following example, the newly created PVC is added to the “app-cli” application. The command specifies the type of volume, the name of the claim, and the mount path inside the container. Any data read from or written to this path will go directly to the persistent volume, which in this case is backed by an NFS server.

After running this command, if you describe the deployment, you will see that the volume is now attached to the deployment. The output will show the name of the volume, which is automatically generated if not specified, and the claim it references. This confirms that the deployment is using the correct persistent volume claim.

By applying the volume to the deployment, OpenShift will automatically trigger a redeployment of the application to incorporate the new persistent volume. The parameters specified in the command are required and define the minimum information needed to create the volume for your application. Optionally, you can specify a custom name for the volume. If not set, OpenShift generates one dynamically.

Describing the Deployment

Using the “oc describe” command, you can list the details of the deployment. This will show that the volume is attached to the deployment and, more specifically, to the pod itself. The output will indicate that the volume was created, linked to the persistent volume claim, and in turn, that claim is linked to the persistent volume object, which is an NFS volume configured on the cluster node.

In the deployment specification, you will see a section where the volume is attached to the deployment. The volume is given an automatic name and is linked to the claim you created earlier.

Because the persistent volume and claims are set up to allow multiple reads and writes using the “ReadWriteMany” access mode, when the application scales horizontally, each new pod will mount the same PVC and be able to read and write data to it.

In summary, you have just modified your containerized application to provide horizontally scalable persistent storage.

Testing the Application

To test this setup, open your browser and visit the web page for the image upload application. The URL is “http://app-cli-image-uploader.apps-crc.testing/.” Upload a new image and remember the name of the file you upload. This will be used later to verify that the file was correctly uploaded to the host or node, under the directory specified in the persistent claim.

Volume Mounts on the Node

Since we are using an NFS server as the source for the persistent volume, the NFS volumes are mounted and created somewhere on the OpenShift nodes. You can verify this by connecting to the OpenShift node where the containers are running. After gaining SSH access to the node, run the “mount” command and search for the mounted volumes from the IP address of the NFS server. The IP address of the OpenShift cluster typically follows the format “one ninety-two dot one sixty-eight dot X dot X.”

You will see multiple results, one for each pod running the application. Earlier, you used the “oc get pv” command to confirm that the persistent volume was being used by the app-cli application. However, this does not explain how the NFS volume is made available in the app-cli container’s mount namespace.

By examining the mounted volumes on the node, you can see exactly how the NFS storage is integrated into the pods, providing persistent, shared storage for your application.


Earlier, in the previous section, we explored how the file system inside a container is kept separate from other containers running on the same cluster host, as well as from the host itself. This isolation is achieved through Linux namespaces, and more specifically, the mount namespace. The mount namespace is a fundamental part of how the Linux kernel manages containerized applications.

However, when it comes to persistent volume mounts, things work a bit differently. The persistent volume is not directly added to the mount namespace of the app-cli containers. Instead, the NFS mount is made available inside the container using a technology called a bind mount.

A bind mount in Linux is a special type of mount where a part of the file system is made accessible at an additional location. For the app-cli container, the NFS mount for the persistent volume is bind-mounted at the path slash opt slash app-root slash src slash uploads, inside the container’s mount namespace. Using a bind mount means the content is available in two places at once. Any change made in one location is instantly reflected in the other.

Bind mounts are used for volumes for two main reasons. First, creating a bind mount on a Linux system is lightweight in terms of performance and CPU usage. This means that when a new container replaces an old one, there’s no need to remount the remote volume, which keeps container startup times low. Second, this approach separates the concerns of persistent storage. With bind mounts, the container definition does not need to include details about the remote volume itself. The container only needs to specify the name of the volume to mount. OpenShift handles the details of accessing the remote volume and making it available to the containers.

This separation of concerns between cluster administration and application usage is a consistent design feature in OpenShift.

To see this in action, you can navigate to the directory slash var slash nfs-share slash app-cli-pv01 on the cluster node. There, you will find the uploaded file or image. This demonstrates that we have successfully mounted a persistent volume claim from the host, using a persistent volume object. The uploaded file, such as image dot j p g, will be present in that directory, confirming that the persistent storage is working as intended.

The goal of this section was to guide you through configuring the components that make persistent storage available to a container in OpenShift. In the following sections, we will use persistent storage to build more scalable and resilient applications.

Stateful applications

Previously, we created persistent storage for the image upload pods. This allowed data to persist beyond the life of a single pod. When a pod failed, a new pod was started in its place and mounted the existing persistent volume locally. Persistent storage in OpenShift enables many stateful applications to run in containers.

However, some stateful applications have requirements that go beyond persistent storage. For example, many workloads distribute data through replication, which requires application-level clustering. In OpenShift, this kind of data replication needs direct pod-to-pod networking, without passing through the service layer. It’s also common for stateful applications, such as databases, to have their own custom load balancing and discovery algorithms, which require direct access between pods.

Other typical requirements for stateful applications include support for sticky services and sessions, as well as the ability to perform predictable, graceful shutdowns.

One of the main goals of the OpenShift container platform is to be a world-class platform for both stateless and stateful applications. To support stateful applications, OpenShift provides a variety of tools to help make almost any application container-native. In this chapter, we’ll walk through the most popular tools, including headless services, sticky sessions, pod discovery techniques, and stateful sets, among others. By the end of this section, you’ll see the power of stateful sets, which bring many stateful applications to life on OpenShift.

Enabling a headless service

A good example of application clustering in everyday life is Amazon’s virtual shopping cart. When customers browse for items and add them to their cart, the cart data is stored so it can be purchased later. If a user is signed in, their cart is stored permanently in a database. For users who are not signed in, the cart is temporary and implemented as an in-memory cache in Amazon’s data centers.

Using in-memory caching gives users fast performance and a better experience. However, if a server crashes, the cached data is lost. A common solution is data replication—when an application puts data in memory, it can be replicated to multiple caches, providing both speed and redundancy.

Before applications can replicate data among themselves, they need a way to dynamically find each other. Previously, we discussed service discovery, where pods use the OpenShift service object. The service object provides a stable IP address and port to access one or more pods. For most cases, this is enough. But some applications, especially those that replicate data, need to find and access each pod in a service directly, on demand.

One way to achieve this is to create a separate service object for each pod, giving each pod a stable IP and port. While this works, it can lead to a large number of service objects, which can be hard to manage.

A better solution is to use a headless service and discover application pods using application-specific discovery methods. A headless service is a service object that does not load balance or proxy between backend pods. It is implemented by setting the spec dot cluster I P field to None in the service API object.

Headless services are most often used for applications that need to access specific pods directly, without going through the service proxy. Two common examples are clustered databases and applications with client-side load balancing logic. Later in this chapter, we’ll look at a headless service example using MongoDB, a popular NoSQL database.

Application clustering with WildFly

Now, let’s look at a classic example of application-level clustering in OpenShift using WildFly, a popular application server for Java-based runtimes. In this section, you’ll deploy a new application, so start by creating a new project.

The command to create a new project in OpenShift is used here. This sets up a new project called stateful-apps, and provides instructions for adding applications to the project, either with the new-app command or by using kubectl to deploy a simple Kubernetes application.

It’s important to note that this example uses cookies stored in your browser to track your session. Cookies are small pieces of data that servers ask your browser to hold, to improve your experience. In this case, a cookie will be stored in your browser with a unique identifier—a randomly generated string called JSESSIONID.

When a user first accesses the web application, the server responds with a cookie containing the Java session ID field and a unique value. On subsequent visits, the application uses the JSESSIONID to look up all information about the user’s session, which is stored in a replication cache. It doesn’t matter which pod the user accesses—the experience will be the same.

The WildFly application you’ll deploy replicates user data among all the pods in its service. The application tracks which user a request comes from by checking the JSESSIONID passed from the browser cookie. Because the user data is replicated, the end user will have a consistent experience, even if some pods fail and new ones are started.

To create the new application, you use an oc command that creates a template for the application and all required dependencies. This command sets up the necessary resources for the stateful application in the stateful-apps project.


To see the list of available templates, you can use a command to display them. Once you have identified the name of your newly created template, you can use it to create a new application. For example, you might use a command that specifies the template name, such as “wildfly-oia-s2i,” to directly create the components of your new application.

Now that the application is deployed, let’s explore application clustering with WildFly on OpenShift. To demonstrate this, you will first add data to your session by registering users on the application page. Make a note of the pod name where your application is running. Next, scale the service to two replicated pods. The WildFly application will automatically replicate your session data in memory between these pods. Then, delete the original pod and verify that your session data is still active.

Querying the OpenShift server

Before getting started, you need to modify some permissions for the default service account in your project. This account is responsible for running the application pods. From the stateful app project, run a command to add the “view” role to the default service account. The “view” role allows the pods running in the project to query the OpenShift API server directly. In this scenario, the application will use this ability to find other WildFly application pods in the project. These instances will send traffic directly to each other and use their own service discovery and load balancing features for communication.

The command you run will add the “view” role to the default service account for your project, ensuring that the application pods can query the OpenShift API server.

Start by registering a few users in the WildFly application page. List the pods for the project and remember their names. You will see that there is one running pod for the application, while the rest are build and deploy pods that have already completed. The application is running in a pod with a name similar to “wildfly-app-1-vr2kz.”

Next, list the routes created for your app and describe one of the routes to find where it is exposed. The route description will show the request host, which in this case points to a URL like “http://wildfly-app-stateful-apps.apps-crc.testing.” Visit that URL and create a few accounts so you can observe how the data persists when you later scale the pods.

Now, scale up the deployment or deployment configuration. This action will cause the old pod to be deleted and two new ones to be created. After scaling, you will see two running pods: the original one and a new one with a different suffix, such as “47nwb.”

At this point, you can delete the original pod, the one ending with “vr2kz.” If your application is working correctly, the data from the original pod will have been replicated to the new pod, meaning no data will be lost when the first pod is deleted. After deleting the pod, the deployment configuration will ensure that a new pod is spun up to maintain the desired number of replicas, which is two. You will see a new pod with a different suffix, such as “k7g28,” replacing the one you deleted.

The two pods discover each other using a WildFly-specific discovery mechanism designed for Kubernetes, called KUBE_PING, which is part of the JGroups project. When the second pod starts, it queries the OpenShift API for all pods in the current project. The API server returns a list of pods, and the KUBE_PING code in WildFly filters this list for pods with special ports labeled “ping.” If any pods match the filter, the JGroups code in WildFly attempts to join any existing clusters among the pods in the list.

To see this from the pod’s perspective, you can navigate to any of the pods in the OpenShift console, open the terminal tab, and run a command to query the API server for a list of pods in the project matching the label “application equals wildfly-app.” Inside the pod, you can also inspect the environment variables to see the OpenShift configuration that WildFly uses to query the server. For example, you might see variables like “OPENSHIFT_KUBE_PING_NAMESPACE” set to “stateful-apps” and “OPENSHIFT_KUBE_PING_LABELS” set to “application equals wildfly-app.” These variables determine which namespace to query and which labels to filter on, ensuring that only the relevant application pods are considered.

Verify the data replication

Now that two pods are successfully clustered together, delete the original pod from the OpenShift console or by using the command line. The OpenShift replication controller will notice that a pod has been deleted and will spin up a new one to maintain two replicas. If clustering is working properly, the original data you entered will still be available, even though it was originally stored in memory in a pod that no longer exists. Double-check by refreshing the application in your browser. If the data is missing, make sure that the policy command to add the “view” role was run correctly. This command must be executed before making any other changes, and before scaling or deleting pods.

Other cases for direct pod access

A Java application server that needs to cluster applications is just one common use case for direct pod discovery and access. Another example is an application that has its own load balancing or routing mechanisms, such as a sharded database. A sharded database stores large data sets in many small databases, rather than one large database. Many sharded databases have intelligence built into their clients and drivers, allowing for direct access to the correct shard without needing to query where the data resides. Sharded databases work well with OpenShift and have been implemented using technologies like MongoDB and Infinispan.


A typical sharded database implementation may involve creating the service object as a headless service. Once this headless service object is in place, DNS can be used as an additional service discovery method. When you perform a DNS query for a headless service, it returns A records for all the pods that are part of that service. More details about DNS and A records will be discussed in the following sections.

Applications can then implement custom logic to decide which pod to access. One well-known example is Apache Kafka, a fast, open-source messaging broker. Most Kafka deployments on OpenShift and other Kubernetes-based platforms use headless services. This setup allows the messaging brokers to communicate directly with each other for sending and replicating messages. The brokers discover each other using DNS queries, which are made possible by the headless service configuration.

Other common scenarios for direct access include routine IT workloads, such as software agents used for backups and monitoring. Backup agents are frequently run alongside traditional database workloads and provide features like scheduled snapshots and point-in-time recovery of data. Monitoring agents, on the other hand, often offer real-time alerting and visualization for applications. These agents may run locally, embedded as instrumented code within the application, or they may communicate through direct network access. In many cases, direct network access is necessary because the agents need to interact with multiple applications across different servers. In such scenarios, agents require consistent, direct access to applications to perform their daily functions.

Describing sticky sessions

In the WildFly example, data is replicated between the WildFly server instances. The application automatically generates a cookie with a unique identifier and stores it in your browser. By using this cookie, the application can track which end user is accessing it. While this approach works well, it does have some drawbacks. The most obvious issue is that if the WildFly server does not support application clustering, or lacks a discovery mechanism compatible with OpenShift, the application may deliver an inconsistent user experience. For example, if there are two application pods—one with user data and one without—users might only see their data half the time, because requests are distributed in a round-robin fashion between the pods.

A common solution to this problem is to use sticky sessions. In the context of OpenShift, enabling sticky sessions ensures that a user making requests to the cluster will consistently receive responses from the same pod for the duration of their session. This added consistency helps provide a smooth user experience and allows many applications that store temporary data locally in the container to run effectively on OpenShift.

By default, OpenShift implements sticky sessions using cookies for HTTP-based routes and some types of HTTPS-based routes. The OpenShift router can either reuse existing cookies or create new ones. In the case of the WildFly application you created earlier, it generates its own cookie, so the router uses that cookie for sticky session management. If cookies are disabled or cannot be used for a particular route, sticky sessions are implemented using a load balancing scheme called "source," which relies on the client's IP address.

Toggling sticky sessions

Let’s explore how sticky sessions work by toggling cookies on and off using the Linux curl command-line tool. This tool can make HTTP requests to a server multiple times and print the results. The WildFly application you deployed includes a couple of REST endpoints that haven’t been discussed yet. One of these endpoints can print out the pod’s IP address and hostname.

The first example involves running a shell loop that uses curl to make eight sequential HTTP requests to the application’s route, specifically targeting the endpoint that returns the pod’s IP and hostname. The output shows the hostname and IP address of each pod, alternating between them in a round-robin pattern. This is the expected default behavior, since curl does not provide any cookie or identifier to influence OpenShift’s routing.

Next, curl can be configured to save cookies locally in a text file, which can then be used for future HTTP requests. This allows you to simulate a browser session where cookies are preserved. The process involves making an initial request to the WildFly application to save the cookie to a file called "cookies.txt." The contents of this file resemble a standard browser cookie, including fields like the cookie type, host, and unique identifiers.

After saving the cookie, you can run another shell loop that makes eight HTTP requests, this time sending the saved cookie with each request. The result is that all requests are routed to the same pod, demonstrating that sticky sessions are in effect and there is no longer a round-robin pattern.

Limitations of cookies

One limitation of using cookies for load balancing is that they do not work for HTTPS connections that use pass-through routing. In pass-through routing, the connection from the client—typically a browser—to the application pod is fully encrypted. In this scenario, cookies are ineffective because OpenShift cannot decrypt the connection, and the routing layer cannot see the request.

To address this, OpenShift uses the client’s IP address to implement sticky sessions. However, this approach has its own drawbacks. Many client IP addresses are translated using Network Address Translation, or NAT, before reaching their destination. When a request is translated using NAT, the client’s private IP address is replaced with a public IP address. This often results in all users on a particular home or business network appearing to have the same source IP address. For example, if you run three pods to serve an application to everyone in your office, but all requests are translated by NAT, everyone in the office might be routed to the same pod.

Additionally, OpenShift uses an internal hashing scheme based on the client IP address and the number of pods to determine its load balancing. When the number of replicas changes, such as during autoscaling, it is possible to lose sticky sessions.

For the remainder of this section, you will not need multiple instances of the WildFly application. Therefore, let’s scale it back down.

The next command scales the WildFly application down to two replicas. This ensures that only two pods are running for the application.

Shutting down applications


So far in this section, you have learned how to use sticky sessions to ensure that users have a consistent experience in OpenShift. You have also explored custom load balancing and service discovery within OpenShift services.

To demonstrate custom load balancing, you deployed an application that keeps user data in memory and replicates this data to other pods. When you looked at clustering, you entered data, then scaled up to two pods, which replicated the data you had entered. After that, you killed the original pod and verified that your data was still present. This approach worked well, but only in a controlled and limited capacity.

Now, imagine a scenario where autoscaling is enabled and pods are spinning up and down more quickly. How would you know that the application data had been replicated before a particular pod was killed—or even which pod was killed? OpenShift provides several ways to address this issue.

Application grace period

The easiest and most straightforward solution is to use a grace period, allowing the pod to shut down gracefully. Normally, when OpenShift deletes a pod, it sends the pod a Linux TERM signal, often called SIGTERM. This signal notifies the process that it needs to finish what it is doing, then exit and pass control to the parent process.

One important caveat is that the application needs custom code to catch the signal and handle the shutdown sequence. Fortunately, many application servers have this code built in. If the container does not exit within a given grace period, OpenShift sends a Linux KILL signal, or SIGKILL, which immediately terminates the application.

In this section, you will deploy a new application to demonstrate how the OpenShift grace period works. In the same stateful apps project you are already using, you run a command to build and deploy the application directly from GitHub using a resource template. This command creates a new application labeled as graceful, using the context directory for a Dockerfile that demonstrates graceful shutdown.

The application may take a minute to build, since it might need to pull down a new base image. Once the application is successfully built and running, you delete it with a grace period of ten seconds. This deletion command prompts the container runtime to wait ten seconds before forcefully killing the container process. This gives you a ten-second window to observe the logs produced by the pod.

When you run the delete command with a ten-second grace period, OpenShift sends a SIGTERM signal immediately to the pod, and then forcibly kills it in ten seconds if it has not exited by itself within that time. You can quickly run a command to view the logs for the pod during this window. The log command dynamically selects the pod ID, ensuring you do not lose time tracking it down, since you only have ten seconds to observe the logs.

In the logs, you see that the pod prints messages while it is running. Since this is an example application created to test SIGTERM, it waits for you to send that signal through the delete command. After the signal is caught, it runs the code in the signal hook or callback section. The logs show repeated messages indicating the application is waiting for SIGTERM and sleeping, followed by messages that it has caught SIGTERM and is gracefully shutting down, counting each second until it reaches the ten-second limit.

The process running in this example is a simple Bash script that waits for a SIGTERM signal, then prints a message to standard output until it is killed. In this case, the pod was given a grace period of ten seconds, and the logs reflect approximately ten seconds of graceful shutdown before the pod was forcibly killed.

By default, the grace period is set to thirty seconds. If you have an important container that you never want to kill or be killed, you must set the terminationGracePeriodSeconds field in the deployment to negative one.

It is important to remember that in a container, the main process runs as process ID one. This is crucial when handling Linux signals, because only process ID one receives the signal. Although most containers have a single process, some have multiple processes. In this scenario, the main process needs to catch the signal and notify the other processes in the container. Systemd can also be used as a seamless solution for this.

For containers with multiple processes that all need to handle Linux signals, it is best to use systemd for this implementation. You can now proceed and delete this example application demo by running a command that deletes all resources tagged with the label graceful.

Container lifecycle hooks

Although catching basic Linux signals such as SIGTERM is a best practice, many applications are not equipped to handle these signals. A useful way to externalize the logic from the application is to use the preStop hook, which is one of two container lifecycle hooks available in OpenShift.

Container lifecycle hooks allow users to take predetermined actions during a container management lifecycle event. The two events available in OpenShift are preStop and postStart. The preStop hook executes a handler before the container is terminated. This event is blocking, meaning it must finish before the pod is terminated. The postStart hook executes a handler immediately after the container is started.

Similar to readiness and liveness probes, the handler can be a command that executes in the container, or it can be an HTTP call to an endpoint exposed by the container. Container lifecycle hooks can be used together with pod grace periods. If preStop hooks are used, they take precedence over pod deletion. SIGTERM will not be sent to the container until the preStop hook finishes executing.

Container lifecycle hooks and Linux signal handling are often used together, but in many cases, users decide which method to use for their application. The main benefit of using Linux signal handling is that the application will always behave the same way, no matter where the image is run. It guarantees consistent and predictable shutdown behavior because the behavior is coded in the application itself.

Sending SIGTERM signals on delete is fundamental not only to Kubernetes and OpenShift platforms, but also to all runtimes like Docker and containerd. If the user handles the SIGTERM signal in their application, the image will behave consistently even if it is moved outside of OpenShift. Because preStop hooks need to be explicitly added to the deployment or template, there is no guarantee that the image will behave the same way in other environments.

Many applications, such as third-party applications, do not handle SIGTERM properly, and the end user cannot easily modify the code. In this case, a preStop hook must be used. A good example of this is the NGINX server, a popular and lightweight HTTP server. When NGINX is sent a SIGTERM, it exits immediately rather than forking. Instead of modifying the NGINX image itself and adding code to handle the Linux SIGTERM signal, an easy solution is to add a preStop hook that gracefully shuts down the NGINX server from the command line.

A general rule to follow is that if you control the code, you should code your application to handle SIGTERM. If you do not control the code, use a preStop hook if needed.

Stateful sets

So far in this chapter, you have learned that OpenShift has many capabilities to support stateful applications. These features let users make traditional workloads first-class citizens on OpenShift. However, some applications require even more predictable startup and shutdown sequencing, as well as predictable storage and networking identifying information.

Imagine a scenario with the WildFly application, in which data replication is critical to the user experience. If a massive scaling event destroys too many pods at one time while replication is happening, how will the application recover? Where will the data be replicated to? Is it going to be lost forever?


To solve this problem, OpenShift provides a special object called a stateful set. In older versions of OpenShift, this was known as a pet set. A stateful set is a powerful tool in the OpenShift user’s toolbox, designed to help run many traditional workloads in a modern, containerized environment.

A stateful set object is used instead of a replication controller or replica set, which are used in newer versions of the OpenShift platform. The stateful set is the underlying implementation that ensures replicas in a service are managed, but it does so in a more controlled and predictable way.

A replication controller cannot control the order in which pods are created or destroyed. For example, if a user configures a deployment to scale from one to five replicas in OpenShift, that task is handed off to a replica set or controller, which starts four new pods all at once. The order in which these pods are started and marked as ready is completely random and unpredictable.

Deterministic sequence

A stateful set introduces a deterministic, sequential order to pod creation and deletion. Each pod that is created has an ordinal index number associated with it, which indicates its startup order. For instance, if a WildFly application is using a stateful set with three replicas, the pods would be started and named in this order: wildfly-app dash zero, wildfly-app dash one, and wildfly-app dash two.

A stateful set also ensures that each pod is running and ready—meaning it has passed the readiness probe—before the next pod is started. This process is strictly sequential. In the previous scenario, wildfly-app dash two would not be started until wildfly-app dash one was running and ready.

The reverse is also true when scaling down. A replica set or controller will delete pods at random when reducing the number of replicas. In contrast, a stateful set controls the shutdown sequence, starting with the pod that has the highest ordinal index and working its way backward to meet the new replica requirements. A pod will not be shut down until the previous pod has been fully terminated.

Refer back to the previous section regarding SIGTERM and lifecycle hooks, as these will also affect this behavior.

The controlled shutdown sequence is critical for many stateful applications. For example, in the case of the WildFly application, user data is shared between several pods. When the WildFly application is shut down gracefully, a data synchronization process may occur between the remaining pods in the application cluster. This process can often be interrupted if pods are shut down in parallel, which is what happens without a stateful set. By using a predictable, one-at-a-time shutdown sequence, the application is less likely to lose any data, resulting in a better user experience.

Examining a stateful set

To see how stateful sets work, first create a new project. The following command creates a new namespace or project for testing purposes.

This command creates a new OpenShift project called statefulset.

Next, you can create the template from a provided file, which will later be used to create the actual application.

This command creates a template for a MongoDB stateful set replication using an empty directory for storage.

Now, create the application from the template you just created.

This command instantiates the MongoDB stateful set application from the template.

Notice how the two pods are correctly labeled with dash zero and dash one. These are the ordinal values of the pods, and they were created in that exact order by the stateful set controller.

This command lists the pods in the project, showing their names, readiness, status, restarts, and age.

Now, you can view the logs of the very first pod with index zero. In the logs, you can see how the pod is configured to look for other pods, such as the pod with index one, to replicate the data. In this case, a single user entry was added to the database, and that data was replicated to the other pod. The logs show how the mongodb dash zero pod tries to access the other pod through its deterministic host address, which is mongodb dash one dot mongodb dash internal dot statefulset dot service dot cluster dot local.

The log output shows that a user was successfully added, and the pod is connecting to the other pod for replication. The replica set configuration includes both members, and the state transitions for the secondary pod are shown.

Next, you can scale the stateful set to three replicas, which simply adds one more pod to the list. Notice that the pods are in the ready state, which means you can scale them up. If the mongodb dash one pod had been in a non-ready state, the scale command would have failed. This is because the stateful set will not allow scaling up when there are pods that are not ready.

This command scales the MongoDB stateful set to three replicas and then lists the pods to show their status.

Now, you can also look up the logs of the mongodb dash zero pod. You will see that the newest pod, which is mongodb dash two, is detected by the mongodb dash zero pod. The data is then replicated to the mongodb dash two pod, using the same process as before.

The log output shows that the mongodb dash zero pod is connecting to the new pod, updating the replica set configuration, and confirming the state transitions for the new secondary pod.

In summary, stateful sets in OpenShift provide a deterministic and controlled way to manage the lifecycle of pods, ensuring predictable startup and shutdown sequences. This is especially important for stateful applications like databases, where data consistency and graceful scaling are critical.


Unlike previous uses of the scale command, this time you need to explicitly state that you are scaling a stateful set. In the OpenShift console, notice that the new pod that was created has a deterministic pod name, with the ordinal index associated with it.

The reason we are using the stateful set object to scale, instead of a deployment, is because the stateful set is a replacement for the regular deployment object when working with stateful application contexts, such as this one. The StatefulSet has similar, but also different, properties in its specification compared to the Deployment. This reflects the features that the StatefulSet exposes to OpenShift users, especially those related to managing application state.

Similar to the WildFly application, the three MongoDB pods are replicating data to each other. To check that this replication is fully functional, click any of the pods at the bottom of the MongoDB StatefulSet Details page, and then click the Terminal tab. Any commands executed here will run inside the pod.

First, log into MongoDB as the admin user by running a command that connects to the admin database with the admin username and password. After logging in, check the status of the MongoDB replica set by running a command that displays the replica set status.

Constant Network Identity

Stateful sets also provide a consistent way of determining the host-naming scheme for each pod in the set. Each predictable hostname is also associated with a predictable DNS entry.

To examine the pod hostname for mongodb dash zero, you can run a command that iterates over each MongoDB pod and prints the contents of the hostname file from within each pod. This will show the actual hostname of each pod.

The output will list hostnames such as mongodb dash zero, mongodb dash one, and mongodb dash two. We know that the hostname is the same as the pod name, which is something we have seen and investigated earlier in previous sections.

The stateful set also ensures a DNS entry for each pod running in the set. This can be found by running a DNS lookup command using the DNS entry name for each pod. To find the IP addresses, you can run a command from one of the OpenShift nodes. Because this command relies on the OpenShift-provided DNS, it must be run from within the OpenShift environment to work properly.

When you are using stateful sets, the pod hostname in the DNS is listed in the format: pod name, dot, service name, dot, namespace, dot, service, dot, cluster, dot, local.

Because this example also contains a headless service, there are DNS A records for the pods associated with the headless service. To ensure that the pod IPs in DNS match the previous listing, you can run a command from the OpenShift nodes that uses a DNS lookup utility to query the DNS entries for each pod.

Consistent Persistent Storage

Pods running as part of a stateful set can also have their own persistent volume claims associated with each pod. But unlike a normal persistent volume claim, these remain associated with a pod and its ordinal index as long as the stateful set exists.

In the previous example, you deployed an ephemeral stateful set without persistent storage. Imagine that the previous example was using persistent storage, and the pods were writing log files that included the pod hostname. You would not want the persistent volume claim to later be mapped to the volume of a different pod with a different hostname, because it would be hard to make sense of those log files for debugging and auditing purposes.

Stateful sets solve this problem by providing a consistent mapping through the use of a volume claim template. This template ensures that the persistent volume is associated with each pod. If a pod dies or is rescheduled to a different node, then the persistent volume claim will be mapped only to the new pod that starts in its place, with the same hostname as the old pod.

Providing a separate and dedicated persistent volume claim for each pod in the stateful set is crucial for many different types of stateful applications, which cannot use the typical deployment configuration model of sharing the same persistent volume claims across many application instances.

The Stateful Set Limitations

Under normal circumstances, pods controlled by a stateful set should not need to be deleted manually. But there are a few scenarios in which a pod being controlled by a stateful set could be deleted by an outside force. For instance, if the kubelet or node is unresponsive, then the API server may remove the pod after a given amount of time and restart it somewhere else in the cluster. A pod could also exit accidentally or be manually removed by a user.

In those cases, it is likely that the ordinal index will be broken. New pods will be created with the same hostname and DNS entries as the old pods, but the IP addresses may be different. For this reason, any application that relies on hard-coded IP addresses is not a good fit for stateful sets. If the application cannot be modified to use DNS or hostnames instead of IP addresses, you should use a single service per pod for a stable IP address.

Another limitation is that all the pods in a stateful set are replicas of each other, which makes sense when you want to scale. But that would not help in any situation where disparate applications need to be started in a particular order. A classic example is a Java or .NET application that throws errors if a database is unavailable. Once the database is started, the application also needs to be restarted to refresh the connections. In that scenario, a stateful set would not help with the order between the two disparate services.

Non-Native Stateful Applications

One of the reasons OpenShift has gained so much market adoption is that traditional IT workloads work just as well as modern stateless applications. Yet, there is still work to be done. One of the biggest promises of using containers is that applications will behave the same way between environments. Containers start from well-known image binaries that contain the application and the configuration it needs to run. If a container dies, a new one is started from the previous image binary that is identical to how the previous container was started.

One major problem with this model occurs for applications that are changed on the fly and store their information in a way that makes it difficult to re-create. A good example of this issue can be seen with WordPress, an extremely popular blogging application that was designed many years before containers became popular.

In a typical WordPress workflow, a blogger might go to the admin portion of their website, add some text, and then save it. WordPress saves all that text in a database, along with any HTML and styling. When the blogger has completed this action, the container has drifted from its original image. Container drift is normal for most applications, but in this case, if the container crashed, the blog would be lost. Persistent storage can be used to ensure that the data is persisted. When a new WordPress pod starts, it could map to the database and would have all the blogs available. But promoting such a snapshot of a database among various environments is a major challenge.

There are many examples of using an event-driven workflow that can be triggered to export and import a database after a blogger publishes content, but it is not easy nor is it native to the platform. Containers start from well-known, immutable container images, but engineering a reverse workflow in which images are created from running container instances is more error-prone and rigid.

Other examples that have worked with some engineering include applications that open a large number of ports, applications that rely on hard-coded IP addresses, and other legacy applications that rely on older Linux technologies.

Cleanup Resource Limits and Quotas

This section introduces the topic of cleaning up resource limits and quotas, which is important for managing resources in a containerized environment.


After you finish this chapter, it’s important to either delete the resource quota and limit range objects you created, or edit them to be more lenient. If you want to scale up your replicas for the app-cli project, the current limits will prevent you from running more than two replicas. Be careful with this, as it can significantly restrict your ability to scale.

Operations and Security

This section covers cluster-wide concepts and the knowledge you’ll need to manage an OpenShift cluster at scale. These are essential skills for any operations team working with OpenShift.

In the previous chapter, you learned about OpenShift’s integrated role-based access control. You changed the authentication provider for your cluster, managed users, and worked with system accounts built into OpenShift, along with the default roles for different user types.

Other sections focus on the software-defined network that OpenShift deploys. This network is how containers communicate with each other and how service discovery works within an OpenShift cluster.

Finally, we’ll bring everything together and look at OpenShift from a security perspective. We’ll discuss how SELinux is used in OpenShift, and how you can work with security policies to provide the most effective level of access for your applications.

Permissions versus the Wild West

A platform like OpenShift isn’t effective for multiple users without robust access and permissions management for its various components. If every user had full access to all OpenShift resources, it would be chaotic—like the wild west. On the other hand, if it were too difficult to access resources, OpenShift wouldn’t be very useful.

OpenShift has a strong authentication and access control system. It strikes a balance between self-service workflows, which keep productivity high, and limiting users to only what they need to do their jobs.

When you first deployed OpenShift, the default configuration allowed any username and any non-empty password to log in. This method uses the “allow-all” identity provider that comes with OpenShift.

In OpenShift, the identity provider is a plugin that defines how users authenticate and which backend service manages user information. While the allow-all provider is fine for learning, you’ll need a more secure authentication method to enforce access rules.

In the next section, you’ll replace the allow-all provider with one that uses a local database file. Specifically, you’ll configure the OpenShift cluster to use an Apache htpasswd database for user access, and set up a few users to authenticate with that source.

You’ll create three users: developer, who has permissions typical for a developer in OpenShift; project-admin, who has permissions similar to a team lead; and admin, who has administrative control over the entire cluster.

Please complete that setup now, then continue with this section.

After configuring OpenShift in this way, if you try to log in with your original dev or other user, that user won’t be authenticated because it’s not in your htpasswd database. However, if you log in as the new developer or any of the new users, you’ll notice you no longer have access to the previous projects you created, such as image-uploader. That’s because the old dev user still owns those namespaces or projects.

Setting up Authentication

There are many different user databases available to IT professionals for managing access and authentication. To support as many as possible, OpenShift provides eleven identity providers that interface with various user databases, including the allow-all provider you’ve been using so far.

These providers include allow-all, which lets any username and non-empty password log in; deny-all, which blocks all logins; htpasswd, which authenticates with Apache htpasswd database files; keystone, which uses OpenStack Keystone as the authentication source; LDAP, which authenticates using an LDAP provider like OpenLDAP; Basic, which uses Apache Basic authentication on a remote server; Request Header, which uses a custom HTTP header for user authentication; GitHub and GitLab, which both use OAuth2 for authentication; and Google, which uses Google OpenID Connect.

Each authentication provider has its own options and unique format. For example, the options for the htpasswd provider are different from those required for the GitHub provider, because they access very different user databases.

What is htpasswd? It’s a utility from the early days of the Apache web server, dating back to the late nineteen nineties. Back then, computers had much less memory, and even the name of an application could affect system performance. Application names were often limited to eight characters, so names were shortened or abbreviated—hence, htpasswd.

Introduction to htpasswd

The htpasswd provider uses Apache-style htpasswd files for authentication. These are simple databases containing a list of usernames and their corresponding passwords in encrypted form. Each line in the file represents a user, with the username and password sections separated by a colon. The password section includes the algorithm used to encrypt the password, surrounded by dollar signs, and the encrypted password itself.

For example, an htpasswd file with two users, admin and developer, would have each user on a separate line, with their encrypted passwords.

You create htpasswd files using the htpasswd command-line tool. By default, this tool uses a custom encryption algorithm based on the MD5 hashing method.

Creating htpasswd Files

To create an htpasswd database file, you need to connect to your master server or cluster node. On the master server, the configuration files for the OpenShift master process are located in the directory slash etc slash origin slash master.

There, you’ll create an htpasswd file called openshift dot htpasswd with three users: developer, project-admin, and admin. This file will serve as the database for the htpasswd provider.

You’ll use the htpasswd command to add each user. The first time you run the command, include the dash c option to create a new htpasswd file.

First, make sure the htpasswd utility is installed. You can perform these operations on your host machine, since the file itself is what matters. The commands will create the file and add the users.

This sequence of commands creates the directory and file, then adds the users kubeadmin, admin, developer, and project-admin to the htpasswd database. The kubeadmin user is kept temporarily because it’s the only cluster-admin. Until you assign cluster-admin rights to another user, you shouldn’t remove kubeadmin, or you’ll lose the ability to perform cluster-level operations. Once you’ve added the new admin user and assigned the necessary roles, you can safely delete kubeadmin.

Changing the Provider

Before adding the new database with usernames and passwords, you can check how the CodeReady Containers, or CRC, does it by default. It also uses an htpasswd file database, and you can see this with a specific command.

This command extracts the contents of the htpass-secret object, which is the default secret containing the two users and their credentials. The secret is an opaque object that contains the htpasswd file encoded in base sixty-four. This means it’s not encrypted, and you can view the contents of the file.

To define a new secret, you use the contents of your htpasswd file and apply it to the cluster. You’ll use a new secret name to avoid deleting the old one, but the process follows the same configuration steps.


Let’s walk through the process of updating user authentication and role management in an OpenShift cluster.

First, you create a new secret in the openshift-config namespace. This is where all OpenShift configuration is stored. The secret is generated from a database file containing user credentials. The command you use will base sixty-four encode the file and create a secret named htpass-secret-custom, using the new file with updated users and passwords. After running this command, the secret is created and ready for use.

Next, you need to update the cluster’s OAuth configuration. By default, there is already a cluster-level OAuth provider. When you apply the new provider configuration, the fileData field in the OAuth object is updated to reference the new secret you just created, which is htpass-secret-custom. This change ensures that OpenShift uses your updated list of users and passwords for authentication.

To apply this change, you run a command that updates the cluster configuration with the new OAuth object. After applying the update, you should log out of any existing sessions and log back in using the new credentials. For example, the kubeadmin user’s password is now simply “admin.” At this point, all users are considered developers by default and do not have access to any projects. You will address permissions in the next steps.

Working with roles

Roles in OpenShift define what permissions users have within the cluster. Previously, you used a special admin account to configure physical volumes. The admin user is a privileged account. To manage roles, you use a command-line tool called oadm, which stands for OpenShift administration. This tool is installed by default on your cluster.

On your master node, the OpenShift deployment program sets up the root user. You can view the configuration for the root user by running a command as root on your master node. This command displays the user information and confirms that administrators with root access on the master node have cluster administration privileges by default. While this is convenient for small clusters, in larger environments, you may want to separate server root access from OpenShift administration. You can distribute the administrative certificate as needed for your cluster administrator workstations.

Assigning user roles

Now, let’s assign permissions to the users you created earlier. Developer users need permission to view and add content to the image-uploader project. First, ensure you are working in the context of the image-uploader project. You do this by switching to that project using the appropriate command.

Within the project namespace, you add the edit role to your developer user. This role allows the user to add and modify content. Assigning a role to a user for a project or the entire cluster is called binding a role to a user. To do this, you run a command on the cluster node or master server, not from your host machine. This command binds the edit role to the developer user.

To confirm the new role is applied, log in again as the developer user, either through the web interface or the command line. You should now have access to the image-uploader project and its deployed application.

Creating administrator users

So far, your OpenShift cluster has a single project. As your cluster grows, you may have dozens or even hundreds of projects. To manage this, you need users who can administer individual projects or the entire cluster.

To make a user the administrator for the image-uploader project, you bind the admin role to the project-admin user. This gives the project-admin user full administrative privileges within the image-uploader namespace. You do this by running a command as root on your master server, binding the admin role to the project-admin user.

Now, you have a developer user who can work in the image-uploader project and a project-admin who can administer that project.

Next, you need a user who can manage the entire OpenShift cluster. The cluster-admin role allows a user to administer all projects and manage internal OpenShift configuration and state. To create a cluster admin, you run a command as root on your master node. This command binds the cluster-admin role to the admin user you created earlier. Unlike the previous commands, this one grants privileges across all projects and namespaces in OpenShift.

Everything you have done so far helps you edit existing users and ensure they have the correct privileges. But what about new users? In the next section, you will configure OpenShift to bind the edit role to new users by default when they are created.

Setting default roles

OpenShift has three default groups, which are configured during installation. These groups determine whether a user is authenticated and can be targeted for additional actions, but the groups themselves cannot be modified.

The first group is system:authenticated. This includes any user who has successfully authenticated through the web interface, command line, or API.

The second group is system:authenticated:oauth. This includes users authenticated by the OpenShift internal OAuth two server, but excludes system accounts.

The third group is system:unauthenticated. This includes users who have failed authentication or have not attempted to authenticate.

In your cluster, it is helpful to allow any authenticated user to access the image-uploader project. You can do this by running a command that binds the edit role for the image-uploader project to the system:authenticated group. This means any user who successfully logs in will have direct access to this project.

Depending on your needs, you can use the other default groups in a similar way. The system:authenticated:oauth group excludes system accounts used for building and deploying applications. This group consists of all human users and external services accessing OpenShift. The system:unauthenticated group can be used to provide anonymous access, but is most commonly used to redirect users to the OpenShift login page.

To confirm that your new default user role is working, add a new user named user one to your htpasswd database file. After adding the user, log in with that account and verify that the new user can access the image-uploader project by default. This user should be able to work in the image-uploader project from their first login.


Any time you have a shared environment, you need processes in place to ensure that one user or project does not take up too many resources or privileges in your cluster—either accidentally or on purpose. Limit ranges and resource quotas are the mechanisms that manage this potential problem.

In OpenShift, these resource constraints can differ for each deployment, depending on whether explicit resource quotas are requested. For the applications we deployed earlier, we did not specify any processor or memory resources for either deployment. These best-effort deployments do not request specific resources and are assigned a best-effort quality of service. You can govern default values at the project level in OpenShift by using limit ranges.

In the next section, we will discuss limit ranges in more depth, and you will create your own and apply them to the image-uploader project.

Limit ranges

For each project in OpenShift, a limit range—defined as a LimitRange when working with the OpenShift API—provides resource constraints for most objects that exist in a project. These objects are the types of OpenShift components that users deploy to serve applications and data. Limit ranges apply to the maximum processing and memory resources, as well as the total object count for each component.

The limits for each component are as follows. For pods, you can set CPU and memory per pod, and the total number of pods per project. For containers, you can set CPU and memory per container, default memory and CPU, the maximum burstable ratio per container, and the total number of containers per project. For images, you can set the maximum image size for the internal registry, as well as the maximum number of image tag references and image references per image stream. For persistent volumes, you can set the minimum and maximum storage request size per persistent volume claim.

Before an application is deployed or scaled up, the project limit ranges are analyzed to confirm that the request is within the allowed range. If a project limit range does not allow the desired action, then it does not happen. For example, if a project limit range defines the memory per pod as being between fifty megabytes and one thousand megabytes, a request for a new application deployment with a defined quota of one thousand five hundred megabytes will fail because it is outside the pod memory limit range for that project.

Limit ranges have the additional benefit of being able to define default compute resource values for a project. When you deployed app-gui and app-cli, you had not yet defined a limit range for the image-uploader project and did not specify the resources for each deployment. As a result, each application pod was deployed with no resource constraints. In OpenShift, a deployment with no defined resources quota can lead to problems. If users start accessing the graphical user interface heavily, it can consume resources to the point that the performance of the app-cli deployment is affected. For a busy cluster with multiple users and running applications, that is a major problem. With limit ranges, you can define the default compute resources for a project that does not specify a quota, to prevent this from happening in your cluster.

Define resource limit ranges

Limit ranges define the minimum and maximum RAM and CPU an application can be allocated when it is deployed. In addition to the top and bottom of the range, you can specify default request values and limits. The difference between an application's requested value and its maximum value limit is called the burstable range.

Let us create a limit range for the image-uploader project using a template. You will create a file containing the limit range definition and apply it to the project.

The YAML file in this example defines a LimitRange object named core-resource-limits. It sets minimum and maximum CPU and memory values for both pods and containers. It also specifies default and default request values for containers, and sets a maximum limit-to-request ratio for CPU.

To define a limit range, a user needs to have the cluster admin role. To log in as the admin user, you can use a command that logs in with the kubeadmin user and password to the cluster host address. After logging in, you apply the resource limits for the project or namespace using a command that applies the limit range YAML file to the image-uploader project. You can then list the limit ranges in the project and optionally describe the object for more details.

After running the apply command, the new resource limit range is created for the project or namespace. In this case, we are using the image-uploader project, but other projects can be used instead by changing the value of the namespace argument in the command.

Throughout this section and the rest of the document, you will need to create YAML template files that are referenced in the OpenShift commands. Relative file name paths are used in the examples to keep them easy to read, but if you are not running the command from the directory where those files are created, be sure to reference the full path when you run the command.

You can use the command line to confirm that the image-uploader project limit range was created and to confirm that the settings you specified in the template were accurately read. As with every other resource in the OpenShift cluster, you can do this by using the get command for limit ranges. This can be combined with a describe command to list the details for each of these resource limits.

You can also use the web interface to confirm that the limit range you just set is in place. This can be found under Resources, then Quotas.

Limit ranges act on components in a project. They also provide default resource limits for deployments that do not provide any specific values themselves. However, they do not provide project-wide limits to specify maximum resource amounts. For that, you will need to define a resource quota for the image-uploader project.

In the next section, that is exactly what will be done.

Resource quotas

Nobody likes a noisy neighbor, and OpenShift users are no different. If one project's users were able to consume more than their fair share of the resources in an OpenShift cluster, all manner of resource availability issues would occur. For example, a resource-hungry development project could stop applications in a production-level project in the same cluster from scaling up when their traffic increases.

To solve this problem, OpenShift uses project quotas to provide resource caps and limits at the project level. Whereas limit ranges provide maximum resource limits for individual components, quotas provide limits for the entire project.

Quotas fall into three primary categories. The first is compute and storage resources, such as memory and processing power. The second is object counts, which include services, storage claims, config maps, secrets, and replication controllers. The third category is other resource types.

In one of the very first sections, we discussed pod life cycles. Project quotas apply only to pods that are not in a terminal phase. Quotas apply to any pod in a pending, running, or unknown state. Before an application deployment is started or a deployed application is changed, OpenShift evaluates the project quotas.

In the next section, you will create a compute resource quota for the image-uploader project.

Creating compute quotas

Compute resource quotas apply to CPU and memory allocation. They are related to limit ranges because they represent quotas against totals for requests and limits for all applications in a project.

You can set the following six values with compute resource quotas. The first is CPU, or request dot CPU, which is the total of all CPU requests in a project, typically measured in cores or millicores. CPU and requests dot CPU are synonyms and can be used interchangeably. The second is memory, or request dot memory, which is the total of all memory requests in a project, typically expressed in megabytes or gigabytes. Memory and requests dot memory are also synonyms and can be used interchangeably. The third and fourth are limits dot CPU and limits dot memory, which are the totals for all CPU and memory limits in a project.


In addition to setting quotas, you can also define the scope to which a quota applies. OpenShift supports four quota scopes.

The first scope is called Terminating. This applies to pods that have a defined life cycle, such as builder and deployment pods.

The second scope is NotTerminating. This covers pods without a defined life cycle. For example, application pods like app-gui and app-cli, as well as most other applications you deploy in OpenShift, fall under this category.

The third scope is BestEffort. This applies to pods that have a best-effort quality of service for processing and memory. These are deployments that did not specify a resource request or limit when they were created.

The fourth scope is NotBestEffort. This is the inverse of BestEffort. It applies to pods that do not have a best-effort quality of service for processing and memory. This scope is useful when you have a mix of low-priority, transient workloads deployed with best-effort quality of service, alongside higher-priority workloads that have dedicated resources.

To create a new quota for a project, you need cluster admin privileges. This means you must be logged in as the admin user to run the necessary commands. The developer user only has the edit role for the image-uploader project and does not have privileges for the rest of the cluster. To log in as the admin user, follow the steps from the previous section where you logged in to set range limits.

The next block of configuration defines a resource quota named compute-resources. It sets hard limits for the number of pods, CPU and memory requests, and CPU and memory limits. The scope is set to NotTerminating, meaning it applies to pods without a defined life cycle.

Save this template to a file. Then, apply the resource quota object using the appropriate commands. This ensures that the image-uploader project is now restricted by both limit ranges and resource quotas. As a result, you will never be able to deploy something that might consume too many resources in the image-uploader project due to the limit ranges. Additionally, during the active execution of your containers, they will never be able to use more runtime resources than allowed by the resource quotas.

The next set of commands logs you in as a user with the cluster-admin role, applies the resource quota from the template, and lists the resource quotas in the image-uploader project. You can also describe the object for more details. The output shows the current usage and limits for pods, CPU, and memory in the project.

Creating resource quotas

Resource quotas track all resources in a project that are deployed by Kubernetes. Core components in OpenShift, such as deployment configs and build configurations, are not covered by quotas. This is because these components are created on demand for each deployment and are managed by OpenShift controllers.

The components managed by resource quotas are the primary resources in an OpenShift cluster that consume storage and compute resources. Keeping track of a project's resources is important when you need to plan how to grow and manage your OpenShift cluster to accommodate your applications.

The following components are tracked by resource quotas: config maps, which provide a way to configure and define data for containers; persistent volume claims, which are application requests for persistent storage; resource quotas, which count the total number of quotas per project; replication controllers, which count the number of controllers in a project—typically equal to the number of deployed applications, though this can vary; secrets, which are a variation of config maps; services, which count the total number of services in a project; and image streams, which count the total number of image streams in a project.

Most of these items should be familiar, as they have been discussed in previous sections.

The next configuration block defines a resource quota named object-counts. It sets hard limits for the number of config maps, persistent volume claims, resource quotas, replication controllers, secrets, services, and image streams in the project.

Save this template to a file and apply the resource quota object using the appropriate command. This ensures that the image-uploader project is now properly restricted by OpenShift object or resource counts as well.

Working with quotas and limits

Now that the image-uploader project has both limit ranges and quotas, it is time to test them. The compute quota for the app is not being reflected yet, and your first task is to fix that.

Quotas for existing applications

When you deployed the apps in previous sections, no quotas or limits were defined for the image-uploader project. At that time, your cluster was essentially unregulated, and any deployed application could consume any amount of resources in the cluster. If an application is created and there are no limit ranges to reference, and no resources were requested—as was the case when you deployed the metrics pod—the Linux kernel components that define resource constraints for each container are created with unlimited values for resource limits. This is what happened when you deployed the app-cli and app-gui, which is why their CPU and memory quotas are not reflected in OpenShift.

Now that you have applied limit ranges and quotas to the image-uploader project, you need OpenShift to re-create the containers for these applications to include these constraints. The easiest way to do this is to delete the current pods for each application. When you run the delete command for the pods, OpenShift will automatically deploy new pods that contain the default limit ranges you defined earlier.

The next command deletes all pod resources related to the specified labels. Note that you are only deleting the pods, not the deployments or replication sets. The deployment and replication set will ensure that the pods are re-created with the correct limits.

Because you did not specify specific resource values, your new app-gui and app-cli pods inherit the default request values defined in the core-resource-limits limit range object. Each pod is assigned two hundred millicores of CPU and one hundred mebibytes of RAM. You can see in the previous output that the consumed CPU and memory quotas for the image-uploader project are twice the default request.

It is not a best practice to start using projects without having set limits and resources first, but for teaching purposes, you began using OpenShift without discussing proper configuration rules.

Changing quotas for deployed applications

When you deploy a new application, you can specify limits and quotas as part of its definition. You can also edit the YAML definition for an existing deployment configuration directly from the command line. To edit the resource limits for your deployment, run the edit command for the deployment. This opens your default system editor and allows you to edit the contents of the manifest interactively.

To edit the resource limits, find the section of the configuration that defines resource requests and limits for the containers. This section is currently empty because nothing was defined for the application when it was initially deployed. You will change that.

The next configuration block defines resource requests and limits for the pod. The pod is set as burstable because the maximum limits are higher than the requests. Specifically, the pod can burst up to one CPU, or one thousand millicores, while only requesting seven hundred fifty millicores. The memory limit is twice as much as requested, with a request of five hundred mebibytes and a limit of one thousand mebibytes.


Saving the new configuration will trigger a new deployment for the app-cli application. This new deployment will incorporate your updated resource requests and limits. Once the build completes, your deployment will be available with more guaranteed resources. You can verify this using the regular describe command, or by checking through the web UI console.

You can edit a deployment configuration to make more complex changes to deployed applications, but keep in mind that this is a manual process. For new application deployments, your project should use the default limit ranges whenever possible, so that it inherits default values.

Now, while your resource requests and limit ranges are still fresh in your mind, let’s dig a little deeper and discuss how these constraints are enforced in OpenShift. This enforcement is handled by the Linux kernel and the container runtime—such as Docker or containerd—using a feature called cgroups.

Cgroups for managing resources

Cgroups, or control groups, are components of the Linux kernel that provide per-process limits for CPU, memory, network bandwidth, and block storage bandwidth. In an OpenShift cluster, cgroups enforce the main limits and quotas configured for applications and projects.

Overview of the cgroups

Cgroups are organized in a hierarchy under the directory slash sys slash fs slash cgroup on the application node. Within this directory, there is a folder for each type of cgroup controller that is available. A controller represents a specific system resource that can be managed by cgroups. In this section, we are focusing on the CPU and memory cgroup controllers.

Inside the directories for the CPU and memory controllers, you will find a directory named kubepods dot slice. Cgroup slices are used to create subdivisions within the cgroup controller. These slices act as logical dividers and define resource limits for groups of resources below them in the cgroup hierarchy.

For example, if you log into the node and list the contents of the slash sys slash fs slash cgroup directory, you will see various files and directories, including the kubepods dot slice directory. This is the directory we care about for OpenShift resource enforcement.

If you navigate into the kubepods dot slice directory, you will find two important subdirectories: kubepods dash besteffort dot slice and kubepods dash burstable dot slice. These two slices are how resource limits for best-effort and burstable quality of service levels are enforced, as discussed earlier.

Because you defined resource requests for app-cli and app-gui, both will be defined in the kubepods dash burstable dot slice. Within both the besteffort and burstable slices, there are multiple additional slices. There is not an immediate identifier to tell you which slice contains the resource information for a given container, but you can get that information directly from the container runtime on your application node.

Identifying container cgroups

To determine which cgroup slice controls the resources for your deployment, you need to get the cgroup information from the container runtime. The cgroup slice that each container belongs to is listed in the output of the inspect command. You can filter for the cgroupsPath element to limit the output to only the cgroup slice information.

For example, when you inspect the app-cli containers, you will see that each replica has its own cgroupsPath value. The long identifier in the cgroupsPath shows that the container is in the burstable slice. This confirms that the app-cli deployment is being managed under the burstable quality of service level.

It’s important to note that slices do not define resource constraints for individual containers, but they can set default values for multiple containers. That’s why the hierarchy of slices can appear a bit excessive. To get to the resource constraints for the app-cli container, you need to go one more layer down in the hierarchy.

At the lowest level, there is a scope directory. Each scope is named after the full hash that a container’s short ID is based on. For example, if you navigate to the burstable slice subdirectory and list its contents, you will see directories for each pod, identified by their unique pod IDs.

Cgroups configurations are created on OpenShift application nodes using this process. It can be a bit complex, and because cgroups are listed according to the cgroup controller and not the process ID they manage, troubleshooting them can be challenging on a busy system. However, when you need to see the cgroup configuration for a single container, the process is more straightforward.

In the next section, we will look at how the cgroup information from the host is mounted in each container that is created.

Confirming cgroups limits

When a container runtime creates a container, it mounts the cgroup scope that applies to it inside the container, under the slash sys slash fs slash cgroup directory. It truncates the slices and scope so that the container appears to have only a single cgroup controller.

We are going to focus on the limits that enforce CPU and memory constraints for the app-cli container. To begin, let’s look at the CPU consumption limits.

To start an interactive shell prompt in your running container, you first need to select one of the containers that are currently active for the deployment. You can do this by listing the running containers and choosing the appropriate short ID. If you have multiple replicas, you can scale down your deployment to a single replica to avoid confusion.

Once you have the container ID, you can use the container runtime’s exec command to log into the container interactively using bin slash bash.

In summary, this process allows you to directly inspect and confirm the cgroup limits that are being enforced for your container, ensuring that your resource requests and limits are being applied as expected.


As we discussed earlier in this chapter, CPU resources in OpenShift are allocated in millicores. A millicore is one one-thousandth of the CPU resources available on the server. For example, if your application node has two processors, then a total of two thousand millicores is available for the containers on that node.

The ratio expressed here is what is represented in the CPU control group, or cgroup. The actual number is not expressed in the same units, but the ratios are always the same. For instance, the app-cli container has a request of seven hundred fifty millicores, with a limit of one thousand millicores, which is equivalent to one CPU.

To confirm that the limit for the app-cli container is correctly configured, you need two values from the file located at slash sys slash fs slash cgroup slash cpu dot max. This file contains one line with two values separated by a space. The first value is the time period in microseconds during which the cgroup quota for processor access is measured and reallocated. This period can be adjusted to create different processing quota ratios for different applications. The second value is the time in microseconds that the cgroup is allowed to access the processor during the defined period. This period is also adjustable.

For example, if the allowed value is one hundred, the cgroup will be allowed to access the processor for one hundred microseconds during the set period. If that period is also one hundred, then the cgroup has unlimited access to the processor on the system. If the period were set to one thousand, the process would have access to the processor for one hundred microseconds out of every one thousand.

For the app-cli container, this cgroup limits the container’s access to one CPU during one hundred thousand out of every one hundred thousand microseconds. If you convert these values to a ratio, app-cli is allocated a maximum of one thousand millicores of one CPU. That is the limit we have set for app-cli. This is how CPU time limits are managed for each container in an application deployment.

Next, let us look at how the request values are controlled by cgroups. The request limit for app-cli is managed by the value in slash sys slash fs slash cgroup slash cpu slash cpu dot weight. This value is a ratio of CPU resources relative to all the cores on the system.

The memory limit for the app-cli container is controlled by the value in slash sys slash fs slash cgroup slash memory slash memory dot current. There are other files, such as memory dot max, memory dot min, and memory dot peak, which are mostly self-explanatory. The most interesting one is memory dot max, which defines the maximum amount of allowed memory that the container can use. In this case, it matches perfectly with the limits configuration, meaning it has a value of one billion forty-eight million five hundred seventy-six thousand, which is as configured, a maximum amount of one gigabyte. The memory dot current value varies and depends on the application that is running. For app-cli, that value is at three hundred fifty megabytes, or thirty-four million six hundred thirty-five thousand seven hundred seventy-six.

Resource limits for OpenShift containers are enforced with kernel cgroups. The only exception is the memory request value. There is no cgroup to control the minimum amount of RAM available to a process. This value is primarily used to determine which node a pod is assigned to in your OpenShift cluster.

This section covered much of what is required to create and maintain a healthy OpenShift cluster. We have gone deep into the Linux kernel to confirm how container resource limits are enforced. Although limits, requests, and quotas are not the most exciting things to work through, they are absolutely critical and essential components of OpenShift, ready to handle production workloads effectively.

Your cluster is now connected with an authentication database, and the project you have been working on has effective resource limits and quotas. In the following sections, we will keep building on that momentum.

To summarize, this is how the values are distributed. Note that this table uses cgroup version one values as a reference. The formula for cgroup version two is a bit different. It is worth noting that during the last couple of years, container runtimes have moved to cgroups version two, which changes a few things, including the default values for different limits, such as cpu dot weight. In the older cgroups version one, this was called cpu dot shares. The shares value and weight have different scaling, and they are calculated very differently. Most container runtimes still use the old cgroups version one as a reference, and they adjust and scale the value to match the cgroups version two specification when creating containers. The reason for this is that the Open Container Initiative, or OCI, reference specification was written with cgroups version one in mind.

Here is a summary of how OpenShift values map to cgroup files. The CPU limit of one thousand millicores is managed by the cpu dot max file in both cgroup version one and version two. The CPU request of seven hundred fifty millicores is managed by cpu dot cfs underscore quota underscore us and cpu dot cfs underscore period underscore us in cgroup version one, and by cpu dot weight in cgroup version two. The memory request value does not have a corresponding cgroup file in either version. The memory limit, such as one thousand megabytes, is managed by memory dot max in both cgroup versions.

Networking

The importance of network design and configuration in an OpenShift cluster cannot be overstated. It is the fabric that binds your cluster together. With that perspective in mind, OpenShift does a lot of work to ensure its networking configuration is stable, performs well, and is highly configurable and available. These principles are what we will cover in this section.

Let us start with an overview of how the network stack in OpenShift is designed.

Managing the Software Defined Network

Open vSwitch, or OVS, is an enterprise-grade, scalable, high-performance software-defined network. In OpenShift, it is the default SDN used to create the pod network in your cluster. OVS is installed by default when you deploy OpenShift.

OVS runs as a service on each node in the cluster. You can check the status of the service by running a system control command on any node. First, you log in to the cluster node, and then query systemd for the status of the service on the node. The command shows that the service is running and enabled out of the box.

The service is automatically enabled on cluster nodes as part of the OpenShift deployment. The configuration file for the OVS service is located at slash etc slash sysconfig slash openvswitch, and each node’s local OVS database is located in the slash etc slash openvswitch directory.

For day-to-day operations, OVS should be transparent. Its configuration and updates are controlled by OpenShift. Using OVS provides several advantages to OpenShift. This transparent operation is possible because OpenShift uses the Kubernetes Container Network Interface, or CNI. The container network interface provides a plugin architecture to integrate different solutions to create and manage the pod network. OpenShift uses OVS as its default, but it can function with other network providers as well.

The OVS used in your OpenShift cluster is the communication backbone for all your deployed pods. Traffic in and out of every pod is affected by it in the OpenShift cluster. For that reason, you need to know how it works and how to effectively use it for your needs.

Let us start with the network configuration of your OpenShift application node.

Configuring the Application Node Network

When a node is added to an OpenShift cluster, several network interfaces are created in addition to the standard loopback interface and the eth zero physical interface. For our purposes, we will call eth zero the physical interface, even though you may be using a virtual machine for your cluster. That is because OpenShift creates the following additional virtual interfaces.

First, there is br zero, which is an OVS bridge. All OpenShift interfaces are associated with this bridge. OVS creates this interface when the node is added to the OpenShift cluster.

Next, there is tun zero, which is attached to br zero. This acts as the default gateway for each node. Traffic in and out of your OpenShift cluster is routed through this interface.

Finally, there is vxlan underscore sys underscore four seven eight nine, which is also attached to br zero. This virtual extensible local area network is encrypted and is used to route traffic to containers on other nodes in your cluster. It connects the nodes in your OpenShift cluster to create your pod network.


Additionally, each pod has a corresponding virtual Ethernet interface. This interface is linked to the eth0 interface inside the pod by the Linux kernel. Any network traffic sent to either interface in this relationship is automatically presented to the other. These relationships are fundamental to how pods communicate within the cluster.

Let’s clarify some key networking concepts in Linux and OpenShift.

Linux bridges, TUN interfaces, and VXLAN

A Linux bridge is a virtual interface used to connect other interfaces together. If two interfaces on a host are attached to a bridge, they can communicate with each other directly, without the need for explicit routing. This setup improves communication speed and keeps networking configuration simple, both on the host and inside containers.

A VXLAN, or Virtual Extensible LAN, is a protocol that creates an overlay network between the nodes in your OpenShift cluster. An overlay network is a software-defined network that operates on top of another network. In OpenShift, VXLANs are deployed on top of the host’s existing network configuration. To enable secure communication between pods, VXLAN encapsulates pod network traffic in an extra layer of network information. This encapsulation ensures that traffic is delivered to the correct pod on the correct server, using IP addresses. The overlay network, in this context, is the pod network within your OpenShift cluster. Each node’s VXLAN interface provides access to and from this network. For more details, you can refer to the official VXLAN specification in its RFC documentation.

You can view these interfaces on your application nodes by running the IP command. In practice, this command lists all network interfaces on the node, including loopback, bridges, virtual Ethernet pairs, and others. The output is often filtered for clarity, showing only the interface names and their indexes.

The networking configuration for the master node is essentially the same as for an application node. The master node uses the pod network to communicate with pods on application nodes as they are deployed, deliver their applications, and are eventually deleted.

Next, let’s look more closely at how the interface inside a container is linked to a corresponding virtual Ethernet, or veth, interface on the cluster node.

Linking containers to host interfaces

Previously, we discussed network namespaces and how each container has its own unique loopback and eth0 interface for network communication. From the perspective of an application inside a container, these two interfaces are the only visible networks. To get network traffic in and out of the container, the eth0 interface inside the container is linked by the Linux kernel to a corresponding veth interface in the host’s default network namespace. This ability to link two interfaces is a feature of the Linux kernel.

To determine which veth interface a container is linked to, you need to log into the application node where the container is running. The process involves a few steps, which we’ll illustrate using an example with a pod named app-cli.

Any virtual interface on a Linux system can be linked by the kernel to another virtual or physical interface. When two interfaces are linked, the kernel treats them as essentially the same interface. Any event on one interface is automatically reflected on its linked counterpart. The kernel maintains this relationship in a file called iflink, located at slash sys slash class slash net slash interface name slash iflink. This file contains the index number of the linked interface.

To find the linked interface number for the app-cli container, you can use the oc exec command to run a cat command inside the pod. This command outputs the contents of the iflink file for eth0, which is a single number representing the linked interface’s index on the host.

For example, if the output is one hundred fifteen, that means the eth0 interface in the app-cli pod is linked to interface number one hundred fifteen on the application node. But which veth interface is number one hundred fifteen? You can find this information by running the IP command on the node and looking for the interface with that index. The link ID, also called the ifindex, is the number at the beginning of each interface listed by the command. For each eth0 interface in a container, its iflink value matches the ifindex value of its corresponding veth interface.

By searching for the interface with the matching index, you can confirm the exact veth interface on the node that is linked to the pod’s eth0. This is how network traffic enters and exits containers in general.

Next, let’s confirm that this veth interface on the node is connected to the cluster’s pod network, allowing network traffic to flow in and out of the OpenShift cluster.

Working with Open vSwitch, or OVS

The command-line tool for working directly with Open vSwitch is called ovs-vsctl. To use this tool, you need to be logged into the host cluster node you want to inspect.

Earlier, we mentioned that all OpenShift SDN interfaces are attached to an OVS bridge named br-zero. We call it an OVS bridge because it is created and controlled by Open vSwitch itself. You can also create bridge interfaces with the Linux kernel, using the brctl command, but those are managed separately.

To confirm that the bridge interfaces are being managed by OVS, you can run the ovs-vsctl command to list active OVS bridges. This command will show you the names of the bridges, such as br-ex and br-int. From the names, you can deduce that one is internal and the other is external.

You can then list the interfaces attached to each bridge. For the internal bridge, br-int, the output will include the identities of the veth interfaces for each pod running on the cluster node. This includes not only your application pods, but also many operator and utility pods that OpenShift runs by default.

If you have not used Linux bridges before, it can be confusing to expect a bridge to appear when running brctl, only to find none listed. This is because the bridges are being managed by OVS, not by the kernel’s bridge utilities.

The node typically has two OVS bridges named br-ex and br-int. The output of the ovs-vsctl command lists all interfaces connected to the br-int bridge. This is how OpenShift SDN functions: when a new pod is deployed, a new veth interface is created and attached to br-int. At that point, the pod can send and receive network traffic on the pod network. It can also communicate outside the cluster through the br-int and br-ex bridges.

In the next section, we will put these bridge interfaces and the SDN to work by exploring how application traffic is routed and how applications communicate within your cluster. Let’s start at the beginning, with a request for the app-cli deployment.

Routing application requests

When you browse to the route of app-cli at the address “app dash cli dash image dash uploader dot apps dash crc dot testing,” your request first arrives at the node on port eighty, which is the default HTTP port.

To determine which service is listening on port eighty, you can log in to the cluster node and run the netstat command. This command will show that the process listening on port eighty is haproxy, which is responsible for handling incoming HTTP requests and routing them to the appropriate service or pod within the cluster.


There is an HAProxy service running on the node’s port, which is quite interesting. In OpenShift, HAProxy acts as the front door to your application. HAProxy itself is an open source, software-defined load balancer and proxy application. Within OpenShift, it takes the URL route associated with an application and proxies those requests into the correct pod, ensuring the requested data is returned to the user who made the request.

We won’t go into all the capabilities of HAProxy here. Instead, we’ll focus on how OpenShift uses HAProxy.

Using the HAProxy Service

The routed pod, which is responsible for handling incoming requests, runs in the project named “openshift-ingress” in OpenShift. This router pod manages incoming user requests for your OpenShift cluster applications and proxies them to the appropriate pod so the user can be served.

The router pod listens directly on the host interface of the node where it’s deployed. It uses the pod network to proxy requests for different applications to the correct pod. Once the session is complete, the response returns to the user from the pod host through the TUN interface.

Here’s how the request flow works:

First, a user requests information for the app-cli application by using the route’s URL, which connects to the OpenShift cluster node.

Next, the HAProxy pod receives the request URL and maps it to its corresponding pod.

HAProxy then uses the pod network to proxy the connection to a node where an app-cli pod is deployed.

The request travels through the pod network and is delivered to the app-cli pod.

The TUN interface, which is attached to the bridge, routes traffic to the host network interface.

Finally, the app-cli processes the request and sends the response back to the user through its host TUN interface.

Because the router listens directly on the host interface, it is configured differently than a typical pod in OpenShift.

In the next section, we’ll investigate HAProxy in more detail.

How Does HAProxy Always Deploy to the Same Node?

In OpenShift, it’s possible to tag an entire cluster application node with a label. When deploying specific pods, you can instruct them to be deployed on a particular cluster node based on this label. This is similar to how namespaces are used to deploy applications in different project namespaces.

This is accomplished using the “nodeSelector” value in the manifest file, specifically in the deployment configuration component. The default OpenShift router uses a node selector that specifies the node with the matching “region equals infra” label. You can see this node selector in the router deployment configuration.

For example, the deployment configuration for the router might specify a node selector that requires the region to be “infra,” the operating system to be Linux, and the node to have the master role.

Investigating the HAProxy Service

The “lsns” tool, which you may have used in previous sections, displays the namespaces associated with the HAProxy process that listens on port 80. To use this tool, you first need to find the process ID, or PID, of your app pod.

To do this, you can use a combination of commands to inspect the router-default pod and extract its PID. Once you have the PID, you can use it to list the namespaces bound to this process.

For example, after extracting the PID, you might see that the PID is 7702. Listing the namespaces for this process reveals something interesting: the time, user, and network namespaces are owned by PID 1, which is always the very first process started on the host. This is not the router pod’s PID, which is 7702.

This means that the router pod is using the host’s network namespace, allowing HAProxy to listen directly on the host interfaces for incoming requests. By listening on the host interface, HAProxy can receive application requests directly, acting as the front door for application traffic in OpenShift.

The router pod also has its own mount namespace, which means that configuration files for HAProxy are isolated within the container.

To enter the router pod, you can use the “oc rsh” command, substituting the name of your router pod. This command initializes a remote shell session into the pod.

Once inside the pod, you can run the “ip a” command to list network interfaces. You’ll notice that the first interface, which you may have already seen on the host for the app-cli pod, is also visible from the router pod. In fact, all host interfaces are visible from the router pod. This is because the “ip” command is executed in the namespace of the host, not the pod, allowing access to interfaces on the host.

HAProxy and Request Routing

The configuration file for HAProxy is located in the pod at “slash var slash lib slash haproxy slash conf slash haproxy dot config.” This configuration file is maintained by the OpenShift cluster and its operators or controllers. Whenever an application is deployed, updated, or deleted, OpenShift updates this configuration file and instructs the HAProxy process to reload it.

To see this in action, you can display the contents of the configuration file and search for entries related to the app-cli application. You’ll see configuration entries for both pods currently running for this app. Each backend entry lists the server details for the app-cli pods, including their IP addresses and ports, as well as health check settings.

If you scale down the deployment for app-cli to a single replica, the configuration will update accordingly to reflect only the remaining pod.

In summary, HAProxy in OpenShift acts as a dynamic, host-level load balancer and proxy, routing incoming application requests to the correct pods based on up-to-date configuration managed by the OpenShift platform. This setup ensures efficient and reliable delivery of application traffic within the cluster.


From the router pod, if you execute the command again, you will notice that only one entry is now present. This matches the single replica configured for this project after scaling was performed earlier.

The output shows details such as the IP addresses of the pods, as well as their names and IDs. HAProxy takes incoming user requests, maps the requested URL to a defined route in the cluster, and then proxies the request to the IP address of a pod in the service associated with that route. All of this traffic moves through the pod network created by OpenShift SDN.

This process works together with iptables on each host. OpenShift uses a dynamic and complex iptables configuration to ensure that requests on the pod network are routed to the correct application pod. While iptables are a complex topic and not covered in detail here, it is important to know that OpenShift’s routing method is effective.

However, this approach can create challenges when deploying applications that depend on each other. If a new pod is added or a pod is replaced and receives a new IP address, all applications that reference it would need to be updated and redeployed. This is not a practical solution.

Fortunately, OpenShift includes a DNS service on the pod network. Let’s take a closer look at how this works.

Locating services with internal DNS

Applications often depend on each other to deliver information to users. For example, middleware applications depend on databases, and web presentation layers depend on middleware. In an application that spans multiple, independently scalable pods, these relationships can be complex to manage.

To simplify this, OpenShift deploys SkyDNS when the cluster is set up. SkyDNS is a DNS service that uses etcd, the primary Kubernetes database, to store DNS records. These records, also known as zone files, are configuration files where DNS information is recorded for a domain controller by a DNS server.

In OpenShift, SkyDNS manages the zone files for several domains that exist only on the pod network. The top-level domain for everything in your OpenShift cluster is cluster dot local. All services running in your cluster are under svc dot cluster dot local. Domains for each project are also created. For example, image-uploader dot svc dot cluster dot local is used to access all the services created in the image-uploader project.

A DNS A record is created in SkyDNS for each service in OpenShift. When an application is deployed, a service represents all the deployed pods for that application.

To view the services for the image-uploader project, you can run an oc command. This command lists the services in the image-uploader namespace, showing details such as the service name, type, cluster IP, external IP, ports, and age.

What is more interesting is the route, which was created earlier by the expose command and is directly linked to the service. The router object is configured to serve a specific service on a given path and port.

In the route manifest, you can see that the route is linked to the service. The manifest also includes other important parts of the specification, such as the target port and the host, which points to a combination of the app name, project name, and cluster host.

The relationship established between the application and external traffic is as follows: incoming traffic enters through ingress, then passes to the route, then to the service, then to the pod, then to the container, and finally reaches the application.

DNS resolution in the pod network

When a pod is deployed, the etc slash resolv dot conf file from the application node is mounted in the container at the same location. In Linux, etc slash resolv dot conf configures the servers used for DNS service name resolution. By default, this file on the application node is set up with the IP address for the node itself.

DNS requests on each application node are forwarded to SkyDNS, which runs on the master server node. The search parameter in etc slash resolv dot conf is also updated when it is mounted in the container. It is updated to include cluster dot local, svc dot cluster dot local, and all other domains managed by the SkyDNS service.

Any domain defined in the search parameter in resolv dot conf is used when a fully qualified domain name, or FQDN, is not used for a hostname. An FQDN is a complete address on a network, such as server dot domain dot com, while just server is not a complete domain name. The search parameter provides one or more domains that are automatically appended to non-FQDN hostnames for DNS queries.

When a request comes in from your cluster, those requests are automatically forwarded to the master server, where SkyDNS handles them.

To test this in action, you can use the format service name dot project name dot svc dash cluster dot local, followed by the port. For example, you can use the oc rsh command to enter the namespace for the app-cli pod and use curl to download the index page from app-cli and the default page for the router service. This demonstrates how DNS resolution works within the pod network, allowing you to access services by name rather than by IP address.

Configure OpenShift SDN

When you deploy OpenShift, the default configuration for the pod network topology is a single flat network. This means every pod in every project can communicate without restrictions.

OpenShift SDN uses a plugin architecture that provides different network topologies. There are currently three OpenShift plugins that can be enabled in the OpenShift configuration without making major changes to your cluster.

The ovs-subnet plugin is enabled by default. It creates a flat pod network, allowing all pods in all projects to communicate with each other.

The ovs-multitenant plugin separates pods by project. Applications deployed in a project can only communicate with pods in the same project. You will enable this plugin later in this section.

The OVS-networkpolicy plugin provides fine-grained ingress and egress rules for applications. This plugin offers a lot of configuration power, but the rules can be complex. This plugin is out of scope for this discussion.

The Kubernetes container network interface, or CNI, accepts different networking plugins. OpenShift SDN is the default CNI plugin in OpenShift. It configures and manages the pod network for your cluster.

Let’s review the available OpenShift SDN plugins.

Using the ovs-subnet

Earlier, you were able to communicate directly with an application from the stateful-apps project from a pod in the image-uploader project. This was possible because of how the ovs-subnet plugin is configured in the pod network. A flat network topology for all pods in all projects allows communication between any deployed applications.

With this setup, an OpenShift cluster is deployed like a single tenant, with all resources available to one another. If you need to separate network traffic for multiple tenants, you can use the multitenant plugin.

Using the ovs-multitenant

The ovs-multitenant network plugin isolates pod communications at the project level. Each pod for each application deployment can communicate only with pods and services in the same project on the pod network.

For example, the app-gui and app-cli pods can communicate directly because they are both in the image-uploader project namespace. However, they are isolated from the wildfly-app application in the stateful-apps project in your cluster.

This isolation relies on two primary tools in Open vSwitch.

First, VXLAN, which stands for Virtual Extensible LAN, acts similarly to a VLAN in a traditional network. It is a unique identifier that can be associated with an interface and used to isolate communication to interfaces with the same virtual network ID.

Second, OpenFlow is a communication protocol that can be used to map network traffic across a network infrastructure. In OpenShift, OpenFlow helps define which interfaces can communicate and when to route traffic through the vxlan zero and tun zero interfaces on each node.


When the OVS multitenant plugin is enabled, each project in OpenShift is assigned a unique VNID, or Virtual Network Identifier. The VNID for each project is stored in the etcd database on the OpenShift master node. When a pod is created, its associated virtual Ethernet, or veth, interface is linked to the project's VNID. OpenFlow rules are then created to ensure that the pod can only communicate with other pods in the same project.

The router and registry pods, which are part of the default project, are assigned VNID zero. This is a special VNID that allows these pods to communicate with all other VNIDs on the system.

If a pod needs to communicate with another pod on a different host, the VNID is attached to each packet. With the OVS multitenant plugin enabled, if a pod needs to communicate with a pod in another project, the request must be routed off the pod network and connect to the desired application through its external route, just like any other external request. This is not always the most efficient architecture.

The OpenShift SDN OVS network policy plugin provides more fine-grained control over how applications communicate across projects.

Creating advanced network designs

The OVS network policy plugin allows for detailed access control for individual applications, regardless of which project they belong to. These rules can become complex very quickly. While we do not have the space to cover all the details here, you can learn more about them on the official OpenShift documentation site.

Enabling the multi-tenant plugin

To enable the multi-tenant plugin, you need to SSH into your master node and application nodes, if you have more than one node in your cluster, and edit the cluster network configuration.

First, on the master server, log in with a user that has cluster-admin roles. You can use the OpenShift command-line tool to describe the current network cluster object. This command shows details about the deployed network configuration, including the network type, CIDR ranges, and other relevant settings.

Next, you need to apply specific network policies that are prerequisites for setting up the multitenant plugin. This is done by applying a YAML configuration file that defines the necessary network policies. For example, applying the network-policy-multitenant YAML file creates policies that allow traffic from the OpenShift ingress and monitoring namespaces, as well as allowing traffic within the same namespace.

After applying these configurations, you can use the command-line tool to describe the network policies and verify that they are in effect. The output will show details such as the policy name, namespace, creation date, and the specific rules for allowing ingress traffic.

Once these changes are committed, you must restart the master node server for the changes to take effect. This can be done using the command-line tool on your host machine to stop and then start the cluster. After the server restarts, the new network policies and multitenant plugin configuration will be active.

Testing the multi-tenant plugin

Previously, you logged in to the app-cli pod using the OpenShift remote shell and downloaded the web index pages for other applications. Now, you will repeat this process. Connect to the pods again and attempt to download the index pages for both applications, one from the current project, such as the image-uploader project, and the other from a different project, such as the wildfly-app project. The specific projects do not matter, as long as they are different.

Security

Each topic in this chapter focuses on security and making OpenShift a secure platform for your applications. This chapter is not a comprehensive summary of all OpenShift security features, as that would require a much larger document. Instead, we will walk through the fundamentals of OpenShift security, providing examples of the most crucial concepts and pointing you toward additional resources for topics we cannot cover in detail.

We begin by discussing important security concepts and how to make OpenShift secure, starting from the very first section. Key areas include understanding OpenShift's role in your environment, deploying applications with specific users, exploring how container processes are isolated, confirming application health and status, autoscaling applications for resilience, implementing continuous integration and continuous deployment pipelines, working with persistent storage, controlling access to pods and managing interactions between pods, using identity providers, managing roles, limits, and quotas, and creating secure, stable networking.

We use a broad definition of security here, but every section in this documentation contributes to your understanding of OpenShift and how to deploy it in an automated and secure fashion. Automation and security go hand in hand, because humans are not well-suited for repetitive tasks. The more you can automate tasks for your application, the more secure those applications can become.

Even though we have already covered a lot of ground regarding security, we still need to devote this entire section to security-specific topics. OpenShift has layers of security, from the Linux kernel on each application node through the routing layer that delivers applications to end users. We will begin this discussion with the Linux kernel and work our way up through the application stack.

For containers and OpenShift, security begins in the Linux kernel with SELinux.

SELinux core concepts

SELinux, or Security-Enhanced Linux, is a Linux kernel module used to enforce mandatory access control, also known as MAC. This is a set of access levels assigned to users by the system, and only users with root-level privileges can alter them. For typical users, including the automated user accounts in OpenShift that deploy applications, the SELinux configuration specified for a deployment is an immutable fact.

Mandatory access control is in contrast to discretionary access control, or DAC, in Linux. DAC is the system of users and file ownership access modes that we use every day on Linux hosts. If only discretionary access control were in effect in your OpenShift cluster, users could allow full access to their container resources by changing the ownership or access mode for the container process or storage resources.

One of the key security features of OpenShift is that SELinux automatically enforces mandatory access control policies that cannot be changed by unprivileged users for pods and other resources, even if they deployed the application.

We need to take a moment to discuss some fundamental information that will be used throughout this section. As with security in general, this will not be a full introduction to SELinux. Entire books have been written on that topic, including the SELinux coloring book. However, the following information will help you understand how OpenShift uses SELinux to create a secure platform.

We will focus on the following SELinux concepts.


Labels, Contexts, and Policies in SELinux

SELinux labels are applied to every object on a Linux server. These labels are essential for controlling access and interactions between different objects. The context in SELinux refers to how these labels are assigned, typically based on the file system location of the object. Policies, on the other hand, are the rules that govern how objects with different SELinux labels can interact with each other.

Working with SELinux Labels

Whenever a new object is created on your OpenShift servers, SELinux automatically applies a label to it. This label determines how the object interacts with the SELinux kernel module. In this context, an object can be anything a user or process creates or interacts with on a server. This includes files, directories, TCP ports, Unix sockets, and shared memory resources.

Each SELinux label consists of four sections, separated by colons. The first section is the user, which specifies which SELinux user has access to the object. The second section is the role, which defines the SELinux role that can access objects with the matching label. The third section is the type, which is where most SELinux rules are written. The fourth section is the multi-category security, often called the MCS bit, which is unique for each container.

For example, Open vSwitch is used for communication on OpenShift cluster nodes. The socket file for Open vSwitch is located at slash var slash run slash open-vswitch slash db dot sock. To view the SELinux label for this socket, you can use the ls command with the dash Z option. This command will display not only the regular file permissions, group, and user owner, but also the SELinux labels for the socket object.

In the output, you might see a label like system underscore u colon object underscore r colon openvswitch underscore var underscore run underscore t colon s zero. Here, system underscore u is the SELinux user, which is used for the MCS implementation. Object underscore r is the SELinux role, also used primarily for MCS. Openvswitch underscore var underscore run underscore t is the SELinux type, which is used in type enforcement policies to define interactions between objects on a Linux host. Finally, s zero is the MCS value, which distinguishes between different category levels on the system.

In addition to the standard POSIX attributes like mode, owner, and group ownership, the output also includes the SELinux label for the Open vSwitch socket.

Next, let's look at how SELinux labels are applied to files and other objects when they are created. Most command-line tools, such as ls, ps, and netstat, accept the dash Z option to include SELinux information in their output. Because objects are presented in the Linux operating system as files, their SELinux labels are stored in their filesystem extended attributes. You can view these attributes directly for the Open vSwitch socket using the getfattr command. This command will show the security dot selinux attribute, which contains the full SELinux label.

If you are looking for comprehensive SELinux documentation, the Red Hat Enterprise Linux 7 SELinux guide is a great place to start. You can find it on the official Red Hat website.

Applying Labels with SELinux Context

Labels are applied to files using SELinux context rules. These rules use regular expressions to apply labels depending on where the object exists in the file system. One of the most common issues system administrators face is when a developer claims that SELinux is breaking their application. In reality, the application is likely creating objects on the server that do not have a defined SELinux context. If SELinux does not know how to apply the correct label, it cannot properly handle the application objects. This often results in SELinux policy denials, leading to requests to disable SELinux.

To query the context for a system, you can use the semanage command and filter the output with grep. The semanage command allows you to search for contexts that apply to any label related to any file or directory, including the Open vSwitch socket. For example, searching for openvswitch in the semanage output shows that the context system underscore u colon object underscore r colon openvswitch underscore var underscore run underscore t colon s zero is applied to any object created in the slash var slash run slash openvswitch directory.

Properly applied SELinux labels create policies that control how objects with different labels can interact with each other. Let's discuss those policies next.

Enforcing SELinux with Policies

SELinux policies are complex, highly optimized, and compiled so they can be interpreted quickly by the Linux kernel. While creating or examining the code for a policy is outside the scope here, let's look at a basic example using the Apache web server.

If you have installed Apache, the httpd binary is located in slash sbin slash httpd. If it is not installed, you can use the d n f command to install it on your master cluster node. To inspect the SELinux policies on the executable, use the ls command with the dash z and dash l options. This will show that the httpd executable has an SELinux label of system underscore u colon object underscore r colon httpd underscore exec underscore t colon s zero.

On CentOS and Red Hat systems, the default Apache web content directory is slash var slash www slash html. This directory has an SELinux label of system underscore u colon object underscore r colon httpd underscore sys underscore content underscore t colon s zero. The default cgi-script directory for Apache is slash var slash www slash cgi-bin, and it has the SELinux label of system underscore u colon object underscore r colon httpd underscore sys underscore script underscore exec underscore t colon s zero. There is also the httpd underscore port underscore t label for certain TCP port numbers, including eighty, eight thousand eight, eight thousand nine, eight thousand four hundred thirty-three, nine thousand, eighty-one, four hundred forty-three, and four hundred eighty-eight.

An SELinux policy enforces the following rules using these labels for the httpd underscore exec underscore t object type. The httpd underscore exec underscore t type can write only to objects with an httpd underscore sys underscore content underscore t type. It can execute scripts only with the httpd underscore sys underscore script underscore exec underscore t type. It can read from directories with the httpd underscore sys underscore script underscore exec underscore t type. Finally, it can open and bind only to ports with the httpd underscore port underscore t type.

In summary, SELinux labels, contexts, and policies work together to control access and interactions between objects on a Linux system. Properly applied labels and well-defined policies are essential for maintaining security and functionality, especially in complex environments like OpenShift.


This means that even if the Apache web server is compromised by a remote user, it can only read content from the directory var slash w w w slash html, and run scripts from var slash w w w slash cgi dash bin. It cannot write to var slash w w w slash cgi dash win. All of these restrictions are enforced by the Linux kernel, regardless of who owns the binary, the permissions on these directories, or which user owns the httpd process.

The default SELinux policy loaded on a Linux system is called the targeted type. The rules in this targeted SELinux type are only applied to objects that have a matching context. Every object on a server is assigned a label based on the SELinux context it matches. If an object does not match any context, it is assigned the unconfined underscore t type in its SELinux labels. The unconfined underscore t type has no contexts or policies associated with it. As a result, interactions between objects that are not covered by a policy in targeted SELinux are allowed to run without interference.

To summarize, the httpd executable can only bind to specific ports that are labeled with the httpd underscore port underscore t type. The binary can only execute scripts with the httpd underscore sys underscore script underscore exec underscore t type. It can only read from directories with the httpd underscore sys underscore script underscore exec underscore t type, and it can serve content from and write to directories tagged with the httpd underscore sys underscore content underscore t type.

For CentOS and Red Hat Enterprise Linux, the default policies use type enforcement. Type enforcement uses the type value from SELinux labels to control how objects interact with each other.

Let us review what we have discussed so far.

SELinux is used to enforce mandatory access control, or MAC, in your OpenShift cluster. MAC provides access controls at a deeper level than traditional user or group ownership and access modes. It also applies to objects on the operating system that are not just files and directories.

Every object on an OpenShift node is assigned an SELinux label, including a type. Labels are assigned according to an SELinux context as objects are created. With these labels applied, SELinux policies enforce how objects can interact with each other. SELinux uses type enforcement policies on your OpenShift cluster to ensure proper interactions between objects.

This SELinux configuration is standard for any CentOS or Red Hat system running with SELinux in enforcing mode. Just as with the Apache web server process we have been discussing, a container is essentially a process. Each container process is assigned an SELinux label when it is created, and that label dictates the policies that affect the container.

To confirm the SELinux label that is used for containers in OpenShift, you first get the container process ID from the container runtime, and then use the process status command with the dash Z option, searching for that process ID. This will show you the SELinux context for the container process.

For example, you might use a series of commands to find the process ID for a container named router dash default, and then inspect its SELinux label. The output would show a label like system underscore u colon system underscore r colon s p c underscore t colon s zero, followed by the process ID and the command line for the process. This confirms the SELinux context assigned to the container.

OpenShift hosts operate with SELinux in enforcing mode. Enforcing mode means that the policy engine controlling how objects can interact is fully activated. If an object attempts to do something that is against the SELinux policies present on the system, that action is not allowed, and the attempt is logged by the kernel.

To confirm that SELinux is in enforcing mode, you can run the getenforce command. The output will simply say "Enforcing," which confirms that SELinux is active and enforcing policies on the master node.

In other servers, tools like virus scanners can cause issues with SELinux. A virus scanner is designed to analyze files on a server that are created and managed by other services. This makes writing an effective SELinux policy for a virus scanner a significant challenge. Another common issue is when applications and their data are placed in locations on the file system that do not match their corresponding SELinux context. For example, if the Apache web server tries to access content from the slash data directory, it will be denied by SELinux because slash data does not match any SELinux context associated with Apache. These sorts of issues sometimes lead people to disable SELinux.

The user and role portions of the SELinux label are not used for type enforcement policies. The svirt underscore lxc underscore net underscore t type is used in SELinux policies that control which resources on the system a container can interact with.

We have not yet discussed the fourth part of the SELinux label, which is the MCS level. This level isolates pods in OpenShift. Let us examine how that works next.

Isolating pods with levels

The original purpose of the MCS bit was to implement the MCS security standards on Linux servers. These standards control data access for different security levels on the same server. For example, secret and top secret data could exist on the same server. A top secret level process should be able to access secret level data, a concept called data dominance. However, secret processes should never be able to access top secret level data, because that data has a higher MCS level.

This is the security feature you can use to prevent a pod from accessing data it is not authorized to access on the host. OpenShift uses the MCS level for each container process to enforce security as part of the pod security context. A pod security context is all the information that describes how it is secured on its application node.

Let us look at the security context for the app dash c l i pod.

Investigating pod security context

Each pod security context contains information about its security posture. You can find full documentation on the possible fields that can be defined in the official OpenShift documentation.

In OpenShift, the following parameters are configured by default. Capabilities define an application's ability to perform various tasks on the host. Capabilities can be added to or dropped from each pod. We will look at these in depth in this section. Privileged specifies whether the container is running with any of the host namespaces. RunAsUser is the user ID with which to run the container process. This can be configured, and is often used in Dockerfile images. SELinuxOptions are the SELinux options for the pod. Normally, the only needed option is to set the SELinux level.

You can view the security context for a pod in the graphical user interface by choosing Pods, then selecting the pod you want information about, and then choosing Actions, Edit, and YAML. From the command line, you can use a command to get the YAML manifest for a pod. The output will always include the securityContext field, which contains the seLinuxOptions levels.

For example, you might run a command to get the YAML for a pod named app dash c l i dash seven nine seven six b four c eight eight eight dash r d b v seven in the image dash uploader namespace. In the output, you would see a securityContext section with fields like fsGroup, seLinuxOptions with a level such as s zero colon c twenty-six comma c zero, and a seccompProfile.

Examining the MCS levels

The structure of the MCS level consists of a sensitivity level, such as s zero, and two categories, such as c eight and c seven, as shown in the output from the previous command. You may have noticed that the order of the categories is reversed in the oc output compared with the process status command. This makes no difference in how the Linux kernel reads and acts on the MCS level.

A detailed discussion of how different MCS levels can interact is out of scope. OpenShift assumes that applications deployed in the same project will need to interact with each other. With that in mind, the pods in a project have the same MCS level. Sharing an MCS level lets applications share resources easily and simplifies the security configuration you need to make for your cluster.

Let us examine the SELinux configuration for a pod in a different project. You already know the MCS level for app dash c l i. Because the app dash c l i and app dash g u i pods are in the same project, they should have the same MCS level.

To get the MCS level of the app dash g u i pod, use the same oc get command. First, extract the pods for the project, and then pull the manifest, searching for the relevant field to verify that the levels are indeed the same in both pods in the same project, just as described above.

For example, you might run a command to list the pods in the image dash uploader namespace, and then get the YAML for the app dash c l i pod, searching for the level field. The output would show the level as s zero colon c twenty-six comma c zero, confirming that both pods share the same MCS level.


This confirms what we have stated earlier: the SELinux levels for the app-gui and app-cli pods are the same because they are deployed in the same project.

Now, let’s use the wildfly-app you deployed in the earlier chapter. To get the name of the deployed pod that is running, you can use the oc command to list the pods in that project. Then, compare the security options of the SELinux labels.

If you display the manifest output for the stateful-apps project, specifically for the wildfly-app, you will see that the SELinux levels are completely different from those in the app-gui and app-cli pods. For example, when you inspect the manifest file for the wildfly-app pod, you will notice a securityContext section. In this section, the seLinuxOptions level is set to s0 colon c26 comma c10. This c10 MCS level rule is different from the one used in the image-uploader project.

Each project uses a unique MCS, or Multi-Category Security, level for deployed applications. This MCS level ensures that applications within a project can only communicate with resources in the same project. This isolation is a key part of OpenShift’s security model.

Let’s continue by looking at other pod security context components, starting with pod capabilities.

Managing Linux capabilities

The capabilities listed in the app-cli security context are Linux capabilities that have been removed from the container process. Linux capabilities are permissions that the Linux kernel assigns to, or removes from, processes.

In the security context, you might see a list of capabilities such as KILL, MKNOD, STGID, SETUID, and SYS-CHROOT being dropped. This means these permissions are explicitly removed from the container process.

Capabilities allow a process to perform administrative tasks on the system. For example, the root user on a Linux server can run commands with all Linux capabilities by default. This is why the root user can perform tasks like opening TCP ports below one thousand twenty-four, which is provided by the CAP_NET_BIND_SERVICE capability, or loading modules into the Linux kernel, which is provided by the CAP_SYS_MODULE capability.

If a pod needs to perform a specific administrative task, you can add the required capabilities to the add list in the pod’s security context. Conversely, to remove default capabilities from pods, you add them to the drop list. This is the default action in OpenShift. The goal is to assign the fewest possible capabilities for a pod to function properly. This least-privileged model ensures that pods cannot perform tasks unrelated to their application’s proper function.

By default, the privileged option is set to false. Setting the privileged option to true is equivalent to giving the pod the capabilities of the root user on the system. Although this should not be common practice, privileged pods can exist and be useful in certain circumstances. For example, the HAProxy pod runs as a privileged container so it can bind to port eighty on its node to handle incoming application requests. When an application needs access to host resources that cannot be easily provided to the pod, running a privileged container may help.

The last value in the security context that we need to look at is what controls the user ID that the pod runs with—the runAsUser parameter.

Controlling the user ID

In OpenShift, by default, each project deploys pods using a random user ID, or UID. Just like the MCS level, the UID is common for all pods in a project to allow easier interactions between pods when needed. The UID for each pod is listed in the security context under the runAsUser parameter.

By default, OpenShift does not allow applications to be deployed using UID zero, which is the default UID for the system’s root user. While there are no known ways for UID zero to break out of a container, being UID zero in a container means you must be incredibly careful about removing capabilities and ensuring proper file ownership on the system. This is a precaution that can prevent security issues down the road.

The components in a pod or container security context are controlled by the security context constraints, or SCC, assigned to the pod when it is deployed. An SCC is a configuration applied to pods that outlines the security context components it will operate with. We will discuss SCCs in more depth in the next section, when you deploy an application in your cluster that needs a more privileged security context than the default one. This application is a container image scanning utility that looks for security issues in container images in your OpenShift registry.

Scanning container images

OpenShift is only as secure as the containers it deploys. Even if your container images are built using proven, vetted base images supplied by vendors or created using your own secure workflows, you still need a process to ensure that the images you are using do not develop security issues as they age in your cluster.

The most straightforward solution for this challenge is to scan your container images. In this section, we are going to scan a single container image on demand. In a production environment, image scanning should be an integral component of your application deployment workflow.

Companies like Black Duck Software and Twistlock offer image scanning and compliance tools that integrate with OpenShift. You must be able to trust what is running in your containers and quickly fix issues when they are found. In recent years, an entire industry has emerged to provide container image scanning products to help make this a daily reality.

These scanning utilities can annotate or tag images with metadata. That metadata is then used to determine if the image is considered a security risk. In the next section, we will see how to manually annotate images ourselves. The process is the same as what an automated image scanner would do.

Annotating images with security information

OpenShift is configured with image policies that control which images are allowed to run on your cluster. The full documentation can be found at the official Red Hat OpenShift documentation page.

Annotations in the image metadata enforce image policies, and you can add these annotations manually. The deny-execution policy prevents an image from running on the cluster under any condition. To apply this policy to an image, you can use the annotate command with the oc command line tool for OpenShift.

This command will annotate the image with the specified SHA ID, setting the images.openshift.io slash deny-execution annotation to true. This prevents the image from being used for deployments.

Image policies do not affect running pods, but they prevent an image with the deny-execution annotation from being used for new deployments. To see this in action, delete the active pods for the annotated image. Normally, the replication controller for the deployment will automatically deploy a new version of the pod based on the correct base image. However, in this case, no new pod will be deployed. The replication controller will be stopped when it checks and sees that the image is annotated, and therefore it will not be allowed to execute the container runtime and start a new container for the target image.

If you look at the events for the app project, you can see that the image policies in OpenShift are reading the annotation that was added to the image and preventing a new pod from being deployed.

For example, when you view the events for a given namespace or project, you might notice a warning labeled FailedCreate. The error message will indicate that the pod is invalid because the specified container image is forbidden. The message will state that the image is prohibited by policy.

This demonstrates how OpenShift enforces image policies and helps maintain the security of your cluster by preventing the use of images that have been flagged as security risks.


This process involves manually scanning a container image. If any security issues are found, an annotation is added to the image. The OpenShift image-policy engine reads this annotation, and as a result, it prevents any new pods from being deployed using that image.

Automated solutions, such as Black Duck and Twistlock, perform this scanning and annotation dynamically. They include details about the security findings and information about the scan itself. These annotations are valuable for security reporting, and they help ensure that only the most secure applications are deployed in OpenShift at all times.

We began this section by discussing SELinux, and then moved up to the security context, which defines how pods are assigned security permissions in OpenShift. As mentioned at the start, this is not a comprehensive list or a complete security workflow. Our aim has been to introduce the most important security concepts in OpenShift, and to provide you with enough information to start using and customizing them as you gain more experience with OpenShift.


