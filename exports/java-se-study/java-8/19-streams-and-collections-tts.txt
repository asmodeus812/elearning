Streams

Streams are one of the two most notable features introduced in Java 8. The other major feature is the lambda function. In fact, the lambda functional interface enables the existence of the Streams API.

In Java, streams are special types of objects that are not designed to hold any data themselves. Instead, they operate on data structures such as lists and maps. Importantly, streams do not modify the original data structure in any way. If a stream operation needs to produce a result—such as an aggregate value or a filtered version of the original data—it will create a completely new instance of the structure. This new instance contains only the relevant elements, while the source structure remains unchanged.

Streams provide a wrapper API around common data structures. This allows for operations like filtering, searching, ordering, aggregation, combining, grouping, and many other data transformation utilities, all built on top of the standard Java collection utilities.

BaseStream

The BaseStream interface is the super interface for all other stream interfaces, including the Stream interface itself. It contains the most fundamental methods that every stream must support. Two of the most important methods are close and iterator.

The BaseStream interface defines several key methods. The iterator method allows you to obtain an iterator for the stream, using the iterator of the underlying data structure. The spliterator method provides a split-iterator, which is useful for parallel processing. The sequential method returns a sequential stream representation of the source stream. If the stream is already sequential, it simply returns the same instance. The parallel method returns a parallel stream representation, and again, if the stream is already parallel, it returns the same instance. The unordered method returns an unordered stream representation, following the same pattern. Finally, the close method closes the stream, which means that no further terminal operations can be called on it.

Stream

The Stream interface extends BaseStream. It is designed to provide the most commonly used aggregation and transformation operations that can be performed on any data structure. This interface is central to the Streams API and enables powerful, expressive data processing in Java.


Java Stream API: Method Signatures and Descriptions

This section covers the most important methods available in the Java Stream API, along with a brief explanation of what each method does and why it matters.

The filter method takes a predicate, which is a function that returns true or false, and returns a stream containing only the elements that match the given condition.

The map method transforms each element of the stream using a provided mapping function, and returns a new stream with the mapped elements.

The mapToInt method converts each element in the stream into an integer, and returns a specialized stream of integers, called an IntStream.

The mapToLong method works similarly, but converts each element into a long value, returning a LongStream.

The mapToDouble method transforms each element into a double, and returns a DoubleStream.

The flatMap method takes a function that maps each element to a stream, and then flattens the result into a single stream. This is useful when you have nested collections and want to process all elements as a single sequence.

The flatMapToInt method is like flatMap, but it flattens streams of integers into a single IntStream.

The flatMapToLong method flattens streams of long values into a LongStream.

The flatMapToDouble method flattens streams of double values into a DoubleStream.

The mapMulti method applies a function to each element, which can produce zero or more results per input element, and returns a new stream with all these results. This is useful for cases where one input might map to multiple outputs.

The mapMultiToInt method is a multi-mapping function that produces an IntStream, allowing each input to generate multiple integer outputs.

The mapMultiToLong method is similar, but produces a LongStream.

The mapMultiToDouble method produces a DoubleStream, allowing for multiple double outputs per input.

The distinct method returns a stream with only unique elements, removing any duplicates.

The sorted method returns a stream where the elements are sorted in their natural order.

The sorted method with a comparator allows you to specify a custom sorting order for the elements in the stream.

The peek method returns a stream where an action is performed on each element as it is consumed. This is mainly used for debugging or logging purposes.

The limit method restricts the stream to a specified number of elements, discarding the rest.

The skip method skips the first n elements in the stream, returning the remaining elements.

The takeWhile method returns a stream consisting of the longest prefix of elements that match a given predicate. As soon as an element does not match, the stream ends.

The dropWhile method does the opposite: it drops the longest prefix of elements that match the predicate, and returns the rest.

The forEach method performs a given action for each element of the stream. This is often used for side effects, such as printing or updating external state.

The forEachOrdered method is similar, but guarantees that the action is performed in the encounter order of the stream, preserving the original sequence.

The toArray method collects all elements of the stream into an array.

The toArray method with a generator function allows you to specify how the array should be created, which can be useful for creating arrays of a specific type.

The reduce method with an identity and an accumulator performs a reduction on the elements, starting with an identity value and combining elements using the accumulator function.

The reduce method with only an accumulator performs a reduction without an identity value, combining elements pairwise.

The reduce method with an identity, an accumulator, and a combiner is used for parallel streams, allowing for efficient reduction by combining partial results.

The collect method with a supplier, accumulator, and combiner performs a mutable reduction, such as collecting elements into a collection.

The collect method with a collector uses a Collector object to handle accumulation and final value production, which is a flexible way to gather results.

The toList method collects the elements of the stream into a List.

The min method finds the minimum element of the stream using a provided comparator.

The max method finds the maximum element of the stream using a comparator.

The count method returns the number of elements in the stream.

The anyMatch method returns true if any elements in the stream match the provided predicate.

The allMatch method returns true if all elements in the stream match the predicate.

These methods form the core of the Java Stream API, enabling powerful and expressive data processing pipelines.


Let’s walk through the key operations available in Java Streams, and how they are classified and used.

First, there are several important methods you can use with streams.

The allMatch method returns true if all elements in the stream match the provided condition, or predicate.

The noneMatch method returns true if no elements in the stream match the given predicate.

The findFirst method returns the first element of the stream, if one is present.

The findAny method returns any element from the stream, which is especially useful when working with parallel streams.

The builder method returns a Stream Builder, which allows you to incrementally build a stream by adding elements one at a time.

The empty method returns an empty stream, containing no elements.

The of method, when given a single value, returns a stream containing just that one element.

The ofNullable method returns a stream containing the provided element if it is not null. If the value is null, it returns an empty stream.

The of method, when given multiple values, returns a stream containing all of those elements.

The iterate method, with a seed and a function, creates an infinite stream where each next element is generated by applying the function to the previous one, starting with the seed value.

A variant of iterate also takes a predicate, called hasNext, and continues generating elements as long as the predicate returns true.

The generate method creates an infinite stream where each element is produced by a supplier function.

The concat method combines two streams into a single stream.

Now, each of these operations falls into one of two major groups: intermediate or terminal.

Intermediate operations do not close the stream. They do not produce a final output, and you can chain more intermediate or terminal operations after them. Examples include map, filter, and sorted.

Terminal operations, on the other hand, close the stream. After a terminal operation, you cannot use the stream for further processing. The stream is considered consumed, and all intermediate operations attached to it are executed at this point. Terminal operations produce a result, such as a collection, a single value, or, in the case of forEach, no return value at all.

Intermediate operations always produce another stream instance. Usually, they return the same stream instance, not a copy. Terminal operations, in contrast, produce a result or side effect.

It’s important to note that intermediate operations are not executed immediately when you attach them to a stream. They are only executed when a terminal operation is called.

Another key point about intermediate operations is that most of them are stateless. This means they operate on each element of the stream independently. However, some operations, like sorted, do maintain state or relationships between elements, because sorting requires comparing elements to each other. This distinction becomes important when discussing parallel, sequential, or unordered stream types.

Let’s review some common stream methods and their classification.

The following methods are intermediate operations: filter, map, mapToInt, mapToLong, mapToDouble, flatMap, flatMapToInt, flatMapToLong, flatMapToDouble, mapMulti, mapMultiToInt, mapMultiToLong, mapMultiToDouble, distinct, sorted, sorted with a comparator, peek, limit, skip, takeWhile, dropWhile, iterate, generate, concat, accept for the builder, and add for the builder.

The following methods are terminal operations: forEach, forEachOrdered, toArray, toArray with an IntFunction, reduce, reduce with a BinaryOperator, collect, toList, min, max, count, anyMatch, allMatch, noneMatch, findFirst, findAny, and build for the builder.

Obtaining a Stream

To obtain a stream, the most common use case is to call the stream or parallelStream methods on a data structure that implements the Collection interface. If a parallel stream cannot be obtained, a regular sequential stream will be returned instead. The method will not throw an exception.

A stream can be converted into a sequential or parallel stream at any time, depending on your needs. Each call to sequential or parallel creates a new stream instance using the source data of the original. The new stream will be either parallel or sequential, as specified.

Mapping

One of the most common operations is mapping, which transforms a stream of one type of elements into another type. Mapping usually takes a function that accepts an element of the original type and returns an element of the new type. This function is applied to all elements in the stream.

For example, consider a list of numbers. You can use the map operation to square each number in the list, and then collect the results into a new list. In code, this would look like creating a list of numbers, calling parallelStream, mapping each number to its square, and then collecting the results into a list.

Filtering

Another common operation is filtering. Filtering applies a function to each element in the stream, and keeps only those elements for which the function returns true. If the function returns false, the element is discarded.

For example, you might have a list of numbers and want to keep only the even numbers. You would use the filter operation with a function that checks if a number is even, and then collect the results into a new list.

Collecting

Collecting is the process of converting a stream into a collection, such as a list, map, set, or even a custom user-defined collection. There are no real restrictions on how the collection is done, as long as the collection provides an implementation of the Collector interface.

There is a utility class called Collectors, which contains common methods for collecting streams into lists, sets, maps, and more. Some of the most frequently used methods are toList, toSet, and toMap.

ToList

One of the most frequently used methods in the Collectors API is toList. It provides two main ways to create a list from the underlying stream.


Let’s start by looking at the toList collector.

The toList collector uses an ArrayList as its underlying implementation. It adds all elements from the stream to this list. Remember, the elements themselves are added by reference. The only thing that gets discarded is the original data structure that the stream wrapped around when it was created.

Next, there is toUnmodifiableList. This works similarly to toList, still using an ArrayList to collect the elements. However, at the end, the elements are moved from the mutable ArrayList into an unmodifiable list. This is done using the List.of API, which collects the elements into an immutable list, implemented internally by the Java Development Kit as ImmutableCollections.

Now, let’s talk about ToSet.

The ToSet collector works much like toList. It provides two methods: one to produce a mutable set, and another to produce an immutable one. Internally, it stores elements in a HashSet. For the immutable version, it uses Set.of to create an unmodifiable set.

Moving on to ToMap.

The toMap method has many overloads, offering different ways to collect elements into a map. Most of these overloads deal with how keys are handled and how to resolve collisions. The most basic usage of toMap takes each value from the source and passes it to two mapper functions. One function returns the key for the map, and the other returns the value for that key.

In the first example, the name is mapped through the identity function, so the key is simply the entry from the list. The value for the mapping is the length of the entry, that is, the length of the name.

In the second example, the mapping function remains the same, but a merging function is added. This tells the underlying collectors how to reconcile values that map to the same key.

In the third example, the mapping and merging functions are the same as before, but a third argument is added: the supplier, or map constructor. This allows clients to pass in custom map implementations.

It’s important to note that by default, if there are duplicate keys, the basic version of toMap, which does not receive a merger function, will use an internal one. This internal function will throw an exception if a duplicate key is inserted into the map.

The code block that follows demonstrates these three scenarios. It shows how to collect a list of names into a map, mapping each name to its length, then how to handle duplicate keys by summing counts, and finally, how to provide a custom map implementation.

The toMap functions also have a version called toConcurrentMap. This is designed to optimize performance when collecting elements by leveraging parallel streams. The interface for these concurrent methods is the same as for the basic toMap methods.

Now, let’s discuss Grouping.

Grouping is a special case of mapping that produces a map where a merging function is designed to collect all values with the same key into a bucket, usually an array or list. The idea is that a list of entries or values can be grouped by some common property, so that when they map to the same key, they are collected together.

In the first example, names are grouped by the length of the name, using a simple classifier function. In the second and third examples, grouping factory methods are provided. One is for the top-level map result, which represents the grouping, and the second is for the value type, which is where the elements are accumulated.

Just like the toMap methods, the grouping methods also support a concurrent version. This optimizes grouping by using parallel streaming.

The code block that follows demonstrates grouping names by their length, and shows how to provide custom factories for the map and the grouped values.

Next, let’s talk about Reduction.

One of the key features of streams is reduction operations. These are terminal operations that return a result based on the elements in the stream, and the result is not of the same type as the initial data structure wrapped in the stream.

The code block that follows shows how to use reduction operations. It creates an ArrayList of integers, adds several numbers to it, and then creates a stream from the list. The min method is called on the stream to reduce the result to a single minimum value. This call to min terminates the stream, meaning that no more terminal or intermediate operations can be called on it. To perform another reduction, such as finding the maximum value, a new stream must be obtained from the list.

The list of reduction operations supported by the stream API includes several special cases of the general reduce operation. These exist for convenience, since they are often used. Examples include min, max, sum, and count.

The reduce operation is the general reduction method, accepting an accumulator lambda. The count operation returns the number of elements in the stream. The min and max operations compute the minimum and maximum elements, respectively. The sum operation computes the sum of all elements, and average computes the average value. There are also operations like anyMatch, allMatch, and noneMatch, which check if any, all, or none of the elements match a given predicate. Finally, findFirst and findAny are used to find the first or any element that matches a predicate, with findAny being especially relevant for parallel streams.

There are some important restrictions for reduction operations. The reduction must be stateless, meaning the lambda or operation itself must not store any state about the iteration process or the elements being visited. It must also be non-interfering, meaning the reduction operation must never interfere with or mutate the source structure while the reduction is being executed. Finally, the reduction must be associative. This means that no matter how the elements are traversed, the reduction must always produce the correct result without storing any state about the elements being traversed. For example, multiplication is associative, so the order does not matter, but an expression like ten times the sum of two and seven is not associative, so you cannot rely on the reduce operation to be correct in that case.

Associativity is particularly important for reduction operations on parallel streams, which we will discuss next.

Now, let’s look at Parallel streams.

Parallel streams are especially important when processing large amounts of data. They can dramatically speed up execution of certain operations. A parallel stream can be obtained either at the time of obtaining the stream, using the parallelStream method instead of stream from the Collections API, or by converting an existing stream to a parallel one using the API provided by the BaseStream interface.


Parallel Streams and Reduce Operations

Parallel streams in Java have a unique caveat when it comes to reduce operations. Because a parallel stream can split its workload, multiple reduce operations may run on separate chunks of the stream’s elements at the same time. To handle this, you must provide an additional function called a combiner. This combiner function tells the underlying implementation how to merge results from different parallel executions.

For example, imagine you want to multiply the squares of all elements in a list. First, you create an ArrayList of integers and add several numbers to it. Then, you obtain a parallel stream from this list. When you call the reduce method on this parallel stream, you provide three arguments: an initial value, an accumulator function, and a combiner function. The accumulator function multiplies the current result by the square of each element. The combiner function multiplies two partial results together. This setup ensures that, even though the computation is split across multiple threads, the final result is correct.

In contrast, if you use a sequential stream, you only need to provide the accumulator function. There is no need for a combiner because the computation happens in a single thread, so there are no partial results to merge.

To summarize, the accumulator function tells the stream how to combine the current result with each element from the list. The combiner function tells the stream how to merge two partial results, which is essential for parallel processing.

How Parallel Streams Work Internally

Under the hood, Java’s parallel stream implementation uses the fork and join framework along with split iterators. Here’s the basic idea:

First, the Spliterator divides the source data into smaller segments. Each segment can be processed independently by different threads.

Next, the Fork and Join framework manages a pool of worker threads. Each worker thread takes one of the segments provided by the Spliterator and processes it.

Finally, after processing, the results from the segments are combined using the combiner function. This step is crucial for operations like reduce, where partial results must be aggregated into a final result.

Unordered Streams

Another important property of streams is whether they are ordered or unordered. This characteristic affects how data is processed in certain situations. Generally, the ordered or unordered nature of a stream depends on the underlying data structure.

If the stream is based on structures like ArrayList, LinkedList, or TreeSet, it is considered ordered. On the other hand, streams based on HashMap or HashSet are considered unordered.

The unordered property becomes especially relevant for parallel streams. If a parallel stream is ordered, processing still happens in parallel, but when it’s time to combine the results, they are merged in the original order. This may require waiting for all parallel tasks to finish before combining and sorting the results.

If the stream is unordered, the combination of results can happen in any order. As soon as two partial results are ready, they can be combined, without waiting for the rest. This can lead to faster processing, as the system does not need to maintain any specific order.

In summary, parallel streams will eagerly combine results as soon as chunks are finished and ready, especially when the stream is unordered. When order matters, the system may need to wait and sort before combining, which can affect performance.


