<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>openshift-deep-dive</title>
  <style>
    html {
      font-size: 12pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  \usepackage{listings}
  \usepackage{xcolor}

  \lstset{
      basicstyle=\ttfamily,
      backgroundcolor=\color{black!10},
      showspaces=false,
      showstringspaces=false,
      showtabs=false,
      tabsize=2,
      captionpos=b,
      breaklines=true,
      breakautoindent=true,
      linewidth=\textwidth
  }
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#container-platform" id="toc-container-platform">Container
platform</a>
<ul>
<li><a href="#containers-in-openshift"
id="toc-containers-in-openshift">Containers in OpenShift</a></li>
<li><a href="#orchestrating-containers"
id="toc-orchestrating-containers">Orchestrating Containers</a></li>
</ul></li>
<li><a href="#examining-the-architecture"
id="toc-examining-the-architecture">Examining the architecture</a>
<ul>
<li><a href="#integrating-container-images"
id="toc-integrating-container-images">Integrating container
images</a></li>
<li><a href="#accessing-applications"
id="toc-accessing-applications">Accessing applications</a></li>
<li><a href="#handling-network-traffic"
id="toc-handling-network-traffic">Handling network traffic</a></li>
</ul></li>
<li><a href="#examining-an-application"
id="toc-examining-an-application">Examining an application</a>
<ul>
<li><a href="#building-an-application"
id="toc-building-an-application">Building an application</a></li>
<li><a href="#deploying-and-serving-applications"
id="toc-deploying-and-serving-applications">Deploying and serving
applications</a></li>
</ul></li>
<li><a href="#use-case-for-platforms"
id="toc-use-case-for-platforms">Use case for platforms</a></li>
<li><a href="#technology-use-cases"
id="toc-technology-use-cases">Technology use cases</a></li>
<li><a href="#businesses-use-cases"
id="toc-businesses-use-cases">Businesses use cases</a></li>
<li><a href="#invalid-use-cases" id="toc-invalid-use-cases">Invalid use
cases</a></li>
<li><a href="#container-storage" id="toc-container-storage">Container
storage</a></li>
<li><a href="#scaling-applications"
id="toc-scaling-applications">Scaling applications</a></li>
<li><a href="#integrating-stateful-and-stateless-apps"
id="toc-integrating-stateful-and-stateless-apps">Integrating stateful
and stateless apps</a></li>
</ul></li>
<li><a href="#starting" id="toc-starting">Starting</a>
<ul>
<li><a href="#cluster-runtimes" id="toc-cluster-runtimes">Cluster
runtimes</a></li>
<li><a href="#cluster-installation"
id="toc-cluster-installation">Cluster installation</a></li>
<li><a href="#cluster-login" id="toc-cluster-login">Cluster
login</a></li>
<li><a href="#cluster-config" id="toc-cluster-config">Cluster
config</a></li>
<li><a href="#cluster-debugging" id="toc-cluster-debugging">Cluster
debugging</a></li>
<li><a href="#cluster-software" id="toc-cluster-software">Cluster
software</a></li>
<li><a href="#first-project" id="toc-first-project">First
project</a></li>
<li><a href="#application-components"
id="toc-application-components">Application Components</a>
<ul>
<li><a href="#container-images" id="toc-container-images">Container
images</a></li>
<li><a href="#build-configs" id="toc-build-configs">Build
configs</a></li>
<li><a href="#deployment-configs" id="toc-deployment-configs">Deployment
configs</a></li>
<li><a href="#image-stream" id="toc-image-stream">Image stream</a></li>
</ul></li>
<li><a href="#deploying-an-app" id="toc-deploying-an-app">Deploying an
app</a></li>
<li><a href="#providing-access-to-apps"
id="toc-providing-access-to-apps">Providing access to apps</a></li>
<li><a href="#exposing-application-services"
id="toc-exposing-application-services">Exposing application
services</a></li>
</ul></li>
<li><a href="#containers" id="toc-containers">Containers</a>
<ul>
<li><a href="#defining-containers" id="toc-defining-containers">Defining
containers</a></li>
<li><a href="#openshift-component-interaction"
id="toc-openshift-component-interaction">OpenShift component
interaction</a></li>
<li><a href="#openshift-manages-deployments"
id="toc-openshift-manages-deployments">OpenShift manages
deployments</a></li>
<li><a href="#kubernetes-schedules-applications"
id="toc-kubernetes-schedules-applications">Kubernetes schedules
applications</a></li>
<li><a href="#docker-creates-containers"
id="toc-docker-creates-containers">Docker creates containers</a></li>
<li><a href="#linux-isolates-resources"
id="toc-linux-isolates-resources">Linux isolates resources</a></li>
<li><a href="#working-with-cluster"
id="toc-working-with-cluster">Working with cluster</a></li>
<li><a href="#listing-kernel-components"
id="toc-listing-kernel-components">Listing kernel components</a>
<ul>
<li><a href="#mount-namespace" id="toc-mount-namespace">Mount
namespace</a></li>
<li><a href="#uts-namespace" id="toc-uts-namespace">UTS
namespace</a></li>
<li><a href="#pid-namespace" id="toc-pid-namespace">PID
namespace</a></li>
<li><a href="#memory-namespace" id="toc-memory-namespace">Memory
namespace</a></li>
<li><a href="#networking-namespace"
id="toc-networking-namespace">Networking namespace</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#cloud-native-apps" id="toc-cloud-native-apps">Cloud native
apps</a>
<ul>
<li><a href="#testing-app-resiliency"
id="toc-testing-app-resiliency">Testing app resiliency</a></li>
</ul></li>
<li><a href="#scaling-applications-1"
id="toc-scaling-applications-1">Scaling applications</a></li>
<li><a href="#application-health"
id="toc-application-health">Application Health</a>
<ul>
<li><a href="#creating-liveness-probes"
id="toc-creating-liveness-probes">Creating liveness probes</a></li>
<li><a href="#creating-readiness-probes"
id="toc-creating-readiness-probes">Creating readiness probes</a></li>
</ul></li>
<li><a href="#auto-scaling-with-metrics"
id="toc-auto-scaling-with-metrics">Auto-scaling with metrics</a>
<ul>
<li><a href="#determining-expected-workloads"
id="toc-determining-expected-workloads">Determining expected
workloads</a></li>
<li><a href="#installing-openshift-metrics"
id="toc-installing-openshift-metrics">Installing OpenShift
metrics</a></li>
<li><a href="#understanding-the-metrics"
id="toc-understanding-the-metrics">Understanding the metrics</a></li>
<li><a href="#using-pod-metrics-autoscaling"
id="toc-using-pod-metrics-autoscaling">Using pod metrics &amp;
autoscaling</a></li>
<li><a href="#testing-the-autoscaling-setup"
id="toc-testing-the-autoscaling-setup">Testing the autoscaling
setup</a></li>
<li><a href="#avoiding-thrashing" id="toc-avoiding-thrashing">Avoiding
thrashing</a></li>
</ul></li>
<li><a href="#continuous-integration-deployment"
id="toc-continuous-integration-deployment">Continuous integration &amp;
deployment</a>
<ul>
<li><a href="#container-images-are-the-centerpiece"
id="toc-container-images-are-the-centerpiece">Container images are the
centerpiece</a></li>
<li><a href="#promoting-images" id="toc-promoting-images">Promoting
images</a></li>
<li><a href="#cicd1-creating-an-environment---todo-this-needs-rewrite"
id="toc-cicd1-creating-an-environment---todo-this-needs-rewrite">CI/CD:1
Creating an environment - TODO: this needs rewrite</a></li>
<li><a href="#deployment-strategies"
id="toc-deployment-strategies">Deployment strategies</a></li>
</ul></li>
<li><a href="#stateful-applications"
id="toc-stateful-applications">Stateful applications</a>
<ul>
<li><a href="#container-storage-1"
id="toc-container-storage-1">Container storage</a></li>
</ul></li>
<li><a href="#handling-permanency" id="toc-handling-permanency">Handling
permanency</a></li>
<li><a href="#creating-resources" id="toc-creating-resources">Creating
resources</a>
<ul>
<li><a href="#creating-storage" id="toc-creating-storage">Creating
storage</a></li>
<li><a href="#logging-in-as-kubeadmin"
id="toc-logging-in-as-kubeadmin">Logging in as
<code>kubeadmin</code></a></li>
<li><a href="#physical-volume" id="toc-physical-volume">Physical
volume</a></li>
<li><a href="#creating-volumes" id="toc-creating-volumes">Creating
volumes</a></li>
<li><a href="#creating-claims" id="toc-creating-claims">Creating
claims</a></li>
<li><a href="#modifying-deployment"
id="toc-modifying-deployment">Modifying deployment</a></li>
</ul></li>
<li><a href="#volume-mounts" id="toc-volume-mounts">Volume
mounts</a></li>
</ul></li>
<li><a href="#stateful-applications-1"
id="toc-stateful-applications-1">Stateful applications</a>
<ul>
<li><a href="#enabling-a-headless-services"
id="toc-enabling-a-headless-services">Enabling a headless
services</a></li>
<li><a href="#application-clustering-with-wildfly"
id="toc-application-clustering-with-wildfly">Application clustering with
WildFly</a></li>
<li><a href="#querying-the-openshift-server"
id="toc-querying-the-openshift-server">Querying the OpenShift
server</a></li>
<li><a href="#verify-the-data-replication"
id="toc-verify-the-data-replication">Verify the data
replication</a></li>
<li><a href="#other-cases-for-direct-pod-access"
id="toc-other-cases-for-direct-pod-access">Other cases for direct pod
access</a></li>
<li><a href="#describing-sticky-sessions"
id="toc-describing-sticky-sessions">Describing sticky sessions</a></li>
<li><a href="#toggling-sticky-sessions"
id="toc-toggling-sticky-sessions">Toggling sticky sessions</a></li>
<li><a href="#limitations-of-cookies"
id="toc-limitations-of-cookies">Limitations of cookies</a></li>
<li><a href="#shutting-down-applications"
id="toc-shutting-down-applications">Shutting down applications</a>
<ul>
<li><a href="#application-grace-period"
id="toc-application-grace-period">Application grace period</a></li>
<li><a href="#container-lifecycle-hooks"
id="toc-container-lifecycle-hooks">Container lifecycle hooks</a></li>
</ul></li>
<li><a href="#stateful-sets" id="toc-stateful-sets">Stateful sets</a>
<ul>
<li><a href="#deterministic-sequence"
id="toc-deterministic-sequence">Deterministic sequence</a></li>
<li><a href="#examining-a-stateful-set"
id="toc-examining-a-stateful-set">Examining a stateful set</a></li>
<li><a href="#constant-network-identity"
id="toc-constant-network-identity">Constant Network identity</a></li>
<li><a href="#consistent-persistent-storage"
id="toc-consistent-persistent-storage">Consistent persistent
storage</a></li>
<li><a href="#the-stateful-set-limitations"
id="toc-the-stateful-set-limitations">The Stateful set
limitations</a></li>
<li><a href="#non-native-stateful-applications"
id="toc-non-native-stateful-applications">Non-native stateful
applications</a></li>
<li><a href="#cleanup-resource-limits-quotas"
id="toc-cleanup-resource-limits-quotas">Cleanup resource limits &amp;
quotas</a></li>
</ul></li>
</ul></li>
<li><a href="#operations-security"
id="toc-operations-security">Operations &amp; Security</a>
<ul>
<li><a href="#permissions-vs-wild-west"
id="toc-permissions-vs-wild-west">Permissions vs Wild-West</a>
<ul>
<li><a href="#setting-up-authentication"
id="toc-setting-up-authentication">Setting up authentication</a></li>
<li><a href="#introduction-to-htpasswd"
id="toc-introduction-to-htpasswd">Introduction to htpasswd</a></li>
<li><a href="#creating-htpasswd-files"
id="toc-creating-htpasswd-files">Creating htpasswd files</a></li>
<li><a href="#changing-the-provider"
id="toc-changing-the-provider">Changing the provider</a></li>
</ul></li>
<li><a href="#working-with-roles" id="toc-working-with-roles">Working
with roles</a>
<ul>
<li><a href="#assigning-user-roles"
id="toc-assigning-user-roles">Assigning user roles</a></li>
<li><a href="#creating-administrators-user"
id="toc-creating-administrators-user">Creating administrators
user</a></li>
<li><a href="#setting-default-roles"
id="toc-setting-default-roles">Setting default roles</a></li>
</ul></li>
<li><a href="#limit-ranges" id="toc-limit-ranges">Limit ranges</a>
<ul>
<li><a href="#define-resource-limit-ranges"
id="toc-define-resource-limit-ranges">Define resource limit
ranges</a></li>
</ul></li>
<li><a href="#resource-quotas" id="toc-resource-quotas">Resource
quotas</a>
<ul>
<li><a href="#creating-compute-quotas"
id="toc-creating-compute-quotas">Creating compute quotas</a></li>
<li><a href="#creating-resource-quotas"
id="toc-creating-resource-quotas">Creating resource quotas</a></li>
</ul></li>
<li><a href="#working-with-quotas-limits"
id="toc-working-with-quotas-limits">Working with quotas &amp; limits</a>
<ul>
<li><a href="#quotas-to-existing-applications"
id="toc-quotas-to-existing-applications">Quotas to existing
applications</a></li>
<li><a href="#changing-quotas-for-deployed-applications"
id="toc-changing-quotas-for-deployed-applications">Changing quotas for
deployed applications</a></li>
</ul></li>
<li><a href="#cgroups-for-managing-resources"
id="toc-cgroups-for-managing-resources">Cgroups for managing
resources</a>
<ul>
<li><a href="#overview-of-the-cgroups"
id="toc-overview-of-the-cgroups">Overview of the cgroups</a></li>
<li><a href="#identifying-container-cgroups"
id="toc-identifying-container-cgroups">Identifying container
cgroups</a></li>
<li><a href="#confirming-cgroups-limits"
id="toc-confirming-cgroups-limits">Confirming cgroups limits</a></li>
</ul></li>
</ul></li>
<li><a href="#networking" id="toc-networking">Networking</a>
<ul>
<li><a href="#managing-the-sdn" id="toc-managing-the-sdn">Managing the
SDN</a>
<ul>
<li><a href="#configure-application-node-network"
id="toc-configure-application-node-network">Configure application node
network</a></li>
<li><a href="#linking-containers-to-host-interfaces"
id="toc-linking-containers-to-host-interfaces">Linking containers to
host interfaces</a></li>
<li><a href="#working-with-ovs" id="toc-working-with-ovs">Working with
OVS</a></li>
</ul></li>
<li><a href="#routing-application-requests"
id="toc-routing-application-requests">Routing application
requests</a></li>
<li><a href="#using-the-haproxy-service"
id="toc-using-the-haproxy-service">Using the HAProxy service</a>
<ul>
<li><a href="#investigating-the-haproxy-service"
id="toc-investigating-the-haproxy-service">Investigating the HAProxy
service</a></li>
<li><a href="#haproxy-and-request-routing"
id="toc-haproxy-and-request-routing">HAProxy and request
routing</a></li>
</ul></li>
<li><a href="#locating-services-with-internal-dns"
id="toc-locating-services-with-internal-dns">Locating services with
internal DNS</a>
<ul>
<li><a href="#dns-resolution-in-pod-network"
id="toc-dns-resolution-in-pod-network">DNS resolution in pod
network</a></li>
</ul></li>
<li><a href="#configure-openshift-sdn"
id="toc-configure-openshift-sdn">Configure OpenShift SDN</a>
<ul>
<li><a href="#using-the-ovs-subnet" id="toc-using-the-ovs-subnet">Using
the ovs-subnet</a></li>
<li><a href="#using-the-ovs-multitenant"
id="toc-using-the-ovs-multitenant">Using the ovs-multitenant</a></li>
<li><a href="#creating-advanced-network-designs"
id="toc-creating-advanced-network-designs">Creating advanced network
designs</a></li>
<li><a href="#enabling-multi-tenant-plugin"
id="toc-enabling-multi-tenant-plugin">Enabling multi tenant
plugin</a></li>
<li><a href="#testing-the-multi-tenant-plugin"
id="toc-testing-the-multi-tenant-plugin">Testing the multi-tenant
plugin</a></li>
</ul></li>
</ul></li>
<li><a href="#security" id="toc-security">Security</a>
<ul>
<li><a href="#selinux-core-concepts"
id="toc-selinux-core-concepts">SELinux core concepts</a></li>
<li><a href="#working-with-selinux-labels"
id="toc-working-with-selinux-labels">Working with SELinux labels</a>
<ul>
<li><a href="#applying-labels-with-selinux-context"
id="toc-applying-labels-with-selinux-context">Applying labels with
SELinux context</a></li>
<li><a href="#enforcing-selinux-with-policies"
id="toc-enforcing-selinux-with-policies">Enforcing SELinux with
policies</a></li>
<li><a href="#isolating-pods-with-levels"
id="toc-isolating-pods-with-levels">Isolating pods with levels</a></li>
<li><a href="#investigating-pod-security-context"
id="toc-investigating-pod-security-context">Investigating pod security
context</a></li>
<li><a href="#examining-the-mcs-levels"
id="toc-examining-the-mcs-levels">Examining the <code>MCS</code>
levels</a></li>
<li><a href="#managing-linux-capabilities"
id="toc-managing-linux-capabilities">Managing Linux
capabilities</a></li>
<li><a href="#controlling-the-user-id"
id="toc-controlling-the-user-id">Controlling the user ID</a></li>
</ul></li>
<li><a href="#scanning-container-images"
id="toc-scanning-container-images">Scanning container images</a></li>
<li><a href="#annotating-images-with-security-information"
id="toc-annotating-images-with-security-information">Annotating images
with security information</a></li>
</ul></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p>Containers are changing how everyone in the IT industry does their
job. Containers initially entered the scene on developers laptops
helping them develop applications more quickly than they could with
virtual machines, or by configuring a laptop’s operating system. As
containers became more common in development environments their use
began to expand. Once limited to laptops and small development labs,
containers worked their way into enterprise. Within a couple of years
containers progressed to the point that they are powering massive
production workloads like Github.</p>
<h2 id="container-platform">Container platform</h2>
<p>A container platform is an application platform that uses containers
to build deploy serve and orchestrate the application running inside it.
OpenShift uses two primary tools to serve applications in containers a
container runtime to create containers in Linux and an orchestration
engine to manage a cluster of servers or also called nodes, these
servers could be actual physical machines, virtual machines or IOT
devices, running the containers.</p>
<h3 id="containers-in-openshift">Containers in OpenShift</h3>
<p>A container runtime works on a Linux server to create and manage
containers. For that to make sense we need to look at how containers
function when they are running on a Linux system. In subsequent sections
we will dig deeply into how containers isolate applications in
OpenShift. To start, you can think of containers as discrete, portable
scalable units for applications. Containers hold everything required for
the application inside them to function. Each time a container is
deployed it holds all the libraries and code needed to its application
to function properly. Apps running inside a, container can only access
the resources in the container. The applications in the container are
isolated from anything running in other containers or on the host. Five
types of resources are isolated with containers.</p>
<ul>
<li>Mounted filesystems.</li>
<li>Shared memory resources</li>
<li>Hostname and domain names</li>
<li>Network resources (IP addresses, MAC addresses, memory buffers)</li>
<li>Process counters</li>
</ul>
<p>We will investigate each one of those separately, throughout the next
sections. In OpenShift the service that handles the creation and
management of containers is docker. Docker is a large active open source
project started by Docker, Inc is the company. The resources that docker
uses to isolate processes in containers all exist as part of the Linux
kernel. These resources include things like SELinux, Linux namespaces
and control groups (cgroups), which will be covered later on in the
sections in detail. In addition to making these resources much easier to
use, docker has also added several features that have enhanced its
popularity and growth. Here are some of the primary benefits of docker
as container runtime:</p>
<ul>
<li>Portability - Earlier attempts at container formats were not
portable between hosts running different operating systems. This
container format is now standardized as part of the Open Container
Initiative.</li>
<li>Image reuse - any container image can be reused as the base for
other container images.</li>
<li>Application centric API - the API and command line tooling allow
developers to quickly create update and delete containers. This is
reflected in the API of the docker engine, as well as the API of
Kubernetes.</li>
<li>Ecosystem - Docker Inc, maintains a free public hosting environment
for container images it now contains several hundred thousand
images.</li>
</ul>
<h3 id="orchestrating-containers">Orchestrating Containers</h3>
<p>Although the docker engine manages containers by facilitating Linux
kernel resources, it is limited to a single host operating system.
Although a single server running containers is interesting, it is not a
platform that you can use to create robust applications. To deploy
highly available and scalable applications you have to be able to deploy
applications containers across multiple servers. To orchestrate
containers across multiple servers effectively you need to use a
container orchestration engine, an application that manages a container
runtime across a cluster, of hosts to provide a scalable application
platform OpenShift uses Kubernetes as its container orchestration
engine, Kubernetes is an application open source project that was
started by Google, in 2015 it was donated to the Cloud Native Computing
Foundation. Kubernetes employs a master/node architecture, Kubernetes
master servers maintain the information about the server cluster and
nodes run the actual application workloads. It is a great open source
project. The community around it is quickly growing and incredibly
active. It is consistently one of the most active projects on github.
But to realize the full power of a container platform, it needs a few
additional components. This is where OpenShift comes in, it uses
docker/containerd and Kubernetes, as a starting point for its design.
But to be a truly effective container platform it adds a few more tools
to provide a better experience for users.</p>
<h2 id="examining-the-architecture">Examining the architecture</h2>
<p>OpenShift uses Kubernetes master/node architecture as the base point.
From there it expands to provide additional services that a good
application platform needs to include out of the box.</p>
<h3 id="integrating-container-images">Integrating container images</h3>
<p>In a container platform like OpenShift container images are created
when applications are deployed or updated, to be effective that
container image have to be available quickly on all the application
nodes in a cluster. To do this OpenShift includes an integrated image
registry as part of its default configuration. An image registry is a
central location that can serve container images to multiple locations.
In OpenShift the integrated registry runs in a container. In addition to
providing tightly integrated images access OpenShift works to make
access to the applications more efficient.</p>
<h3 id="accessing-applications">Accessing applications</h3>
<p>In Kubernetes containers are created on nodes using components called
pods. There are some distinctions that we will discuss in more depth in
next sections, but they are often similar. When an application consists
of more than one pod, access to the application is managed through a
component called a service. A service is a proxy that connects multiple
pods and maps them to an IP address on one or more nodes in the cluster.
IP addresses can be hard to manage and share especially when they are
behind a fire wall. OpenShift helps to solve this problem by providing
an integrated routing layer. The routing layer is a software load
balancer. When an application is deployed in OpenShift a DNS entry is
created for it automatically. That DNS record is added to the load
balancer and the load balancer interfaces with the Kubernetes service to
efficiently handle connections between the deployed applications and its
users. With applications running in pods across multiple nodes and
management requires coming from the master node there a lot of
communication between servers in an OpenShift cluster. You need to make
sure that traffic is properly encrypted and can be separated when
needed.</p>
<h3 id="handling-network-traffic">Handling network traffic</h3>
<p>OpenShift uses a software defined networking (SDN) Solution to
encrypt and shape network traffic in a cluster. OpenShift SDN, solution
that uses <code>Open vSwitch</code>. Other SDN solutions are also
supported, this will be examined in depth in future sections. Now that
you have a good idea of how OpenShift is designed let us look at the
life cycle of an application in an OpenShift cluster.</p>
<h2 id="examining-an-application">Examining an application</h2>
<p>OpenShift has workflows that are designed to help you manage you
applications through all phases of its lifecycle - build, deploy,
upgrade and retirement.</p>
<h3 id="building-an-application">Building an application</h3>
<p>The primary way to build application is to use a builder image. This
process is the default workflow in OpenShift and its what you will use
in next section to deploy your first application in OpenShift. A builder
image is a special container image that includes applications and
libraries needed for an application in a given language. In next section
we will deploy a PHP web application. The builder image you will use for
your first deployment includes the Apache web server and the PHP
language libraries - things needs to run this type of application. The
build process takes the source code for an application and combines it
with the builder image to create a custom application image for the
application. The custom application image is stored in the integrated
registry, where it is ready to be deployed and served to the application
users.</p>
<h3 id="deploying-and-serving-applications">Deploying and serving
applications</h3>
<p>In the default workflow in OpenShift applications deployment is
automatically triggered after the container image is build and
available. The deployment process takes a newly created application
image and deploys it on one or more nodes. In addition the application
pods a service is also created, along with a DNS route in the routing
layer. Users are able to access the newly created application through
the routing layer after all components have been deployed. App upgrades
use the same workflow, when an upgrade is triggered a new container
image is created and the new application version is deployed. Multiple
upgrade processes are available, we will check them out in future
sections.</p>
<p>That is how OpenShift works at a high level we will dig much much
deeper into all of these components and mechanisms over the course of
future sections. Now that we are armed with the knowledge of OpenShift
let us talk about some of the things container platforms are good and
not so good at doing</p>
<h2 id="use-case-for-platforms">Use case for platforms</h2>
<p>The technology in OpenShift is pretty cool, but unless you can tie a
new technology to some sort of benefit to your mission it is hard to
justify investigating it, in this section, we will take a look at some
of the benefits of OpenShift can provide.</p>
<h2 id="technology-use-cases">Technology use cases</h2>
<p>If you stop and think about it for a minute, you can hand the major
innovations in IT on a timeline of people seeking more efficient process
isolation. Starting with mainframes, we were able to isolate
applications more effectively with the client-server model and the x86
revolution. That was followed by the virtualization revolution. Multiple
virtual machines can run on a single physical server. This give
administrators better density in their datacenters while still isolating
processes from each other. With virtual machines each process was
isolated in its own virtual machine. Because each virtual machine has a
full operating system and a full kernel, must have all the filesystem
required for full operating system. That also means it must be patched
managed and treated like traditional infrastructure. Containers are the
next step in this evolution. An application container holds everything
the application needs to run - the source code, the libraries, and the
configurations and information about connecting to shared data
sources.</p>
<p>What containers do not contain is equally important. Unlike virtual
machines, containers are all run on a single, shared kernel. To isolate
the application containers use components inside the kernel. Because
containers do not have to include a full kernel to serve their
application, along with all the dependencies of an operating system,
they tend to be much smaller than an equivalent virtual machine. For
example whereas a typical virtual machine starts out with a 10GB or
larger disk, the container image could be as small as 100MB. Being
smaller comes with a couple of advantages. First portability is
enhanced. Moving a 100MB from one server to another, is much easier than
doing the same for multi gigabyte images. Second because starting a
container does not include booting up an entire kernel, the startup
process is much faster. Starting a container is typically measured in
milliseconds as opposed to seconds or minutes for virtual machines.</p>
<h2 id="businesses-use-cases">Businesses use cases</h2>
<p>Modern business solutions must include time or resource savings as
part of their design. Solutions today have to be able to use human and
computer resource more efficiently than in the past. Containers ability
to enable both types of savings is one of the major reasons they have
exploded on the scene the way they have.. If you compare a server that
is using virtual machines to isolate processes to one that is using
containers to do the same thing, you will notice a few key
differences:</p>
<ul>
<li><p>Containers consume server resources more effectively. Because
there is a single shared kernel for all containers on a host, instead of
multiple virtualized kernels, in a virtual machine, more of the servers’
resources are used to serve applications instead of for platform
overhead.</p></li>
<li><p>App density increases with containers. Because the basic unit
used to deploy applications is much smaller than the unit for virtual
machines, more applications can fit per server. This means more
applications require fewer servers to run.</p></li>
</ul>
<h2 id="invalid-use-cases">Invalid use cases</h2>
<p>An ever increasing number of workloads are good fit for containers.
The container revolution started with pure web applications but now
includes command line tools, desktop tools and even relation databases.
Even with the massive growth of use cases for containers in some
situations they are not the answer. If you have a complex legacy
application, be careful when deciding to break it down and convert it to
a series of containers. If an application will be around for 18 months
and it will take 9 months of work to properly containerized it you may
want to leave it where it is. Containers solution began in the
enterprise IT world. They are designed to work with most enterprise
grade storage systems and network solutions, but they do not work with
all of them easily. Some applications are always going to be very large,
very resource intensive monolithic applications, examples are software
used to run HR departments and some very large relation databases. If a
single application will take up multiple servers on its own running it
in a container that wants to share resources with other applications on
a server does not make any sense</p>
<h2 id="container-storage">Container storage</h2>
<p>Containers are a revolutionary technology but they can not do
everything. Storage is an area where containers need to be paired with
another solution to deploy production-ready applications. This is
because the storage created when a container is deployed is ephemeral.
If a container is destroyed or replaced the storage from inside that
container is not reused. This is by design to allow containers to be
stateless by default. If something goes bad, a container can be removed
from your environment completely and new one can be stood up in its
place, instantly The idea of a stateless application container is great,
but somewhere in your application usually in multiple places data needs
to be shared across multiple containers and state needs to be preserved.
Here are some examples of these situations:</p>
<ul>
<li><p>Shared data that needs to be available across multiple
containers, like uploaded image, for a web application</p></li>
<li><p>Use state information in a complex application which lets users
pick up where they leave off during a long running transaction.</p></li>
<li><p>Information that is stored in relational or non-relational
databases</p></li>
</ul>
<p>In all of these situations, and many others, you need to have
persistent storage available to your containers. This storage should be
defined as part of your application deployment and should be available
from all the nodes in your OpenShift cluster, luckily OpenShift has
multiple ways to solve this problem. In future sections we will
configure external network storage service, you will then configure it
to interact with OpenShift so applications can dynamically allocate and
take advantage of its persistent storage volumes. When you are able to
effectively integrate shared storage into your application containers
you can think about scalability in new ways.</p>
<h2 id="scaling-applications">Scaling applications</h2>
<p>For stateless application, scaling up and down is straightforward,
because there are o dependencies other than what is in the application
container and because the transactions happening in the container are
atomic by design all you need to do to scale a stateless application is
to deploy more instance of it and load balance them together. To make
this process even easier OpenShift proxies the connection to each
application through a built in load balancer - HAProxy, This allows
applications to scale up and down with no change, in how users connect
to the application. If your application are stateful meaning they need
to store or retrieve shared data, such as a database or data that a user
has uploaded then you need to be able to provide persistent storage for
them. This storage needs to automatically scale up and down with your
application, in OpenShift. For stateful applications persistent storage
is a key component that must be tightly integrated into your design. At
the end of the day stateful pods are how users get data in and out of
your application</p>
<h2 id="integrating-stateful-and-stateless-apps">Integrating stateful
and stateless apps</h2>
<p>As you begin separating traditional monolithic apps into smaller
services that work effectively in containers you will Begin to view your
data needs in a different way. This process is often referred to as
designing apps as microservices. For any app you have services that you
need to be stateful and others that are stateless, for example the
service that provides static web content can be stateless, whereas the
service that processes user authentication needs to be able to write
information to persistent storage. These services all go together to
form your app. Because each service runs in its own container the
services can be scaled up and down independently. Instead of having to
scale up your entire codebase with containers you can only scale the
services in your app that need to process additional workloads.
Additionally because only the containers that need access to persistent
storage have it, the data going into your container is more secure. That
brings us to the end of our initial walkthrough. The benefits provided
by OpenShift save time for human and use server resources more
efficiently. Additionally the nature of how containers work provides
improved scalability and deployment speed versus virtual machines. This
all goes together to provide an incredibly powerful app platform that
you will work with for the rest of this read.</p>
<h1 id="starting">Starting</h1>
<p>There are three ways to interact with OpenShift the command line, the
web interface and the REST API. This chapter focuses on deploying apps
using the command line, because the command line exposes more of the
process that is used to create containerized app in OpenShift. In other
sections the examples may use the web interface or even the API. Our
intention is to give you the most real world examples of using
OpenShift. We want to show you the best tools to get the job done. We
will also try our best not to make you repeat yourself. Almost every
action in OpenShift can be performed using all three access methods. If
something is limited we will do our best to let you know. But we want
you to get the best experience possible from using OpenShift. With that
said in this section we are going to repeat ourselves, but for a good
reason.</p>
<p>The most common task in OpenShift is deploying an app. Because this
is the most common task we need to introduce you to it as early as
practical using both the command line and the web interface. So place
bear with us. This section may seem a little repetitive.</p>
<h2 id="cluster-runtimes">Cluster runtimes</h2>
<p>Before we can start using OpenShift you have to deploy it. There a
few options, to install OpenShift locally on your machine or on cloud
providers. We are going to stop at tools like <code>Minishift</code> or
<code>RedHat's CRC</code>.</p>
<p>Logging in OpenShift must be done, as every action requires
authentication. This allows every action to be governed by the security
and access rules set up for all users in an OpenShift cluster. We will
discuss the various methods of managing authentication in next section,
but by default your OpenShift cluster initial configuration is set to
allow any user and password. The allow all identity provider creates a
user account the first time a user logs in. Each user name is unique and
the password can be anything except an empty field. This configuration
is safe and recommended only for lab and development OpenShift instances
like the one we are setting up.</p>
<p>Using the <code>oc</code> command line tool, it is a front facing
user level program which is used to interact with the REST server of the
running OpenShift cluster, in this case the actual OpenShift server is
backed by the Kubernetes REST API server under the hood, but those are
implementation details, OpenShift builds on top of the Kubernetes REST
API server to bring us more flexibility and robustness. What you need to
remember is that you must first use the login command to perform the
login action, before any other command can be execute with the
<code>oc</code> tool</p>
<p>This is done using the following command
<code>oc login -u dev -p dev https://ocp-1.192.168.122.100.nip.io:8443</code>.
What this does is sets the user and password to <code>dev</code>, and
points the <code>oc</code> tool to a valid running cluster REST API
server, those 3 fields are mandatory otherwise the tool would not know
who to authenticate, or how and where to for that matter.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to obtain the list of configured credentials on the cluster, locally, you can use the following command, this will</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># list every user that is configured along with the password in a ready to use login command to facilitate easier login</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">crc</span> console <span class="at">--credentials</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">minishift</span> console <span class="at">--credentials</span></span></code></pre></div>
<h2 id="cluster-installation">Cluster installation</h2>
<p>To install a OpenShift cluster we have really two major options -
<code>minishift</code> and <code>crc</code>. The latter of which is the
more modern one which we will be using, this is a distribution provided
by RedHat directly, and <code>crc</code> supports more modern versions
of OpenShift, those would be any version =&gt;4.xx.</p>
<p>The runtime first needs a container runtime on your host machine,
that is usually docker, podman or containerd, one of those need to be
installed in order for you to be able to install <code>minishift</code>.
However for the <code>crc</code> runtime the process is a bit different,
while the <code>minishift</code> uses native container technologies to
mimic the OpenShift cluster runtime, the <code>crc</code> product uses
virtualization, meaning that you need to have the virtualization on your
host enabled. For any modern windows, there is nothing more required
here, as Hyper-V would be used to run the <code>crc</code> runtime,
however for linux systems, you are required to install <code>KVM</code>.
For linux systems there are a couple of more dependencies that need to
be installed in order for you to be able to run the <code>crc</code>
runtime:</p>
<ul>
<li><code>virtsh</code> - that is a general purpose virtual network
daemon, that one is used to bridge the connection between the
<code>crc</code> runtime running in the virtual machine and your
host</li>
<li><code>KVM</code> - as already stated that is virtualization platform
that is required core component to run the <code>crc</code> runtime,
once installed proceed further</li>
</ul>
<p>After having all dependencies installed on your platform simply
download the <code>crc</code> runtime from the official RedHat website,
that can be found here -
<code>https://www.redhat.com/en/blog/codeready-containers</code>. Once
you visit this page follow the instructions to setup the orchestration
runtime.</p>
<h2 id="cluster-login">Cluster login</h2>
<p>To enable ssh login into the virtual machine itself, if using the
<code>crc</code> distribution, which is running the OpenShift cluster,
add the following into your ssh config file located in
<code>~/.ssh/config</code>. This will allow you to perform a simple ssh
login command as such <code>ssh crc</code>. Also make sure that the
certificate files specified in the configuration below exist, in the
specified locations, they should for a local installation of
<code>crc</code>, but if another local OpenShift cluster distribution is
used, replace the paths to point to the locally installed certificate
credentials on your machine.</p>
<p>The <code>minishift</code> distribution, does not require any special
ssh setup, since it provides a command line option to directly login
into the cluster using <code>minishift ssh</code>. The set of examples
below will be using <code>crc</code> and the OpenShift version 4.xx
which is supported by <code>crc</code> version</p>
<p>Creating the projects can now be done using the tool. In OpenShift
projects are the fundamental way apps are organized. Projects let users
collect their apps into logical groups. They also serve other useful
roles around security that we will discuss in future sections. For now
though think of a project as a collection of related apps. You will
create your first project and then use it to house a handful of apps
that you will deploy modify, redeploy and do all sorts of things to over
the course of the next few sections.</p>
<p><code>The default project and working with multiple projects - the oc tool's default action is to execute the command you run using the current working project. If you create a new project it automatically becomes your working project. The oc project command changes the current working project from one to another. To specify a command to be execute against a specific project regardless of your current working project use the -n parameter with the project name you want the command to run against. This is a helpful option when you are writing scripts that use oc and act on multiple projects</code></p>
<p>There is a quick and dirty way to configure your local ssh client to
work with the cluster node, and allow you to very quickly login into the
node, by simply providing the name of the host, as shown below in the
ssh/config file, this will send the correct identity material form your
host machine to the cluster, and allow you to directly login into the
cluster node itself, this is very useful, as we will need to interact
with the cluster node to configure it in the future.</p>
<pre class="sshconfig"><code>Host crc
    HostName 127.0.0.1
    Port 2222
    User core
    HashKnownHosts yes
    StrictHostKeyChecking no
    IdentityFile ~/.crc/machines/crc/id_ed25519
    UserKnownHostsFile /dev/null</code></pre>
<p>The <code>minishift</code> has a direct argument that can be used on
the command line which is much simpler, no additional configuration is
required there - <code>minishift ssh</code>. This will do the same as
the configuration above, and does not need any manual intervention from
the user. You will land directly into the cluster node.</p>
<h2 id="cluster-config">Cluster config</h2>
<p>There are certain amount of configuration options to be configured
when or before starting the cluster, this is to ensure, that the cluster
is going to perform well, or even be able to start, most of the default
configurations are okay, however often enough the default resources that
<code>crc</code> or <code>minishift</code> are using for
<code>cpu</code> and memory resources are not good enough, preventing
the clusters from working well or even starting in the first place. This
is very important since the more features you enable the more resources
you will need. There is no good way to calculate the exact resources
that are going to be needed for a local deployment of these options. But
a good estimate is that at the very least at least 16 GB or memory for
<code>minishift</code> and 24 GB of memory for <code>crc</code> are
required, as far as CPU configuration goes, the minimal requirements
should start from 8 cores, for <code>minishift</code> and 10 for
<code>crc</code>. Below are example snippets to set the configuration of
the clusters globally, these require the cluster to be restarted if set
while the cluster is running.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">minishift</span> config set <span class="at">--global</span> disk-size 100GB <span class="co"># increase the base size of the node, by default that would be 30GB</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">minishift</span> config set <span class="at">--global</span> memory 16GB <span class="co"># make sure that memory resources are not constrained too much</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">minishift</span> config set <span class="at">--global</span> cpus 8 <span class="co"># enable more cpu cores to allow for better performance</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">crc</span> config set disk-size 100 <span class="co"># increase the base size of the node, by default that would be 30GB</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">crc</span> config set memory 24512 <span class="co"># make sure that memory resources are not constrained too much</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="ex">crc</span> config set cpus 10 <span class="co"># enable more cpu cores to allow for better performance</span></span></code></pre></div>
<p>There are other configuration options which will be looked into,
which enable different features on the clusters, such as monitoring
further down in the sections. There is another level of configuration
that can be enabled on the cluster, that is not actually directly
related to the cluster tool, that is the patch approach. The patch
approach enables OpenShift features that are off by default, meaning
that those, unlike the configuration options above, are not strictly
related or specific to the clustering solution we are using -
<code>crc</code> or <code>minishift</code>. Rather they are native
OpenShift features that we can turn on or off, by patching the default
<code>clusterversion</code> config object</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here is one such example, which actually removes an entry from the overrides section of the clusterversion/version</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># object, by doing that we modify the default out of the box behavior, whatever the overrides section were enabling or</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># disabling, you can run oc describe clusterversion/version to see what this object contains by default before doing</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># any modifications to it. This command is using the so called json-path, to remove the entry from the overrides array</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and more precisely the 0-th index entry from the spec.overrides[]</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc patch clusterversion/version <span class="at">--type</span><span class="op">=</span><span class="st">&#39;json&#39;</span> <span class="at">-p</span> <span class="st">&#39;[{&quot;op&quot;:&quot;remove&quot;, &quot;path&quot;:&quot;/spec/overrides/0&quot;}]&#39;</span></span></code></pre></div>
<p>In future sections you will also see that the cluster node itself,
contains several files under /etc which can be used to configure the
cluster in more direct and manual way by directly editing these files,
you will be able to modify the behavior of the cluster, directly. The
files stored under /etc in the cluster provide more flexibility but
should not be messed with too much unless you are aware of the changes
you are going to be doing.</p>
<h2 id="cluster-debugging">Cluster debugging</h2>
<p>In case you would like to ever access the OpenShift cluster API, form
the node itself, using the system user, you can do the following as
shown below.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this variable is used by the oc tool, to look for a config file to use, what this file contains is the certificate</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># material that can be used to login into the cluster, without providing any credentials, the idea is that we will be</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># logging in wit ha special user called system:node:crc</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">KUBECONFIG</span><span class="op">=</span>/var/lib/kubelet/kubeconfig</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># now running any oc command will actually run using the config above, meaning that we will be executing command in the</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># context of the system user, for crc that user is system:node:crc</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc <span class="op">&lt;</span>command<span class="op">&gt;</span></span></code></pre></div>
<p>The same can be used when working with <code>minikube</code>, however
the configuration does not live in a virtual machine just like for
<code>crc</code>, but rather the local host <code>$HOME/.kube</code>
config folder is used, therefore the file in question is under -
<code>$HOME/.kube/config</code>. Again run the export expression in your
shell, on the host, and you can use <code>oc to login</code> as a system
user. If your host is Windows, you can use SET instead, while the
general approach remains the same</p>
<p>To allow you to login as admin and have access to the default
password of the cluster root user, one can also take advantage of the
enable emergency login config, this is not always needed, but good to
know. By default you can always assume sudo rights in the host by doing
<code>sudo -i</code> in the shell. That would not require password by
default</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure to configure the cluster first to enable emergency login, this will generate a passwd file in the .crc</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># directory on your host machine, this file will contain the password to the core user in the crc cluster node, which is</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># quite useful when you wish to run some sudo enabled commands in the node, after you run this config, restart the cluster</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># after the cluster is started search for the file - $HOME/.crc/machines/crc/passwd, and fetch the password for the core user</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="ex">crc</span> config set enable-emergency-login true</span></code></pre></div>
<h2 id="cluster-software">Cluster software</h2>
<p>There are ways to install software on our cluster, using the dnf or
yum binaries, which are the package manages for the underlying linux
distribution that the OpenShift cluster node is using which is
CentOS</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh crc <span class="co"># here are few preliminary steps, first login into the cluster node</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-s</span> <span class="co"># login as root user, this will make subsequent commands easier to execute directly</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mount <span class="at">-o</span> remount,rw /usr <span class="co"># remount the /usr file system as read/write, it is read-only by default</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># modify the dnf config file to look like this, this will remove some hard guard set rules, which might interfere with</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># installing software on the cluster</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> vi /etc/dnf/dnf.conf</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> <span class="ex">[main]</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> gpgcheck=0</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> diskspacecheck=0</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> clean_requirements_on_remove=True</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> best=True</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> skip_if_unavailable=True</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> subscription-manager register <span class="co"># will prompt you to enter your RedHat credentials to register this node with RedHat</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> subscription-manager refresh <span class="co"># refresh the indexes and allow us to interact with the RedHat registries from the node</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> dnf clean all <span class="kw">&amp;&amp;</span> <span class="ex">dnf</span> repolist <span class="at">-C</span> <span class="co"># clean any left over artifacts, and also update the repository list of dnf manager</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> dnf <span class="at">-y</span> install httpd httpd-tools iptables-services <span class="dt">\</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        nfs-utils go gcc python3 python-pip <span class="at">--nogpgcheck</span> <span class="co"># install some utility applications to the master cluster node</span></span></code></pre></div>
<h2 id="first-project">First project</h2>
<p>To create a project you need to run the <code>oc new-project</code>
command and provide a project name. For the first project use
<code>image-uploader</code> as the project name</p>
<p>As already mentioned the project term in OpenShift is actually what
Kubernetes refers to as the so called namespaces, the OpenShift Projects
are a way to separate different application contexts more effectively
and also allow for a more general permission control over these
contexts, by restricting which namespace or a project a user has access
to easily</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to create a new project just run the following, that would immediately change the context to that project, as well</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-project image-uploader <span class="at">--display-name</span><span class="op">=</span><span class="st">&#39;Image Uploader Project&#39;</span></span></code></pre></div>
<h2 id="application-components">Application Components</h2>
<p>Apps in OpenShift are not monolithic structures, they consist of a
number of different components in a project that will work together to
deploy update and maintain your app through its lifecycle. Those
components are as follows</p>
<ul>
<li>Container images</li>
<li>Image streams</li>
<li>App pods</li>
<li>Build configs</li>
<li>Deployment configs</li>
<li>Deployments</li>
<li>Services</li>
</ul>
<h3 id="container-images">Container images</h3>
<p>Each app deployment in OpenShift creates a custom container image to
serve your app. This image is created using the app source code and
custom base image called builder image. For example the PHP builder
image contains the Apache web server and the core PHP language
libraries. The image build process takes the builder image you choose
integrates your source code and creates the custom container image that
will be used for the app deployment. Once created all the container
images along with all the builder images are stored in OpenShift
integrated container registry which we discussed in first section. The
component that controls the creation of your app containers is the build
config.</p>
<h3 id="build-configs">Build configs</h3>
<p>A build config contains all the information needed to build an app
using its source code. This includes all the information required to
build the app container image.</p>
<ul>
<li>URL for the app source code</li>
<li>Name of the builder image to use</li>
<li>Name of the app container image that is created</li>
<li>Events that can trigger a new build to occur</li>
</ul>
<p>After the build config does its job, it triggers the deployment
config that is created for your newly created app.</p>
<h3 id="deployment-configs">Deployment configs</h3>
<p>If an app is never deployed it can never do its job. The job of
deploying and upgrading the app is handled by the deployment config
component. Deployment configs track several pieces of information about
an app.</p>
<ul>
<li><p>Currently deployed version of the app</p></li>
<li><p>Number of replicas to maintain for the app</p></li>
<li><p>Trigger events that can trigger a redeployment. By default
configuration changes to the deployment or changes to the container
image trigger an automatic app redeployment</p></li>
<li><p>Upgrade strategy app-cli uses the default rolling upgrade
strategy</p></li>
<li><p>App deployments</p></li>
</ul>
<p>A key feature of app running in OpenShift is that they are
horizontally scalable. This concept is represented in the deployment
config by the number of replicas. The number of replicas specified in a
deployment config is passed into a Kubernetes object called a
replication controller. This is a special type of Kubernetes pod that
allows for multiple replicas - copies of the app pod to be kept running
at all time. All pods in OpenShift are deployed by replication
controllers by default. Another feature that is managed by a deployment
config is how apps upgrades can be fully automated. Each deployment for
an app is monitored and available to the deployment config component
using deployments.</p>
<p>In OpenShift a pod can exist in one of five phases at any given time
in its lifecycle. These phases are described in detail in the Kubernetes
Documentation. The following is a brief summary of the five pod
phases.</p>
<ul>
<li><code>Pending</code> - the pod has been accepted by OpenShift but
its is not yet schedule on one of the app nodes.</li>
<li><code>Running</code> - the pod is scheduled on a node and is
confirmed to be up and running.</li>
<li><code>Succeeded</code> - all containers in a pod have terminated
successfully and wont be restarted</li>
<li><code>Unknown</code> - something has gone wrong and OpenShift can
not obtain a more accurate status for the pod.</li>
</ul>
<p>Failed and Succeeded are considered terminal states for a pod in its
lifecycle. Once a pod reaches one of these states it wont be restarted.
You can see the current phase for each pod in a project by running the
<code>oc get pods</code> command Pod lifecycle will become important
when you begin creating project quotas.</p>
<p>Each time a new version of an app is created by its build config, a
new deployment is created and tracked by the deployment config. A
deployment represents a unique version of an app. Each deployment
references a version of the app image that was created and creates the
replication controller to create and maintain the pod to serve the app.
New deployments can be created automatically in OpenShift by managing
how apps are upgraded which is also tracked by the deployment
config.</p>
<p>The default app upgrade method in OpenShift is to perform a rolling
upgrade rolling upgrades create new versions of an app allowing new
connections to the app to access only the new version. As traffic
increases to the new deployment the pods for the old deployment are
removed from the system.</p>
<p>New app deployments can be automatically triggered by events such as
configuration changes to your app, or a new version of a container image
being available. These sorts of trigger events are monitored by image
streams in OpenShift.</p>
<h3 id="image-stream">Image stream</h3>
<p>Image stream are used to automate actions in OpenShift. The consist
of links to one or more container images. Using image streams you can
monitor apps and trigger new deployments when their components are
updated.</p>
<h2 id="deploying-an-app">Deploying an app</h2>
<p>Apps are deployed using the <code>oc new-app</code> command. When you
run this command to deploy the <code>image uploader</code> app, into the
<code>image-uploader</code> project. You need to provide three prices of
information.</p>
<ul>
<li><p>The type of the image stream you want to use - OpenShift ships
with multiple container images called builder images, that you can use
as a starting point for apps.</p></li>
<li><p>A name for your app - in this example use app-cli because this
version of your app will be deployed from the command line.</p></li>
<li><p>The location of your app source code - OpenShift will take the
source code and combine it with the PHP builder image to create a custom
container image for your app deployment</p></li>
</ul>
<div class="sourceCode" id="cb10"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here is a new app deployment from a github hosting</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-app <span class="at">--image-stream</span><span class="op">=</span>php <span class="at">--code</span><span class="op">=</span>https://github.com/OpenShiftInAction/image-uploader.git <span class="at">--name</span><span class="op">=</span>app-cli</span></code></pre></div>
<p>After you run the <code>oc new-app</code> command you will see a long
list of output, as shown above. This is OpenShift building the image out
of all the components needed to make your app work properly. Now if you
visit the web console you will be able to browse the project structure
as well, and you will also see that there are quite a few new objects
created for the new app in the new project. With the triggering of new
<code>oc new-app</code> command various new objects are created in the
cluster for the current project, this is usually not the way we would do
this in the real world, each of those new objects will be manually
defined in manifest files by the developers where the properties of
these objects can be fine tuned. But for the sake of demonstration and
ease of use OpenShift provides the users with quick and dirty ways to
deploy apps, this is very useful for development purposes where we just
want to put our app into use, and avoid the hassle of manual
configuration of OpenShift objects.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># list some of the objects which would have been automatically created by the OpenShift environment</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get services <span class="kw">&amp;&amp;</span> <span class="ex">oc</span> get pods <span class="kw">&amp;&amp;</span> <span class="ex">oc</span> get deployments</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># here is what the output might look like, there are multiple pods that were created, you may notice that there was a</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># pod called build - which is basically spun up to build the image from the source, and then it is uploading that</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># image into the internal OpenShift registry # and deployed as an actual pod which is in Running state. There is</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># also a new service created for our app.</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>            TYPE      CLUSTER-IP   EXTERNAL-IP PORT<span class="er">(</span><span class="ex">S</span><span class="kw">)</span>           <span class="ex">AGE</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli</span>         ClusterIP 172.30.51.80 <span class="op">&lt;</span>none<span class="op">&gt;</span>      8080/TCP,8443/TCP 55s</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>            READY     STATUS       RESTARTS    AGE</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-1-build</span> 0/1       Completed    0           54s</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-1-vsk5q</span> 1/1       Running      0           12s</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>            REVISION  DESIRED      CURRENT     TRIGGERED BY</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli</span>         1         1            1           config,image<span class="er">(</span><span class="ex">app-cli:latest</span><span class="kw">)</span></span></code></pre></div>
<h2 id="providing-access-to-apps">Providing access to apps</h2>
<p>In future sections we will explore multiple ways to force OpenShift
to redeploy app pods. In the course of a normal day this happens all the
time, for any number of reasons, you are scaling apps up and down, apps
pods stop responding correctly, nodes are rebooted or have issues, human
error, and so on. Although pods may come and go there needs to be a
consistent presence for your app in OpenShift. That is what a service
does. A service uses labels applied to application pods when they are
created to keep track of all pods associated with a given app. This
allows a service to act as an internal proxy for your app. You can see
information about the service for app-cli by running the
<code>oc describe svc/app-cli command</code>.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe svc/app-cli</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                     app-cli</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>                image-uploader</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>                   app=app-cli</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                          <span class="ex">app.kubernetes.io/component=app-cli</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                          <span class="ex">app.kubernetes.io/instance=app-cli</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>                          <span class="ex">app.kubernetes.io/name=php</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>              openshift.io/generated-by: OpenShiftNewApp</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Selector:</span>                 deployment=app-cli</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Type:</span>                     ClusterIP</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="ex">IP</span> Family Policy:         SingleStack</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="ex">IP</span> Families:              IPv4</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="ex">IP:</span>                       10.217.4.162</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="ex">IPs:</span>                      10.217.4.162</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="ex">Port:</span>                     8080-tcp  8080/TCP</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="ex">TargetPort:</span>               8080/TCP</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Endpoints:</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="ex">Port:</span>                     8443-tcp  8443/TCP</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="ex">TargetPort:</span>               8443/TCP</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="ex">Endpoints:</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="ex">Session</span> Affinity:         None</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="ex">Internal</span> Traffic Policy:  Cluster</span></code></pre></div>
<p>Now note that there are the fields which are IP addresses, these are
the IP addresses that each service gets, these are the cluster virtual
IP addresses that are only routable within the OpenShift cluster. Other
information that is maintained includes the IP address of the service
and the TCP ports to connect to the in the pod.</p>
<p><code>Most components in OpenShift have a shorthand that can be used on the command line to save time and avoid misspelled components names. The previous command uses svc/app-cli to get information about the service for the app-cli app. Build configs can be accessed with the bc/&lt;app-name&gt;</code></p>
<p>Services provide a consistent gateway into your app deployment, but
the IP addresses of a service is available only in your OpenShift
cluster, to connect users to your app and make DNS work properly you
need one or more app components Next you will create a route to expose
app-cli externally from your OpenShift cluster</p>
<h2 id="exposing-application-services">Exposing application
services</h2>
<p>When you install your OpenShift cluster, one of the services that is
created is the HAProxy service running in a container on OpenShift, the
HAProxy is an open source software load balancer, we will look at this
service in depth in next sections,. To create a route for the app-cli
run the following command -</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc expose svc/app-cli</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="ex">route.route.openshift.io/app-cli</span> exposed</span></code></pre></div>
<p>As we discussed earlier, OpenShift uses projects to organize
applications. An application project is included in the URL that is
generated when you create an application route. Each application UR
takes the following format -
<code>&lt;application-name&gt;-&lt;project-name&gt;-.&lt;cluster-app.domain&gt;</code>.
This is actually the default format, coming from kubernetes, but
slightly modified by OpenShift to include the name of the project, in
Kubernetes in place of the project name in that format is actually the
namespace for the app, in OpenShift the projects are actually
implemented internally as kubernetes namespaces, but very much
enhanced.</p>
<p>When you did deploy OpenShift in previous sections you specified the
application domain, by default all application in OpenShift are served
using the HTTP protocol when you pull all this together the URL for
app-cli should be as follows -
<code>http://app-cli-image-uploader.&lt;cluster-app.domain&gt;</code>.
You can get more information about the route you just created by running
the <code>oc describe route/app-cli</code> command</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe route/app-cli</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                   app-cli</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>              image-uploader</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Created:</span>                <span class="op">&lt;</span>time<span class="op">&gt;</span> ago</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>                 app=app-cli</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                        <span class="ex">app.kubernetes.io/component=app-cli</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>                        <span class="ex">app.kubernetes.io/instance=app-cli</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>                        <span class="ex">app.kubernetes.io/name=php</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>            openshift.io/host.generated=true</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Requested</span> Host:         app-cli-image-uploader.apps-crc.testing</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>                           <span class="ex">exposed</span> on router default <span class="er">(</span><span class="ex">host</span> router-default.apps-crc.testing<span class="kw">)</span> <span class="op">&lt;</span>time<span class="op">&gt;</span> ago</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Path:</span>                   <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="ex">TLS</span> Termination:        <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Insecure</span> Policy:        <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="ex">Endpoint</span> Port:          8080-tcp</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Service:</span>        app-cli</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="ex">Weight:</span>         100 <span class="er">(</span><span class="ex">100%</span><span class="kw">)</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="ex">Endpoints:</span>      <span class="op">&lt;</span>none<span class="op">&gt;</span></span></code></pre></div>
<p>The output tells you that the host configuration added to the HAProxy
the service associated with the route and the endpoints for the service,
to connect to when handling requests for the route, how that we have
created the route to you application go ahead and verify that by
visiting the IP address in a web browser.</p>
<p>Focusing on the component that deploy and deliver the app-cli
application, you can see the relationship between the service, the newly
created route, and the end users. We will cover this in more depth in
next sections, but in summary the route is tied to the app-cli service,
users access the application pod through the route. This chapter is
about relationships. In OpenShift multiple components work in concert to
build deploy and manage application. We spend the rest of the discussing
the different aspects of these relationships in depth, that fundamental
knowledge of how container platforms operate is incredibly valuable.</p>
<h1 id="containers">Containers</h1>
<p>In the previous sections you deployed your first app, in OpenShift in
this chapter we will look deeper into your OpenShift cluster and
investigate how these containers isolate their processes on the
application node. Knowledge of how containers work in a platform like
OpenShift is some of the most powerful information in IT right now. This
fundamental understanding of how a container actually works as part of
the Linux kernel and server informs how systems are designed and how
issues are analyzed when they inevitable occur. This is a challenging
section, not because of a lot of configuration and making complex
changes, but because we are talking about the fundamental layers of
abstractions that make a container a container in the modern kernel
world.</p>
<h2 id="defining-containers">Defining containers</h2>
<p>You can find five different container experts and ask them to define
what a container is and you are likely to get five different answers,
the following are some our personal favorites all of which are correct
from a certain perspective.</p>
<ul>
<li>A transportable unit to move apps around. This is a typical
developer answer</li>
<li>A fancy linux process</li>
<li>A more effective way to isolate processes on a linux system, this is
a more operations centered answer.</li>
</ul>
<p>What we need to untangle is the fact that they are all correct,
depending on your point of view. In section 1, we talked about how
OpenShift uses Kubernetes and docker to orchestrate and deploy apps in a
container in your cluster. But we have not talked much about which
application component is created by each of these services, before we
move forward it is important for you to understand these
responsibilities as you begin interacting with application components
directly.</p>
<h2 id="openshift-component-interaction">OpenShift component
interaction</h2>
<p>When you deploy and application in OpenShift the request starts in
the OpenShift API server. To really understand how containers isolate
the process within them we need to take a more detailed look at how
these services work together to deploy your application. The
relationship between OpenShift Kubernetes docker and ultimately the
Linux kernel is a chain of dependencies. When you deploy an application
in OpenShift the process starts with the OpenShift services.</p>
<h2 id="openshift-manages-deployments">OpenShift manages
deployments</h2>
<p>Deploying application begin with application components that are
unique to OpenShift the process is as follows:</p>
<ol type="1">
<li><p>OpenShift creates a custom container image using your source code
and the builder image template you specified.</p></li>
<li><p>This image is uploaded to the OpenShift container image
registry</p></li>
<li><p>OpenShift creates a build config to document how your – is built.
This includes which image was created the builder image used the
location of the source code and other information.</p></li>
<li><p>OpenShift creates a deployment config to control deployments and
deploy and update your application. Information in deployment configs
includes the number replicas the upgrade method, and application
specific variables and mounted volumes.</p></li>
<li><p>OpenShift creates a deployment which represents a single deployed
version of an application. Each unique application deployment is
associated with your application deployment config component.</p></li>
<li><p>The OpenShift internal load balancer is updated with an entry for
the DNS record for the application. This entry will be linked to a
component that is created by Kubernetes which we will get to
shortly.</p></li>
<li><p>OpenShift creates an image stream component, in OpenShift an
image stream monitors the builder image, deployment config, and other
components for changes, if a change is detected image streams can
trigger application re-deployments to reflect changes</p></li>
</ol>
<p>The build config creates an application specific custom container
image using the specified builder image and source code, that image is
stored in the OpenShift image registry. The deployment config component
creates an application deployment that is unique for each version of the
app. The image stream is created and monitors for changing to the
deployment config and related images in the internal registry. The DNS
route is also created and will be linked to the Kubernetes object</p>
<h2 id="kubernetes-schedules-applications">Kubernetes schedules
applications</h2>
<p>Kubernetes is the orchestration engine, at the heart of OpenShift, in
many ways an OpenShift cluster is a kubernetes cluster. When you
initially deploy app-cli, Kubernetes created several application
components.</p>
<ul>
<li><p>Replication controller - scales the application as needed in
Kubernetes. This component also ensures that the desired number of
replicas in the deployment config is maintained at all times.</p></li>
<li><p>Service - Exposes the application . A kubernetes service is a
single IP address that is used to access all the active pods for an
application deployment. When you scale an application up or down the
number of pods changes, but they are all accessed through a single
service proxy object.</p></li>
<li><p>Pods - represent the smallest scalable unit in
OpenShift.</p></li>
</ul>
<p>The replication controller dictates how many pods are created for an
initial application deployment is linked to the OpenShift deployment
component. Also linked to the pod components is a Kubernetes service.
The service represents all the pods deployed by a replication
controller. It provides a single IP address in OpenShift to access your
application as it scaled up and down on different nodes in your cluster.
The service in the internal IP address that is referenced in the route
created in the OpenShift load balancer.</p>
<h2 id="docker-creates-containers">Docker creates containers</h2>
<p>Docker is a container runtime. A container runtime is the application
on a server that creates, maintains and removes containers. A container
runtime can act as a stand alone tool on a laptop or a single server,
but it is at its most powerful when being orchestrated across a cluster
by a tool like kubernetes. Kubernetes controls docker to create
containers that house the app. These containers use the custom base
image as the starting point for the files that are visible to
application in the container. Finally the docker container is associated
with the Kubernetes pod. To isolate the libraries and application in the
container image along with other server resources docker uses Linux
kernel components. These kernel level resources are the components that
isolate the application in your container from everything else on the
application node. Let us check them out</p>
<h2 id="linux-isolates-resources">Linux isolates resources</h2>
<p>We are down to the core of what makes a container a container in
OpenShift , and Linux. Docker uses three Linux kernel components to
isolate the application running in containers it creates and limit their
access to resources on the host machine or cluster node in our case.</p>
<ul>
<li><p>Linux namespaces - provide isolation for the resources running in
the container. Although the term is the same this is a different concept
than Kubernetes namespaces, which are roughly analogous to n OpenShift
project. We will discuss these in more depth in next sections. For the
sake of brevity in this section when we reference namespaces, we are
talking about Linux namespaces</p></li>
<li><p>Control groups - Provide maximum guaranteed access to limits for
CPU and memory on the app node, or cluster node.</p></li>
<li><p>SELinux contexts - prevents the container application from
improperly accessing resources on the host or in other containers. An
SELinux context is a unique label that is applied to a container
resources on the application node. This unique label prevents the
container from accessing anything that does not have a matching label on
the host.</p></li>
</ul>
<p>The docker daemon creates these kernel resources dynamically when the
container is created, these resources are associated with the
application that are launched for the corresponding container your
application is now running in a container. Apps in OpenShift are run and
associated with these kernel components they provide the isolation that
you see from inside a container in upcoming sections, we will discus how
you can investigate a container from the application node. From the
point of view of being able inside the container, an application only
has the resources allowed and allocated to it.</p>
<h2 id="working-with-cluster">Working with cluster</h2>
<p>To first make sure we can extract the resources from the cluster, we
have to be able to login into the cluster, what that means, is that we
would like to be able to interact with the virtual machine that is being
created locally when you stood up your OpenShift cluster. This means we
have to login into the cluster, this is done by using ssh and the
following command template -
<code>ssh -i &lt;path-to-private-key&gt; -o &lt;options&gt; -p 2222  core@127.0.0.1</code>.
The ssh command here is setting up a secure connection to the cluster,
by making sure it is using a valid private key that the cluster trusts,
and the port and host are by default 2222, and 127.0.0.1 for the local
cluster, however the same ssh command can be used to login into a
provided cluster. As long as you have a locally setup private key which
is trusted by the cluster’s ssh agent</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will allow you to login into the crc virtual machine, which represents the actual single cluster node that is</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># being simulated locally, refer to the beginning of this document to setup permanent ssh host config for crc</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh <span class="at">-i</span> ~/.crc/machines/crc/id_ed25519 <span class="at">-o</span> StrictHostKeyChecking=no <span class="at">-o</span> UserKnownHostsFile=/dev/null <span class="at">-p</span> 2222 core@127.0.0.1</span></code></pre></div>
<p>To extract the process id of the running container we have to do a
few more things, first we can list all running processes on the cluster
by doing the following command</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># that will list all pods along side their ids, names, and further provide more information about the running pods, the</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># container and image information and more</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo crictl ps <span class="kw">|</span> <span class="fu">head</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># this is a sample of the data that you might see from the crictl ps above, we have to look for the app-cli container</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># information from this output, and get the container id of that row in the table, note that the table is abridged</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># version, more columns actually are provided by the output of crictl, like pod-id, creation date and other...</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="ex">CONTAINER</span>       IMAGE</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="ex">67fb60f6d592d</span>   quay.io/crcont/routes-controller@sha256:9a66245c7669a8741da0db9be13cf565548b0fa93ca97ae1db6b8400d726aa71</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="ex">7c2083341d04d</span>   image-registry.openshift-image-registry.svc:5000/image-uploader/app-cli@sha256:85e47f7e1eefedd6aa1c09c494fbe98eeefe22aa8945ce7665568ee3175a74e6</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="ex">06e7b456c7f39</span>   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c34a7d4a5ba7d78debe6b3961498a19e7c2416b2a8a2c10e5086a02934b4e956</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="ex">c57498c3c0319</span>   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c34a7d4a5ba7d78debe6b3961498a19e7c2416b2a8a2c10e5086a02934b4e956</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="ex">290e91fe31ae4</span>   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c34a7d4a5ba7d78debe6b3961498a19e7c2416b2a8a2c10e5086a02934b4e956</span></code></pre></div>
<p>Now that we can see the container is of the app-cli pod/container we
can use that to inspect it and obtain the PID of it, the command below
will show the details of the container, we would like to find the PID
which must be under the “info” property</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the container id, in this case this id corresponds to the app-cli, see the table above, would provide us with</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a super details spec output/dump of the container&#39;s definition</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo crictl inspect 7c2083341d04d</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># here is the output of the inspect, the output is again abridged, it is quite big, but we care about the first few</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># rows, most notably, the PID property, which we can see here, is 7825</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">{</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;info&quot;</span><span class="ex">:</span> {</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;checkpointedAt&quot;</span><span class="ex">:</span> <span class="st">&quot;0001-01-01T00:00:00Z&quot;</span>,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;pid&quot;</span><span class="ex">:</span> 7825,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;privileged&quot;</span><span class="ex">:</span> false,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;restored&quot;</span><span class="ex">:</span> false,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;runtimeSpec&quot;</span><span class="ex">:</span> { .... }</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">}</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">....</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="er">}</span></span></code></pre></div>
<h2 id="listing-kernel-components">Listing kernel components</h2>
<p>Armed with the process id of the current app-cli we can begin to
analyze how containers isolate process resources with Linux namespaces.
Earlier in this section we discussed how kernel namespaces are used to
isolate the application in a container from the other processes on the
host. Docker creates a unique set of namespaces to isolate the resources
in each container looking again the application is linked to the
namespaces because they are unique for each container. Cgroups and
SELinux are both configured to include information for a newly created
container but those kernel resources are shared among all containers
running on the application node. To get a list of the namespaces that
were created for the app-cli use the <code>lsns</code> command. You need
the POD for the application to pass as a parameter to
<code>lsns</code>.</p>
<p>OpenShift uses the five linux namespaces to isolate processes and
resources on application nodes. Coming up with a concise definition for
exactly what a namespace does is a little difficult, two analogies best
describe their most important properties</p>
<ul>
<li>Namespaces are like paper walls in the linux kernel, they are
lightweight and easy to stand up and tear down, but they offer
sufficient privacy when they are in place.</li>
<li>Namespaces are similar to two way mirrors, from within the container
only the resources in the namespace are available but with proper
tooling you can see what is in a namespace from the host system.</li>
</ul>
<p>The following snippet lists all namespaces for the app-cli with
<code>lsns</code>. The command requires the process id first, that means
that we have to make sure that the process for the correct running
container is extracted first, this is described above in detail, but in
summary it has to be done by first logging into the cluster</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here we can directly list the namespaces for the target process id, in our case we can see that this process id was</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># extracted from the cluster, using the crictl command above, where we inspected the spec of the container</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo lsns <span class="at">-p</span> 7825</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="ex">NS</span>         TYPE   NPROCS     PID USER       COMMAND</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531834</span> time      424       1 root       /usr/lib/systemd/systemd <span class="at">--switched-root</span> <span class="at">--system</span> <span class="at">--deserialize</span> 28</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531837</span> user      424       1 root       /usr/lib/systemd/systemd <span class="at">--switched-root</span> <span class="at">--system</span> <span class="at">--deserialize</span> 28</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="ex">4026534705</span> uts        13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="ex">4026534706</span> ipc        13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535152</span> net        13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535628</span> mnt        13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535698</span> pid        13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535703</span> cgroup     13    7825 1000660000 httpd <span class="at">-D</span> FOREGROUND</span></code></pre></div>
<p>And from the output above, we can clearly see that the process id
which we extracted for the pod and by proxy the container app-cli, which
was <code>7825</code> that there are multiple namespaces attached to
this process, the type column signifies that there are mount, cgroup and
net, along with <code>UTS</code>, and other namespaces created for the
process. The five namespaces that OpenShift uses to isolate the apps are
as follows:</p>
<ul>
<li>Mount - ensures that only the correct content is available to apps
in the container</li>
<li>Network - gives each container its own isolated network stack</li>
<li>PID - provides each container with its own set of PID counters</li>
<li>IPC - provides shared memory isolation for each container</li>
<li>UTS - gives each container its own hostname and domain name</li>
</ul>
<p>There are currently two additional namespaces in the Linux kernel
that are not used by OpenShift</p>
<ul>
<li><p>Cgroup - are used as shared resource on the OpenShift node, so
this namespace is not required for effective isolation.</p></li>
<li><p>User - this namespace can map a user in a container to a
different user on the host, for example a user with ID 0 in the
container could have user ID 5000, when interacting with resources on
the host. This feature can be enabled in OpenShift but there are issues
with performance and node configuration that fall out of scope for our
example cluster, if you like more information on enabling the user
namespace to work with docker and thus with OpenShift see the article
<code>Hardening Docker Hosts with User Namespaces - by Chris Binnie</code></p></li>
</ul>
<h3 id="mount-namespace">Mount namespace</h3>
<p>The mount namespace isolated file system content, ensuring that
content assigned to the container by OpenShift is the only content
available to the process, running in the container, the mount namespace
for the app-cli container allows the app in the container to access only
the content in the custom app-cli container image, and any information
stored on the persistent volume associated with the persistent volume
claim (PVC) for the app-cli.</p>
<p><code>Apps always need persistent storage, persistent storage allows data to persist when a pod is removed from the cluster, it also allows data to be shared between multiple pods when needed, you will learn how to configure and use persistent storage on an NFS server with OpenShift in future section</code></p>
<p>The root file system based on the app-cli container image is a little
more difficult to uncover, but we will do that next. When you configured
OpenShift you specified a block device for docker to use for container
storage. Your OpenShift configuration uses logical volume management on
this device for container storage. Each container gets its own logical
volume when it is created. This storage solution is fast and scales well
for large production clusters. To view all logical volumes created by
docker on your host, run the <code>lsblk</code> command. This command
shows all block devices on your host, as well as any logical volumes. It
confirms that docker has been creating logical volumes for
containers</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># enter sudo mode, with the core user, to give us more access and command execution permissions</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-i</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># list block devices, that lists information about all available or the specified block devices on the system, the</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># command reads the sysfs file system and udev database to gather this information, by default print all block devices</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># except any RAM disks, in a tree list format.</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> lsblk</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="ex">vda</span>    252:0    0   31G  0 disk</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda1</span> 252:1    0    1M  0 part</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda2</span> 252:2    0  127M  0 part</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda3</span> 252:3    0  384M  0 part /boot</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># to list all the active pods, the output is abridged, but we can see what we care interested in, this is the app-cli</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># pods, which are visible and list-able from the command below, we can see that the ps command shows the id of the</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co"># container, along with the pod it is connected to, this way we can inspect the container, by id</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl ps</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="ex">CONTAINER</span>     IMAGE                                                                    CREATED         STATE   NAME            ATTEMPT ID            POD</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="ex">3b014dbd532c3</span> quay.io/openshift-release-dev/ocp-v4.0-art-dev@                          11 minutes ago  Running registry-server 0       e5017350a2c71 redhat-operators-dtxxg</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="ex">1ea26899fa493</span> image-registry.openshift-image-registry.svc:5000/image-uploader/app-cli@ 15 minutes ago  Running app-cli         0       d4c372b145163 app-cli-5fdd99b58d-dplv6</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># after we can inspect the container, we can see from the inspect output, that there are a few mount targets, a few are</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co"># generic ones such as the /etc/hosts, however we can see that there is one which is specific</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl inspect <span class="at">-f</span> <span class="st">&#39;{{ .GraphDriver.Data.DeviceName }}&#39;</span> 1ea26899fa493</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="co"># here we can see two important things, first is the process id of the container, the PID 1, which we have already seen</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="co"># once before, we can use this process id to inspect the state of the process from the host, it is a unique process which</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co"># directly ties all information on the host to the container, the host does not see pod id, or container id, it sees the</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co"># process id</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;info&quot;</span><span class="ex">:</span> {</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;checkpointedAt&quot;</span><span class="ex">:</span> <span class="st">&quot;0001-01-01T00:00:00Z&quot;</span>,</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;pid&quot;</span><span class="ex">:</span> 33398,</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;privileged&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;restored&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a><span class="co"># this section shows us how and where part of the root file system is mounted, the root file system of a container is</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a><span class="co"># built on the basis of layers, but, certain parts of it may be mounted to completely different parts outside of the</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="co"># container, in a way these are very similar to shared directories, on a network. These mount points refer to hostPath and</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="co"># containerPath, which are very much self explanatory</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="ex">....</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;mounts&quot;</span><span class="ex">:</span> [</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>      <span class="kw">{</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;containerPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/etc/hosts&quot;</span>,</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;gidMappings&quot;</span><span class="ex">:</span> [],</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;hostPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/var/lib/kubelet/pods/85c76562-91f8-4857-a075-42f7089b23e6/etc-hosts&quot;</span>,</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;propagation&quot;</span><span class="ex">:</span> <span class="st">&quot;PROPAGATION_PRIVATE&quot;</span>,</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;readonly&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;recursiveReadOnly&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;selinuxRelabel&quot;</span><span class="ex">:</span> true,</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;uidMappings&quot;</span><span class="ex">:</span> []</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>      <span class="ex">},</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>      <span class="kw">{</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;containerPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/dev/termination-log&quot;</span>,</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;gidMappings&quot;</span><span class="ex">:</span> [],</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;hostPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/var/lib/kubelet/pods/85c76562-91f8-4857-a075-42f7089b23e6/containers/app-cli/7e8ccd6b&quot;</span>,</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;propagation&quot;</span><span class="ex">:</span> <span class="st">&quot;PROPAGATION_PRIVATE&quot;</span>,</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;readonly&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;recursiveReadOnly&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;selinuxRelabel&quot;</span><span class="ex">:</span> true,</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;uidMappings&quot;</span><span class="ex">:</span> []</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>      <span class="ex">},</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>      <span class="kw">{</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;containerPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;</span>,</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;gidMappings&quot;</span><span class="ex">:</span> [],</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;hostPath&quot;</span><span class="ex">:</span> <span class="st">&quot;/var/lib/kubelet/pods/85c76562-91f8-4857-a075-42f7089b23e6/volumes/kubernetes.io~projected/kube-api-access-f7lcd&quot;</span>,</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;propagation&quot;</span><span class="ex">:</span> <span class="st">&quot;PROPAGATION_PRIVATE&quot;</span>,</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;readonly&quot;</span><span class="ex">:</span> true,</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;recursiveReadOnly&quot;</span><span class="ex">:</span> false,</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;selinuxRelabel&quot;</span><span class="ex">:</span> true,</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;uidMappings&quot;</span><span class="ex">:</span> []</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>      <span class="kw">}</span></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>    <span class="ex">],</span></span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a><span class="co"># here having the PID we can list the namespaces of the container process for our application, and we can also see the</span></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="co"># multitude of different namespaces that it has, we want to see and inspect the mount namespace</span></span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> lsns <span class="at">-p</span> 33398</span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>        <span class="ex">NS</span> TYPE   NPROCS   PID USER       COMMAND</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531834</span> time      524     1 root       /usr/lib/systemd/systemd <span class="at">--switched-root</span> <span class="at">--system</span> <span class="at">--deserialize</span> 28</span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531837</span> user      523     1 root       /usr/lib/systemd/systemd <span class="at">--switched-root</span> <span class="at">--system</span> <span class="at">--deserialize</span> 28</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535352</span> mnt        13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535353</span> pid        13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535354</span> cgroup     13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535522</span> uts        13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535523</span> ipc        13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a><span class="ex">4026535524</span> net        13 33398 1000660000 httpd <span class="at">-D</span> FOREGROUND</span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a><span class="co"># we can now enter the mount namespace of the container, note that if you now execute ls / you will directly see the</span></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a><span class="co"># contents of the container itself, be able to access the root file system of the container</span></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> nsenter <span class="at">--mount</span> <span class="at">--target</span> 33398 /bin/sh</span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a><span class="co"># to exit the mount namespace, simply use exit</span></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> exit</span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a><span class="co"># now let us see where this is mounted on the host, we can use the process id we have obtained from above, to see</span></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a><span class="co"># where the root file system is mounted, on the host, or in this case the host is the cluster node itself, take a note</span></span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a><span class="co"># of the output and the overlay paths, these reference paths on the host root file system, placed in the /var directory</span></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a><span class="fu">grep</span> <span class="at">-w</span> <span class="st">&quot;/&quot;</span> /proc/33398/mountinfo</span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a><span class="ex">25787</span> 13562 0:853 / / rw,relatime <span class="at">-</span> overlay overlay rw,context=<span class="st">&quot;system_u:object_r:container_file_t:s0:c5,c26&quot;</span>,lowerdir=/var/lib/containers/storage/overlay/l/K565QXNBE4UPBQO2PDIBJBW5U5:/var/lib/containers/storage/overlay/l/QXL4VNC6IZOL</span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a><span class="ex">VYH23JQKKXAMIE:/var/lib/containers/storage/overlay/l/57MMERXGQHPVYCAGGCVVRXUE7F:/var/lib/containers/storage/overlay/l/HITMXQH3G2NMN7M5U4GT2XIYE5:/var/lib/containers/storage/overlay/l/4H26VNEWY45JGAAVUESP7J4CKD,upperdir=/var/lib/contai</span></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a><span class="ex">ners/storage/overlay/f2d783dad2cc2779ef613259e008670c46fa1252439afff3fa10ee8cbde00567/diff,workdir=/var/lib/containers/storage/overlay/f2d783dad2cc2779ef613259e008670c46fa1252439afff3fa10ee8cbde00567/work,volatile</span></span></code></pre></div>
<p>What are these layers, above, referring to. The file system in a
container runtime, is made up of layers, meaning that it is not one big
chunk of data, but rather, it is layered, for example the base image
itself is one layer, but any subsequent changes to the file system in
the container will generate a new layer which will stack on top of the
previous parent layer. For example adding a new file, creating
directories, copying or removing data from the container, will add
layers on top of each other. This is similar to how source control
systems work, where we have commits, which are based on other commits,
and if you follow the commits you can go between different states of the
repository, here the idea is very much the same, the container’s file
system is layered in a very similar fashion, you can think of layers
being like commits in a source control system</p>
<p>Above we have shown how we can actually enter the mount namespace,
detect where the root file system of a container is mounted on the host,
and even explore and operate within the mount namespace, effectively
allowing us to modify or work with the root file system with the
container, all things that are usually hidden behind commands such as
<code>exec</code>. Under the hood, exec does the same thing, it enters
the namespace of the process allowing you to execute processes in the
context of the container’s namespace</p>
<p>Each image is build from read-only layers, or also called Copy on
Write. And it is working similarly to source control systems as Git.
Here is how it works, each image is built from read only layers, if you
take the following Dockerfile</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> ubuntu:22.04         <span class="co"># Base layer (Layer A)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">RUN</span> <span class="ex">apt-get</span> update        <span class="co"># Layer B</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> app.py /opt          <span class="co"># Layer C</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="kw">CMD</span> [<span class="st">&quot;python&quot;</span>, <span class="st">&quot;/opt/app.py&quot;</span>]</span></code></pre></div>
<p>Each command creates a new read only layer, layers are stacked on top
each other, what does that mean we have the following layers in the
given Dockerfile above - Layer C -&gt; Layer B -&gt; Layer A, meaning
that similarly to commits in a source control system each layer is based
on the previous one, in this case Layer C is based off off Layer B and
so on. When you run a container read only layers from the image are
mounted as <code>lowerdir</code> - <code>OverlayFS</code>, a new
writeable layer <code>upperdir</code> is added on top, container
specific changes, the merged view what the container sees is the
<code>upperdir</code> + <code>lowerdir</code>. How are changes handled
in this case, modifying a file from the read only layer - the file is
copied from the <code>lowerdir</code> to the <code>upperdir</code>,
changes are applied to the copy in <code>upperdir</code>. New files on
the other hands are directly written on the <code>upperdir</code>.
Deleting a file, or a whiteout file, is created in <code>upperdir</code>
to hide the file in lower layers, the file is actually obscured or
occluded in the lower layers, not deleted from those layers.</p>
<p>The read only layers are shared between many different images and by
proxy containers, however the writeable layers, are only unique to the
container in question, these layers are ephemeral, when the container is
deleted so are they, the merged view is what the container sees between
the read-only layers and its own writeable layers. As we said each
modification or change in the container file system will generate a new
writeable layer. The read-only layers are only created during the
creation of the container image.</p>
<p>The key implications of this is that this is quite efficient model -
multiple containers share the same read only base layers, saves disk
space, and since a lot of images are based on the same base layers, such
as base images like CentOS, Alpine etc, these layers are shared.
Performance is also good since we have copy on write, only the modified
filed are copied into new layers. The changes in the container is
isolated well by the existence of the <code>upperdir</code> layer, the
writeable layer is ephemeral by default, deleted when the container is
deleted, use the volume to persist data across the containers, note that
volumes live outside of the file system layers, they are simply mount
point in the container, meaning that they are not part of the file
system layers at all</p>
<h3 id="uts-namespace">UTS namespace</h3>
<p><code>UTS</code> stands for UNIX time sharing in the Linux kernel.
The <code>UTS</code> namespace lets each container have its own hostname
and domain name. It can be confusing to talk about time sharing when the
<code>UTS</code> namespace has nothing to do with managing the system
clock. Time sharing originally referred to multiple users sharing time
on system simultaneously. Back in the 70s when the concept was created
it was a novel idea. The <code>UTS</code> data structure in the Linux
kernel had its beginnings then. This is where the hostname domain name
and other system information are retained. If you would like to see all
the information in that structure run <code>uname -a</code> on a Linux
server. That command queries the same data structure.</p>
<p>The easiest way to view the hostname for a server is to run the
hostname command. You could use <code>nsenter</code> to enter the
<code>UTS</code> namespace for the app-cli container the same way you
entered the mount namespace in the previous section. But there are
additional tools that will execute a command in the namespace for a
running container. One of those tools is the docker exec command. To get
the hostname value for a running container pass docker exec a
container’s short ID and the same hostname command you want to run in
the container. Docker executes the specified command for you in the
container’s namespaces and returns the value. The hostname for each
OpenShift container is its pod name:
<code>docker exec &lt;container-id&gt; hostname</code></p>
<p>Each container has its own hostname because of its unique
<code>UTS</code> namespace. If you scale up app-cli the container in
each pod will have a unique hostname as well. The value of this is
identifying the data coming from each container in a scale up system. To
confirm that each container has a unique hostname log into your cluster
as your developer user
<code>oc login -u developer -p developer &lt;cluster-url&gt;</code>. The
<code>oc</code> command line tools has a functionality that’s similar to
docker exec, instead of passing in the short ID for the container
however you can pass it the pod in which you want to execute the
command. After logging in to your <code>oc</code> client, scale the
app-cli application to two pods with the following command
<code>oc scale deployment/app-cli --replicas=2</code>, in this case the
<code>dc</code> in the command stands for deployment config, this is the
shorthand name for the object type. The deployment config is the old
3.xx version of the deployment object which was simply renamed in the
4.xx version to deployment. The command with newer versions would look
like a little bit different, but pretty much the same -
<code>oc scale deployment/app-cli --replicas=2</code></p>
<p>This will cause an update to your app-cli deployment config and
trigger the creation of a new app-cli pod. You can get the new pod’s
name by running the command <code>oc get pods | grep "Running"</code>.
The grep call prevents the output of pods in a completed state so you
see only active pods in the output. Because the container hostname is
its corresponding pod name in OpenShift you know which pod you were
working with using docker directly.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure that we scale up the deployment of the app, which would mean that we have to alter the deployment,</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># that would cause OpenShift to bring two new pods, when they are ready the old ones will be removed,</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale deployment/app-cli <span class="at">--replicas</span><span class="op">=</span>2</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="ex">deployment.apps/app-cli</span> scaled</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we grep only the ones in running state, making sure to avoid additional unwanted pods in completed state which will</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># only pollute the output</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;Running&quot;</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># we have two running pods in this case since we have set the replicas count to 2, now we can resolve the actual</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># hostname of the pod</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-5b9c58956d-p7jpn</span>   1/1     Running                  1          23h</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-5b9c58956d-p8jpn</span>   1/1     Running                  1          23h</span></code></pre></div>
<p>To get the hostname from your new pod, use the <code>oc exec</code>
command targeting the new pod, it is similar to docker exec, but instead
of the container’s short id you use the pod name to specify where you
want the command to run. The hostname for your new pod matches the pod
name, just like your original pod. The command may look something like
that:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that the project here is not passed because by default we are always in a project config, we may use oc -n to</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># specify temporary project namespaces/name for the current command only, this avoids having to first do the oc project</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;project-name&gt; first before having to run the command, and does not change the current context permanently</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc exec app-cli-5b9c58956d-p7jpn <span class="at">--</span> hostname</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the output of this command must always be the same like the pod-id, meaning that the hostname will always match the id</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># of the pod as shown by the command - oc get pods for example, the full id that is, therefore the command above will</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print the same text as the hostname as the pod id we have used to call the exec command with</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-5b9c58956d-p7jpn</span></span></code></pre></div>
<p><code>Remember that in docker, kubernetes and OpenShift, the hostname of the container in essence is always the unique container identifier as provided by the orchestrator or docker</code></p>
<h3 id="pid-namespace">PID namespace</h3>
<p>Because PID are how one app sends signals and information to other
apps isolating visible PID in a container to only the app in it is an
important security feature. This is accomplished using the PID
namespaces. On a linux server the <code>ps</code> command shows all
running processes along with their associated PID on the host. This
command typically has a lot of output on a buys system. The
<code>--ppid</code> option limits the output to a single PID and any
child processes it has spawned, from your app node, run the
<code>ps</code> command with the <code>--ppid</code> option and include
the PID you obtained for your app-cli container. Here you can see that
the process for PID 7825 is httpd and that it has spawned several other
processes:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we run this command on the cluster node itself, this will provide us with a list of processes started by this</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># particular process id, this process id however is the process id of the container, we obtained a few sections earlier</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ps <span class="at">--ppid</span> 7825</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="ex">PID</span> TTY          TIME CMD</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="ex">8938</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="ex">8943</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="ex">8951</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="ex">8959</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="ex">9033</span> <span class="pp">?</span>        00:00:09 httpd</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="ex">9034</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="ex">9035</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="ex">9064</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="ex">9070</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="ex">9071</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="ex">9077</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="ex">9082</span> <span class="pp">?</span>        00:00:08 httpd</span></code></pre></div>
<p>Use <code>oc</code> exec to get the output of <code>ps</code> for the
app-cli pod that matches the PID you collected, If you’ve forgotten you
can compare the hostname in the docker compare to the pod name. From
inside the container do not use the <code>--ppid</code> option, because
you want to see all the PID visible from within the app-cli
container.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># notice that here we use the pod&#39;s directly</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc exec app-cli-5b9c58956d-p7jpn <span class="at">--</span> ps</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="ex">PID</span> TTY          TIME CMD</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="ex">1</span> <span class="pp">?</span>        00:00:01 httpd</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="ex">28</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="ex">29</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">30</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">31</span> <span class="pp">?</span>        00:00:00 cat</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">32</span> <span class="pp">?</span>        00:00:09 httpd</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">33</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">34</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">62</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">68</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">69</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="ex">75</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">80</span> <span class="pp">?</span>        00:00:08 httpd</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="ex">330</span> <span class="pp">?</span>        00:00:00 ps</span></code></pre></div>
<p>There are three main differences in the output.</p>
<ul>
<li>The initial httpd command is listed in the output</li>
<li>The <code>ps</code> command is listed in the output</li>
<li>The PID are completely different</li>
</ul>
<p>Each container has a unique PID namespaces. That means from inside
the container the initial command that started the container (PID 7825)
is viewed as PID 1, this is the parent process, from which all others
get forked. All the processes it spawned also have PID, in the same
container specific namespace. Apps that are created by a process already
in a container automatically inherit the container’s namespace. This
makes it easier for app in the container to communicate. So far we have
discussed how filesystems hostnames and PID are isolated in a container,
next let us take a quick look at how shared memory resources are
isolated.</p>
<h3 id="memory-namespace">Memory namespace</h3>
<p>Apps can be designed to share memory resources. For example app A can
write a value into a special shared section of a system memory and the
value can be read and used by app B. The following shared memory
resource are isolated for each container in OpenShift</p>
<ul>
<li>POSIX message queue interfaces in
<code>/proc/sys/fs/mqueue</code></li>
<li>IPC interfaces in <code>/proc/sysvipc</code></li>
<li>Memory parameters like -
<code>msgmax, msgmnb, msgmni etc</code></li>
</ul>
<p>If a container is destroyed shared memory resources are destroyed as
well. Because these resources are app specific you will work with them
more in next sections. When you deploy a stateful app, the last
namespace to discuss in the network namespace.</p>
<h3 id="networking-namespace">Networking namespace</h3>
<p>The fifth kernel namespace that is used by docker to isolate
containers in OpenShift is the network namespace there is nothing funny
about the name for this namespace. The networking namespace isolated
network resources and traffic in a container the resources in this
definition mean the entire TCP/IP stack, is used by apps in the
container. Future chapters are solely dedicated to going deep into the
OpenShift software defined networking, but we need to illustrate in this
section how the view from within the container is drastically different
than the view from your host. The PHP builder image you used to create
<code>app-cli</code> and <code>app-gui</code> doesn’t have the IP
utility installed. You could install it into the running container using
yum. But a faster way to is to use <code>nsenter</code>. Earlier you
used <code>nsenter</code> to enter the mount namespace of the docker
process so you could view the root filesystem for app-cli.</p>
<p><code>It would be great if we could go through the OSI model here. Unfortunately it is out the scope for now. In short it is a model to describe how data travels in a TCP/IP network. These are seven layers, you will often hear about layer 3 devices, or a layer 2 switch, when someone says that, they are referring to the layer of the OSI model on which a particular device operates. Additionally, the OSI model is a great tool to use any time you need to understand how data moves through any system or app. If you haven't read up on the OSI model before, it is work your time to look at the article - "The OSI model explained, how to understand and (Remember) the 7 layer Network Model.</code></p>
<p>If you run <code>nsenter</code> and include a command as the last
argument, then instead of opening an interactive session in that
namespace the command is executed, in the specified namespace, and
returns the result. Using this tool you can run the IP command from your
server’s default namespace in the network namespace of your app-cli
container. If you compare this to the output from running
<code>/sbin/ip</code> a command on your host the differences are
obvious. Your app node will have 10 or more active network interfaces.
These represent the physical and software defined devices that make
OpenShift function securely. But in the app-cli container you have a
container specific loopback interface device and a single network
interface with a unique MAC IP address:</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># running the following on the host/cluster node can yield a big output similar to the output below</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ip a</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the output below is the abridged version of the actual output, suffice to say that the cluster node has many many more</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># software and hardware network devices active, compared to the container</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="ex">1:</span> lo: <span class="op">&lt;</span>LOOPBACK,UP,LOWER_UP<span class="op">&gt;</span> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/loopback</span> 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 127.0.0.1/8 scope host lo</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> ::1/128 scope host</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="ex">2:</span> eth0: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 52:54:00:bc:15:3a brd ff:ff:ff:ff:ff:ff</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 192.168.122.66/24 brd 192.168.122.255 scope global noprefixroute dynamic eth0</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> 3187sec preferred_lft 3187sec</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::5054:ff:febc:153a/64 scope link</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="ex">3:</span> eth1: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 52:54:00:53:34:7a brd ff:ff:ff:ff:ff:ff</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 192.168.42.124/24 brd 192.168.42.255 scope global noprefixroute dynamic eth1</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> 3146sec preferred_lft 3146sec</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::5054:ff:fe53:347a/64 scope link</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="ex">5:</span> docker0: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue state UP group default</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 02:42:be:ed:e5:72 brd ff:ff:ff:ff:ff:ff</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 172.17.0.1/16 scope global docker0</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::42:beff:feed:e572/64 scope link</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="ex">11:</span> veth3dab0ee@if10: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue master docker0 state UP group default</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> f6:a1:76:bd:cc:1d brd ff:ff:ff:ff:ff:ff link-netnsid 0</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::f4a1:76ff:febd:cc1d/64 scope link</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="ex">15:</span> veth8baab20@if14: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue master docker0 state UP group default</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> e2:10:11:22:12:87 brd ff:ff:ff:ff:ff:ff link-netnsid 2</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::e010:11ff:fe22:1287/64 scope link</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="ex">23:</span> veth1a268b0@if22: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue master docker0 state UP group default</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 6e:f8:23:2d:6d:36 brd ff:ff:ff:ff:ff:ff link-netnsid 6</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::6cf8:23ff:fe2d:6d36/64 scope link</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a><span class="ex">25:</span> veth98336dc@if24: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue master docker0 state UP group default</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 8e:f8:d3:78:6f:27 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::8cf8:d3ff:fe78:6f27/64 scope link</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a><span class="ex">31:</span> vethe2d8c8f@if30: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1500 qdisc noqueue master docker0 state UP group default</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> f2:80:d0:94:f5:af brd ff:ff:ff:ff:ff:ff link-netnsid 3</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::f080:d0ff:fe94:f5af/64 scope link</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span></code></pre></div>
<p>Here is the other command which uses the same user binary
<code>ip</code> from the host, to run in the context of the container
(or as we know a container is a fancy process / PID) namespace.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this we again run on the host/cluster node, however as you can see we are first entering the container namespace, in</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># this case, then we run the IP command, what is cool is that since the container is running on the host/cluster node, we</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># can use the same binary (ip) located in sbin, to obtain information about the network namespace but in the context of</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the container, instead of the host.</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo nsenter <span class="at">-t</span> 7825 <span class="at">-n</span> /sbin/ip a</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># This is pretty much the entire un-edited output from the command when run in the context of the container namespace,</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># as you can see the number of software / hardware devices here are much much less, which is what we would expect, and</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># as mentioned, one loopback - lo, and one regular interface - eth0</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="ex">1:</span> lo: <span class="op">&lt;</span>LOOPBACK,UP,LOWER_UP<span class="op">&gt;</span> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/loopback</span> 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 127.0.0.1/8 scope host lo</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> ::1/128 scope host</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="ex">2:</span> eth0@if11: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1400 qdisc noqueue state UP group default</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 0a:58:0a:d9:00:43 brd ff:ff:ff:ff:ff:ff link-netns 312e9fcc-4ccb-4a32-8488-91b6d821d62e</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet</span> 10.217.0.67/23 brd 10.217.1.255 scope global eth0</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::858:aff:fed9:43/64 scope link</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span></code></pre></div>
<h3 id="summary">Summary</h3>
<p>The network namespace is the first component in the OpenShift
networking solution. We’ll discuss how network traffic gets in and out
of containers in next sections, when we cover OpenShift networking in
depth, in OpenShift isolating processes doesn’t happen in the app- or
even in the userspace, on the app node. This is a key difference between
other types of software clusters, and even some other container based
solutions. In OpenShift isolation and resource limits are enforced in
the linux kernel on the app nodes, isolation with kernel namespaces
provides a much smaller attack surface. An exploit that would let
someone break out from a container would have to exist in the container
runtime or the kernel itself. With OpenShift as we’ll discuss in depth
in next section when we examine security principles in OpenShift
configurations of the kernel and the container runtime is tightly
controlled. The last point we would like to make in this section echoes
how we began the discussion. Fundamental knowledge of how containers
work and use the Linux kernel is invaluable. When you need to manage
your cluster or troubleshoot issues when they arise, this knowledge lets
you think about containers in terms of what they are doing all the way
to the bottom of the Linux kernel. That makes solving issues and
creating stable configurations easier to accomplish. Before you move on
clean up to a single replica.</p>
<h2 id="cloud-native-apps">Cloud native apps</h2>
<p>Cloud native is how the next generation of apps is being created. In
this part of the book, we’ll discuss the technologies in OpenShift that
create the continuously deploying self-healing auto-scaling behaviors we
all expect in a cloud native app. Previous chapters focused on working
with and modifying the services in OpenShift. This section also walks
you through creating problem for your app to ensure that they’re always
functioning correctly.</p>
<h3 id="testing-app-resiliency">Testing app resiliency</h3>
<p>When you deployed the Image Uploader app in previous sections, one
pod was created for each deployment. If that pod crashed, the app would
be temporarily unavailable until a new pod was created to replace it. If
your app became more popular, you would not be able to support new users
past the capacity of a single pod. To solve this problem and provide
scalable apps, OpenShift deploys each app with the ability to scale up
and down. The app component that handles scaling app pods is called the
replication controller.</p>
<h4 id="replication-controller">Replication controller</h4>
<p>The replication controller main function is to ensure that the
desired number of identical pod is running all the time. If a pod exits
or fails, the replication controller deploys a new one to ensure a
healthy app is always available. In other words OpenShift takes care of
maintaining the
<code>desired state, as configured by the developers or app managers</code>.
You can think of the replication controller as a pod monitoring agent
that ensures certain requirements are met across the entire OpenShift
cluster. You can check the current status of the replication controller
(RC) for the app-cli deployment by running the <code>oc describe</code>.
Note that the individual deployment is specified not the name of the
app. The information tracked about the RC helps to establish its
relationships to the other components that make up the app.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to make sure that we create, a replication controller instances, which will be done when we use scale, on the existing</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># deployment, the replication controller is an evolution of the replica controller, but it is basically the same object</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># that is representing the actual replicas being managed by the orchestrator</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale deployment/app-cli <span class="at">--replicas</span><span class="op">=</span>2</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="ex">deploymentconfig.apps.openshift.io</span> <span class="st">&quot;app-cli&quot;</span> scaled</span></code></pre></div>
<ul>
<li>Name of the RS/RC, which is the same as the name of the deployment,
it is associated with</li>
<li>Image name used to create pods for the RC</li>
<li>Labels and selectors for the RC</li>
<li>Current and desired number of pod replicas running in the RC</li>
<li>Historical pod status information i for the RC, including how many
pods are waiting to be started or have failed since the creation of the
RC</li>
</ul>
<p>The labels and selectors in the next listing are key-value pairs,
that are associated with all OpenShift components. They are used to
create and maintain the relationships and interactions between apps. We
will discuss them in more depth in next sections.</p>
<p>Also need to note that the command below will return the list of RC
only if there were ever any replicas created, if the app pods were never
replicated, meaning that there was only ever one pod for the deployment
config, then no RC object will be created, make sure that you have had
created replicas for the given deployment, otherwise you might not
receive the expected result, from <code>oc get rs</code></p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># describe the replication controller</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe rs/app-cli-1</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         app-cli-1</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>    image-uploader</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Selector:</span>     app=app-cli,deployment=app-cli-1,deploymentconfig=app-cli</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       app=app-cli</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment-config.name=app-cli</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  openshift.io/deployer-pod.completed-at=<span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployer-pod.created-at=</span><span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployer-pod.name=app-cli-1-deploy</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment-config.latest-version=1</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment-config.name=app-cli</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment.phase=Complete</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment.replicas=1</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/deployment.status-reason=config</span> change</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>              <span class="ex">openshift.io/encoded-deployment-config={</span><span class="st">&quot;kind&quot;</span><span class="ex">:</span><span class="st">&quot;DeploymentConfig&quot;</span><span class="ex">,</span><span class="st">&quot;apiVersion&quot;</span><span class="ex">:</span><span class="st">&quot;v1&quot;</span><span class="ex">,</span><span class="st">&quot;metadata&quot;</span><span class="ex">:{</span><span class="st">&quot;name&quot;</span><span class="ex">:</span><span class="st">&quot;app-cli&quot;</span><span class="ex">,</span><span class="st">&quot;namespace&quot;</span><span class="ex">:</span><span class="st">&quot;image-uploader&quot;</span><span class="ex">,</span><span class="st">&quot;selfLink&quot;</span><span class="ex">:</span><span class="st">&quot;/apis/apps.openshift.io/v1/namespaces/image-up...</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a><span class="st">Replicas:     2 current / 2 desired</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="st">Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="st">Pod Template:</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="st">  Labels:       app=app-cli</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="st">                deployment=app-cli-1</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="st">                deploymentconfig=app-cli</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="st">  Annotations:  openshift.io/deployment-config.latest-version=1</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="st">                openshift.io/deployment-config.name=app-cli</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a><span class="st">                openshift.io/deployment.name=app-cli-1</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="st">                openshift.io/generated-by=OpenShiftNewApp</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="st">  Containers:</span></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="st">   app-cli:</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="st">    Image:        172.30.1.1:5000/image-uploader/app-cli@sha256:478fe6428546186cfbb0d419b8cc2eab68af0d9b7786cc302b2467e5f11661db</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="st">    Ports:        8443/TCP, 8080/TCP</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="st">    Host Ports:   0/TCP, 0/TCP</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="st">    Environment:  &lt;none&gt;</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a><span class="st">    Mounts:       &lt;none&gt;</span></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="st">  Volumes:        &lt;none&gt;</span></span></code></pre></div>
<p><code>Note that more recent versions of openshift will by default use the ReplicationController instead of the ReplicationController object, this is because the new set object is more feature full, and the old ReplicationController is being phased out, ultimately it is the same object, with some new nice features added on top of the manifest and object specification</code></p>
<h4 id="labels-and-selectors">Labels and selectors</h4>
<p>As we go forward, it is important that you understand the following,
regarding how labels and selectors are used in OpenShift</p>
<ul>
<li><p>When an pp is deployed in OpenShift every object that is created
is assigned a collection of labels. Labels are unique per project, just
like apps names. That means in Image Uploader, only one app can be named
app-cli.</p></li>
<li><p>Labels that have been applied to an object are attributes that
can be used to create relationships in OpenShift. But relationships are
two way streets, if something can have a label, something else must be
able to state a need for a resource with that label. The other side of
this relationship exists in the shape of label selectors</p></li>
<li><p>Label selectors are used to define the labels that are required
when work needs to happen</p></li>
</ul>
<p>Let’s examine this in more depth, using the app-cli app you deployed
in the second section. In the next example you will remove a label from
a deployed pod. This is one of the few times we will ask you to do
something to intentionally break an app. Removing a label from a pod
will break the relationship with the RC and other app components. The
purpose of this example is to demonstrate the RC in action - and to do
that you need to create a condition it need to remedy.</p>
<p>As we have already mentioned selectors in an app component are the
labels it uses to interact with other components, and link up to them,
they are
<code>not simply meant for tagging human readable information to app components</code>.
The app-cli RC will create and monitor apps pods with the following
labels:</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a># this is the labels and selector sections from a describe command run against the deployment of the app-cli, you can</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a># see that in our case the deployment object is tagged with many labels, and the selector in this case matches the</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a># in the pod template. The labels for the dpeloyment are mostly kubernetes specific, and only one is really the user</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a># defined one which is app=app-cli</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>Name:                   app-cli</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>Namespace:              image-uploader</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>CreationTimestamp:      &lt;date&gt;</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>Labels:                 app=app-cli</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>                        app.kubernetes.io/component=app-cli</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>                        app.kubernetes.io/instance=app-cli</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>                        app.kubernetes.io/name=php</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>Selector:               deployment=app-cli</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a># pod contains only one label, which is matched against the selector section in the deployment above, that is how they</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a># are interlinked, and that is how OpenShift knows which pods belong to which deployments, the same rule is true and</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a># followed for any other OpenShift object, that is how relations between them are build in Kubernetes and OpenShift</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>Pod Template:</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>  Labels:       deployment=app-cli</span></code></pre></div>
<p>The fastest way to delete the pods for the app-cli deployments is
through the command line. This process shouldn’t be part of your normal
app workflow, , but it can come in handy when you’re troubleshooting
issues in an active cluster. From the command line run the following
<code>oc delete pod -l app=app-cli</code> what this does is it will
delete all pods that have a matching selector <code>app-cli</code>, the
app selector was automatically attached when we created the app-cli in
the first place by OpenShift, but usually in the real world one would
decide how to name the key (app) and value (app-cli) pair of the label
tag. What is important to note is that labels are fundamental part of
how OpenShift works, and Kubernetes as well for that matter. If we can
make an analogy it would be that the labels are like table relations in
a relational database.</p>
<p>There is also another command that can be used to delete and cleanup
all resources related to and attached to a given label, we can use the
following - <code>oc delete all --selector app=app-cli</code>. This will
make sure to delete all resources, which are attached or related and
associated to the given label.</p>
<p>Returning back to the pod for which we removed or detached the label,
you might be wondering whether the abandoned pod will still receive
traffic from users. It turns out that the service object, responsible
for network traffic, also works on the concept of labels and selectors.
To determine whether the abandoned pod would have served traffic, you
need to look at the Selector field in the service object. You can get
this selector information about the app-cli service by running the
following <code>oc describe svc app-cli | grep Selector</code> this will
print out the selector label for the service, in this case something
like <code>app=app-cli,deploymentconfig=app-cli</code>, as you can see
there are more than one selectors which can be applied to a OpenShift
object, the key and value are usually enough to know how that label or
selector will be used, here we can see that there is one generic top
level selector which is the <code>app=app-cli</code> which basically
says that this service object is linked to all <code>app-cli</code>
objects, in a way tagging them with a namespace of sorts, then there is
a specific selector which links the service to the deployment config
which is in our case not relevant. Because we have deleted the selector
label from the original pod, its label are not longer match for the
app-cli service, since there is no other pod, that means that not
traffic will be routed to the original pod, selectors would no longer
receive traffic requests.</p>
<p>Kubernetes was born out of many lessons learned at Google from
running containers at scale for 10+ years. The main two orchestration
engines internally at Google during this time have been Borg and its
predecessor, Omega. One of the primary lessons learned from the two
systems was that control loops of decoupled API objects were far
preferable to large centralized stateful orchestration. This type of
design is often called control through choreography. Here are just a few
of the ways it is implemented in Kubernetes.</p>
<ul>
<li>Decoupled API components</li>
<li>Avoided stateful information</li>
<li>Looping through control loops against various micro services</li>
</ul>
<p>By running through control loops instead of maintaining a large state
diagram, the resiliency of the system is considerably improved. If a
controller crashes, i,t reruns the loop when it’s restarted whereas a
state machine can become brittle when there are errors or the system
starts to grow in complexity. In our specific examples, this holds true
because the RC loops through pods with the labels in its Selector field
as opposed to maintaining a list of pods that it is supervising</p>
<p>Replication controllers ensure that properly configured apps pods are
always available in the proper number. Additionally the desired replica
counts can be modified manually or automatically. In the next section we
will discuss how to scale app deployments</p>
<h2 id="scaling-applications-1">Scaling applications</h2>
<p>An app can consist of many different pods, all communicating together
to do work for the app users. Because different pods need to be scaled
independently, each collection of identical pods is represented by a
service components, as we initially discussed. More complex apps can
consist of multiple services of independently scaled pods. A standard
app design uses three tiers to separate concerns in the app.</p>
<ul>
<li>Presentation layer - Provides the user interface, styling and
workflows, for the user. This is typically where the website lives.</li>
<li>Login layer - Handles all the required data processing for an app.
This is often referred to as the middleware layer.</li>
<li>Storage layer - Provides persistent storage for app data. This is
often database, filesystems or a combination of both</li>
</ul>
<p>So basically the app code runs in the pods for each app layer. That
code is accessed directly from the routing layer. Each app service
communicates with the routing layer and the pods it manages. This design
results in the fewest network hops between the user and the app. This
design also allows each layer in the three tier design to be
independently scaled to handle its workload effectively without making
any changes to the overall app configuration, also changes in any of app
layers are not going affect the other layers they interact with,
e.g. <code>scaling</code> or generally modifying the midtier end layer
will impact the frontend tier</p>
<h2 id="application-health">Application Health</h2>
<p>In most situations app pods run into issues because the code in the
pod stops responding.</p>
<p>This is typically where the website lives, login layer handles all
the required data processing for an this is often referred to as the
middleware layer, storage layer provides persistent storage for data,
this is often database filesystems or a combination of both, so
basically the code runs in the pods layer, that code is accessed
directly form the routing layer, each service communication with the
routing layer. The first step is building a resilient app is to run
automated health and status checks on your pods, restarting them when
necessary without manual intervention. Creating probes to run the needed
checks on apps to make sure they are healthy is built into Open shift.
The first type of problem we will look at is the liveness probe.</p>
<p>In OpenShift you define a liveness probe as a parameter for specific
containers in the deployment config. The liveness probe configuration
then then propagates down to individual containers created in pods
running as part of the deployment. A service on each node running the
container is responsible for running the liveness probe that is defined
in the deployment config. If the liveness probe was created as a script
then it is run inside the container. If the liveness probe was created
as HTTPS response or TCP socket based probe then it is run by the node
connecting to the container. If a liveness probe fails for a container
then the pod is restarted.</p>
<p>Note that the service that executed liveness probe checks is called
the kubelet service. This is the primary service that runs on each app
node in the OpenShift cluster, and it actually responsible for many
other things, among which is running the liveness status of the
node.</p>
<h3 id="creating-liveness-probes">Creating liveness probes</h3>
<p>In OpenShift the liveness probe component is a simple powerful
concept that checks to be sure an app pod is running and healthy.
Liveness probes can check container health three ways:</p>
<ul>
<li>HTTP checks if a given URL endpoint served by the container, and
evaluates the HTTPS status response code</li>
<li>Container execution check - a command typically a script that is run
at intervals to verify that the container is behaving as expected. Non
zero exit code from the command results in a liveness check
failure.</li>
<li>TCP socket check - Checks that a TCP connection can be established
on a specific TCP port in the app pod.</li>
</ul>
<p>Note that the HTTP response code is a three digit number supplied by
the server as part of the HTTP response headers in a web request. A 2xx
response indicates that a successful connection is made, and a 3xx
response indicated that HTTP redirect. You can find more bout the
response codes on the <code>IETF</code> website</p>
<p>As a best practice always create a liveness probe unless your app is
intelligent enough to exit when it hits an unhealthy state. Create
liveness probes that not only check the internal components of the app,
but also isolate problems from external service dependencies. Fr example
a container shouldn’t fail its liveness probe because another service
that it needs isn’t functional. Modern apps should have code to
gracefully handle missing service dependencies. If you need an app to
wait for a missing service dependency you can use readiness probes.,
which are covered later on in this section. For legacy apps that require
an ordered startup sequence of replicated pods, you can take advantage
of a concept called stateful sets, which we will cover later on. To make
creating probes easier, a health check wizard is built in the OpenShift
web interface, using the wizard will help you avoid formatting issues
that can result from creating the raw YAML template by hand.</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc set probe deployment/app-cli <span class="at">--liveness</span> <span class="at">--get-url</span><span class="op">=</span>http://:8080/ <span class="at">--initial-delay-seconds</span><span class="op">=</span>5</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="ex">deployment.apps/app-cli</span> probes updated</span></code></pre></div>
<p>After one adds the liveness probe, there are ways to check how the
liveness probe has been reflected in the final deployment configuration
by simply inspecting the deployment object, and we will notice that
there is indeed a new line in there which represents the liveness probe,
of type HTTP, which check on the root context path of the app, there are
other minor things that we can also take a note of such that - timeout,
the time to wait for a response from the endpoint, then there is the
period, meaning that every N number seconds a new request will be made,
the threshold is 1 success response means the container is alive, at
most 3 failures means that the pod will be restarted and so will be the
container</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here is the command which will inspect and show the liveness probe, that was recently configured, most of the data for</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the deployment was omitted, just to show the probe info from the otherwise big deployment describe command dump</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe deployment <span class="at">-l</span> app=app-cli</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                   app-cli</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Pod</span> Template:</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Labels:</span>       deployment=app-cli</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Annotations:</span>  openshift.io/generated-by: OpenShiftNewApp</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Containers:</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>   <span class="ex">app-cli:</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Image:</span>         image-registry.openshift-image-registry.svc:5000/image-uploader/app-cli@sha256:17c9ec389e0e130b3891e25e64b53cda350a6223732f62f56032b42cb361ffd1</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Ports:</span>         8080/TCP, 8443/TCP</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Host</span> Ports:    0/TCP, 0/TCP</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Liveness:</span>      http-get http://:8080/ delay=5s timeout=1s period=10s <span class="co">#success=1 #failure=3</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Environment:</span>   <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Mounts:</span>        <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Volumes:</span>         <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Node-Selectors:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Tolerations:</span>     <span class="op">&lt;</span>none<span class="op">&gt;</span></span></code></pre></div>
<p>Here is how the actual YAML manifest file, or at least the part that
would contain the <code>livenessProbe</code> configuration would look
like, had you configured it manually, the information that we can see
here is the same as one can observe in the describe command above</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">livenessProbe</span><span class="kw">:</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">httpGet</span><span class="kw">:</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">scheme</span><span class="kw">:</span><span class="at"> HTTP</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">timeoutSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">5</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">periodSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">successThreshold</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">failureThreshold</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span></code></pre></div>
<h3 id="creating-readiness-probes">Creating readiness probes</h3>
<p>Many apps need to perform any combination of the following before
they are able to receive traffic, which increases the amount of time
before an app is ready to do work. Some common tasks include</p>
<ul>
<li>Loading classes into memory</li>
<li>Initializing datasets and databases</li>
<li>Performing internal checks</li>
<li>Establishing a connection to other containers or external
service</li>
<li>Finishing a startup sequence or other workflow</li>
</ul>
<p>Fortunately, OpenShift also supports the concept of readiness probes,
which ensures that the container is ready to receive traffic before
marking the pod as active. Similar to the liveness probe a readiness
probe is run at the container level in a pod and supports the same
HTTPS, container execution, and TCP socket based checks like the
liveness probe does. Unlike the liveness probe however, a failed
readiness check doe not result in a new pod being deployed, if a
readiness check fails the pod remains running while not receiving
traffic. Let’s run through an example of adding a readiness probe to the
app-cli app, using the command line. For this readiness probe, you will
tell OpenShift to look for a non-existent endpoint (that would simulate
a startup delay between the container running, but the potentially heavy
to start and setup app spinning up into a started state).</p>
<p>Looking for a URL that does not exist in your app-cli deployment will
cause the readiness probe to fail. This exercise illustrates how
OpenShift probe works when it runs into an undesired condition. Until a
deployment passes a readiness probe it will not receive user requests.
If it never passes the readiness probe, as in this example the
deployment will fail and never be made available to users. To create the
readiness probe use the command line and run the command:</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will again as any other deployment update cause the old pods to be deleted and new ones will be spun up in their</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># place which would represent the new state</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc set probe deployment/app-cli <span class="at">--readiness</span> <span class="at">--get-url</span><span class="op">=</span>http://:8080/notreal <span class="at">--initial-delay-seconds</span><span class="op">=</span>5</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="ex">deployment.apps/app-cli</span> probes updated</span></code></pre></div>
<p>The output includes a message that the deployment was updated. Just
like a liveness probe, creating a readiness probe triggers the creation
of a new app-cli deployment. Check to see whether the new pods were
deployed by running the <code>oc get pods</code></p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                     READY STATUS    RESTARTS AGE</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-1-build</span>          0/1   Completed 0        8d</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-59c7ff6b74-h7q8r</span> 1/1   Running   0        14m</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-59c7ff6b74-pnldz</span> 1/1   Running   0        14m</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-6d69c87c88-lm4tc</span> 0/1   Running   0        67s</span></code></pre></div>
<p>Note below that the liveness probe is fine, and is reporting that
there are no failures while that is not true for the readiness probe,
which is reporting a failed state, which is expected after having it
configured with a non existing endpoint to check against</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                   app-cli</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Pod</span> Template:</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Labels:</span>       deployment=app-cli</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Annotations:</span>  openshift.io/generated-by: OpenShiftNewApp</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Containers:</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>   <span class="ex">app-cli:</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Image:</span>         image-registry.openshift-image-registry.svc:5000/image-uploader/app-cli@sha256:17c9ec389e0e130b3891e25e64b53cda350a6223732f62f56032b42cb361ffd1</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Ports:</span>         8080/TCP, 8443/TCP</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Host</span> Ports:    0/TCP, 0/TCP</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Liveness:</span>      http-get http://:8080/ delay=5s timeout=1s period=10s <span class="co">#success=1 #failure=3</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Readiness:</span>     http-get http://:8080/notreal delay=5s timeout=1s period=10s <span class="co">#success=1 #failure=3</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Environment:</span>   <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Mounts:</span>        <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Volumes:</span>         <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Node-Selectors:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Tolerations:</span>     <span class="op">&lt;</span>none<span class="op">&gt;</span></span></code></pre></div>
<p>The new pod is running, but will never be ready (1/1), that is
because our probe was configured to check for a non existent URL, it is
not ready to receive traffic. The previous pod is still running and
receiving any incoming requests. Eventually the readiness probe will
fail two more times and will meet the readiness probe
<code>failureThreshold</code> metrics, which were set to 3 by default.
As we discussed in the previous section, <code>failureThreshold</code>
for a readiness or liveness probe sets the number of times a probe will
be attempted before it is considered a failure</p>
<p><code>Note that the readiness probe will take 10 minutes to trigger a failed deployment. When this happens the pod will be deleted, and the deployment will roll back to the old working configuration, resulting in a new pod without the readiness probe. You can modify the default timeout parameters by changing the timeoutSeconds parameter as part of the spec.strategy.*params in the deployment object. Deployment strategies are covered in greater detail in future chapters</code></p>
<p>Once all three failures occur, the deployment is marked as failed,
and OpenShift automatically reverts back to the previous deployment. The
reason for the failure will be shown in the event view in OpenShift,
that can be shown from the command line tool or the web console Because
events are easier to read through the web console user interface, let us
check that out there. Expand the panel in the events view, and you will
be able to see the deployment has failed, and why that was, the reason
will be stated as well.</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure to restore the readiness probe to point to the correct, endpoint, that is at least valid, that would drop</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the old pods, and make sure the newly created ones are all in a valid state where both the liveness and readiness probe</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc set probe deployment/app-cli <span class="at">--readiness</span> <span class="at">--get-url</span><span class="op">=</span>http://:8080/ <span class="at">--initial-delay-seconds</span><span class="op">=</span>5</span></code></pre></div>
<h2 id="auto-scaling-with-metrics">Auto-scaling with metrics</h2>
<p>In the last section, we learned about the health and status of an
app. You learned that OpenShift deployment use replication controllers -
also called replication controllers, under the covers to ensure that a
static number of pods is always running, that the desired state,
configured by us, matches the actual state the cluster has. Readiness
probes and liveness probes make sure running pods start as expected and
behave as expected. The number of pods servicing a given workload can
also be easily modified to anew static number with a single command or
the click of a button.</p>
<p>This new deployment model gives you much better resource utilization
than the traditional virtual machine model, but it is not a silver
bullet, for operational efficiency. One of the big IT challenges with
virtual machines is resource utilization. Traditionally when deploying
virtual machines developers ask for much higher levels of CPU and RAM
than are actually needed. Not only is making changes to the machine
resources challenging, but many developers typically have no idea what
types of resources are needed to run the app. Even at large companies
like Google and Netflix predicting app workload demand is so challenging
that tools are often used to scale the apps as needed.</p>
<h3 id="determining-expected-workloads">Determining expected
workloads</h3>
<p>Imagine that you deployed a new app and it unexpectedly exploded in
popularity, external monitoring tools notify you that you need more pods
to run your app. Without any historical context there is no data to
indicate how many new pods are needed tomorrow, next week or next month.
A great example is Pokemon GO a popular mobile app that runs on
Kubernetes. Within minutes of its release demand spiked well past
expectations and over the opening weekend it became an international
sensation. Without the ability to dynamically provision pods on demand,
the game likely would have crashed, as millions of users started to
overload the system. In OpenShift triggering horizontal pod scaling
without human intervention is called autoscaling. Developers can set
limits and objective measures to scale pods up and down on demand, and
administrators can limit the number of pods to a defined range. The
indicators that OpenShift uses to determine if the app needs more of
fewer pods are based on pod metrics such as CPU and memory usage. But
those pod metrics are not available out of the box, to use metrics in
OpenShift the administrators must deploy the OpenShift metrics tack.
This metric stack comprises several popular open source technologies,
like <code>Prometheus, Hawkular, Heapster, and Apache Cassandra</code>.
Once the metric stack is installed, OpenShift autoscaling has the
objective measures it needs to scale pods up and down on demand.</p>
<p>The metric stack can also be deployed with the initial OpenShift
installation by using the advanced installation option. The latest
versions of OpenShift also have the option to deploy Prometheus, a
popular open source monitoring and altering solution, to provide and
visualize cluster metrics, in the future, Prometheus may be used as the
default metric solution.</p>
<h3 id="installing-openshift-metrics">Installing OpenShift metrics</h3>
<p>Installing the OpenShift metrics stack is straightforward, by default
the pods that are used to collect and process metrics run in the open
shift infra project, that was created by default during the
installation. Switch to the open shift infra project, from the command
line</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># before running that command make sure you have switched and logged into the administrator user, otherwise the regular</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># user does not have access and will not even be able to see system infra projects</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc console <span class="at">--credentials</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc login <span class="op">&lt;</span>cluster-ip<span class="op">&gt;</span> -u <span class="op">&lt;</span>kubeadmin<span class="op">&gt;</span> -p <span class="op">&lt;</span>adminpassword<span class="op">&gt;</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># this is the important bit, that would change the default cluster version object, which controls certain flags/switches</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># determining which features are enabled or not, in this case we remove the first entry from the default overrides, which would restore the default behavior, and enable metrics</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe clusterversion/version</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc patch clusterversion/version <span class="at">--type</span><span class="op">=</span><span class="st">&#39;json&#39;</span> <span class="at">-p</span> <span class="st">&#39;[{&quot;op&quot;:&quot;remove&quot;, &quot;path&quot;:&quot;/spec/overrides/0&quot;}]&#39;</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods <span class="at">-n</span> openshift-monitoring</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="ex">alertmanager-main-0</span>                                      0/6     Pending   0          8m4s</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="ex">cluster-monitoring-operator-6db6cb4c67-6gcmn</span>             1/1     Running   0          8m23s</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-state-metrics-79fb78866f-dvf4q</span>                      3/3     Running   0          8m5s</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="ex">metrics-server-7d57d94644-2hbkw</span>                          1/1     Running   0          8m</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="ex">monitoring-plugin-d9d9ccfc8-97z98</span>                        0/1     Pending   0          7m59s</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="ex">node-exporter-xl7nq</span>                                      2/2     Running   0          8m5s</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="ex">openshift-state-metrics-5476c76b54-qc49r</span>                 3/3     Running   0          8m5s</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="ex">prometheus-k8s-0</span>                                         0/6     Pending   0          7m59s</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="ex">prometheus-operator-55955cbfbc-c9q8n</span>                     2/2     Running   0          8m15s</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="ex">prometheus-operator-admission-webhook-7cb5b87c4b-92jcz</span>   1/1     Running   0          8m19s</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="ex">telemeter-client-bb985cdff-95fp8</span>                         0/3     Pending   0          2m51s</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="ex">thanos-querier-67d44b859-29klf</span>                           0/6     Pending   0          8m3s</span></code></pre></div>
<h3 id="understanding-the-metrics">Understanding the metrics</h3>
<p>In the previous section you successfully deployed the OpenShift
metrics stack. Three types of pods were deployed to make this happen
each with a different purpose, using technologies like Prometheus. But
none of the pods generate metrics themselves. Those come from kubelets,
a kubelet is a process that runs on each OpenShift node and coordinated
which tasks the node should execute with the OpenShift master. As an
example of an replication controller request that a pod be started, the
OpenShift scheduler which runs on a master eventually tasks an OpenShift
node to start the pod. The command to start the pod is passed to the
kubelet process running on the assigned OpenShift node. One of the
additional responsibilities of the kubelet is to expose the local
metrics available, to the Linux kernel through an HTTPS endpoint. The
OpenShift metrics pods use the metrics exposed by the kubelet on each
OpenShift node as their data source. Although the kubelet exposes the
metrics for individual nodes through HTTPS, no built in tools are
available to aggregate this information and present a cluster wide view.
This is where Prometheus comes in handy, it acts as the back end for
metrics deployment, it queries the API server for the list of nodes and
then queries each individual node to get the metrics for the entire
cluster. It stores the metrics in its internal store data set. On the
frontend the Prometheus pod processes the metrics. All metrics are
exposed in the cluster through a common REST API to pull metrics into
the OpenShift console. The API can also be used for integration into the
third party tools or other monitoring solutions.</p>
<h3 id="using-pod-metrics-autoscaling">Using pod metrics &amp;
autoscaling</h3>
<p>To implement pod autoscaling based around metrics you need a couple
of simple things - first you need a metrics stack to pull and aggregate
the metrics from the entire cluster and then make those metrics easily
available. So far so good. Second you need an object to monitor the
metrics and trigger the pod up and down. This object is called a
Horizontal Pod Auto-scaler - HPA (Remember that abbreviation). And its
main job is to define when OpenShift should change the number of
replicas in an app deployment.</p>
<h4 id="creating-the-hpa-object">Creating the HPA object</h4>
<p>OpenShift provides a shortcut from the CLI to create the HPA object.
This shortcut is available through the
<code>oc autoscale command</code>. Switch to the CLI and use the
following command</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note the scaling factor and conditions, we define the min and max number of replicas, and we also define where the</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># replicas should be created, in this case when the CPU usage reaches a certain threshold</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc autoscale deployment/app-cli <span class="at">--min</span> 2 <span class="at">--max</span> 5 <span class="at">--cpu-percent</span><span class="op">=</span>75</span></code></pre></div>
<p>A couple of things happen when you run that command. First you
trigger an automatic scale up to two app-cli pods by setting the minimum
number of pods to 2. Run the following command to verify the number of
app-cli pods. - <code>oc get pods</code>. We should be seeing at least
two running pods in the ready state, immediately, even if we have not
had any or just one replica. Second the HPA object was created for you.
By default it has the same name as the Deployment object, app-cli this
command gets the name of the HPA object created by the
<code>oc autoscale</code></p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will list all HPA objects, in our case only one deployment has it, and we also have only one deployment anyway.</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get hpa</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>      REFERENCE            TARGETS              MINPODS   MAXPODS   REPLICAS   AGE</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli</span>   Deployment/app-cli   cpu: <span class="op">&lt;</span>unknown<span class="op">&gt;</span>/75%   2         5         2          6d</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe hpa/app-cli</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                                                  app-cli</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>                                             image-uploader</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>                                                <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>                                           <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="ex">CreationTimestamp:</span>                                     <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Reference:</span>                                             Deployment/app-cli</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Metrics:</span>                                               <span class="er">(</span> <span class="ex">current</span> / target <span class="kw">)</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">resource</span> cpu on pods  <span class="er">(</span><span class="fu">as</span> a percentage of request<span class="kw">)</span><span class="bu">:</span>  <span class="op">&lt;</span>unknown<span class="op">&gt;</span> / 75%</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="ex">Min</span> replicas:                                          2</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="ex">Max</span> replicas:                                          5</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Deployment</span> pods:                                       2 current / 2 desired</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="ex">Conditions:</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Type</span>           Status  Reason                   Message</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>  <span class="ex">----</span>           <span class="at">------</span>  <span class="at">------</span>                   <span class="at">-------</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">AbleToScale</span>    True    SucceededGetScale        the HPA controller was able to get the target<span class="st">&#39;s current scale</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="st">  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu in container app-cli of Pod app-cli-d97b4c84b-vbmsg</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="st">Events:</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a><span class="st">  Type     Reason                        Age                      From                       Message</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="st">  ----     ------                        ----                     ----                       -------</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="st">  Warning  FailedGetResourceMetric       8s                       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container app-cli of Pod app-cli-d97b4c84b-vbmsg</span></span></code></pre></div>
<p>The events sections displays some errors, especially around reporting
the fact that the CPU utilization was not computed, note that we can see
the <code>&lt;unknown&gt;</code> state in the table when we listed the
HPA object above. The message states that:
<code>failed to get cpu utilization: missing request for cpu in container app-cli of Pod app-cli-d97b4c84b-vbmsg</code>,
meaning that the pods themselves were not configured to have any CPU
resource limits, remember that above, we configured the auto-scaler,
however that only tells OpenShift when to scale the pods, but since the
pods have to resources restriction, how would any percentage of the CPU
utilization be computed, we have to set a CPU utilization as well</p>
<p>In OpenShift a resource request is a threshold you can set that
affects scheduling and quality of service. It essentially provides the
minimum of resources guaranteed to the pod. For example a user can set a
CPU request of four-tenths of a core written - 400
<code>milli cores</code> or 400m. This tells OpenShift to schedule the
pod on nodes that can guarantee that three will always be at least 400m
of CPU time. CPU is measured in units called milli cores. By default
pods do not get individual cores they get time slices of CPU sharing the
cores on the node with other pods. If a particular node has four CPU
assigned to it, then 4000m are available, to all the running pods in
that node.</p>
<p>Resource requests also can be combined with a resource limit which is
similar to a request but sets the maximum amounts of resources
guaranteed to the pod. Setting requests and limits also allows the user
to set a quality of service level by default.</p>
<ul>
<li><code>BestEffort</code> - Neither a resource nor a limit is
specified this is for low priority apps that can live with very low
amounts of processing and memory resources.</li>
<li><code>Burstable</code> - a request is set, indicating a minimum
amount of resources allocated to the pod</li>
<li><code>Guaranteed</code> - a request and a limit are both set to the
same number. This is for the highest priority apps that need the most
consistent amount of computing power</li>
</ul>
<p>Setting a lower quality of service gives the schedule more
flexibility by allowing it to place more pods in the cluster. Setting a
higher quality of service limits flexibility but, give apps more
consistent resources. Because choosing the quality of service is about
finding reasonable defaults most app should fall into the
<code>Burstable</code></p>
<p>Setting a CPU request, can be done by using the
<code>oc set resources deployment/app-cli --requests=cpu=400m</code>. As
with other changes to the deployment, config, this results in a new
deployment config object, it will in turn create new pods which will
then replace the current ones once the set of new pods produced by the
new deployment go into the ready state. Now we can list again the HPA
objects and see if there are any issues. After we have run the above, we
should now be able to test the auto scaling</p>
<h3 id="testing-the-autoscaling-setup">Testing the autoscaling
setup</h3>
<p>To demonstrate that autoscaling works as expected you need to trigger
the CPU threshold that we have previously set. To help reach this mark
use the Apache benchmark instance that comes pre-installed with
<code>CentOS</code>, and is already available in your path. Before you
run the benchmarking test, make sure you are logged in the open shift
console in another window, so you can switch over to see pods being spun
up and down. Then go to the overview page for the
<code>image-uploader</code> project and run the command in the
following:</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will execute a total number of 50000 requests towards the pod, which will certainly cause overwhelming CPU usage,</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and force the OpenShift and monitoring service to scale more pods</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ab <span class="at">-n</span> 100000 <span class="at">-c</span> 1000 http://app-cli-image-uploader.apps-crc.testing:8080/</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># now if you are lucky and the threshold was hit we can describe the HPA object, and see the following in the events</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># section, which shows that the pods were auto-scaled to 3 and then 4 based on CPU utilization requirements, the actual</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># number will really depend on your local systems capabilities and such</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe hpa/app-cli</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Normal</span> SuccessfulRescale 105s horizontal-pod-autoscaler  New size: 3<span class="kw">;</span> <span class="ex">reason:</span> cpu resource utilization <span class="er">(</span><span class="ex">percentage</span> of request<span class="kw">)</span> <span class="ex">above</span> target</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Normal</span> SuccessfulRescale 105s horizontal-pod-autoscaler  New size: 4<span class="kw">;</span> <span class="ex">reason:</span> cpu resource utilization <span class="er">(</span><span class="ex">percentage</span> of request<span class="kw">)</span> <span class="ex">above</span> target</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># eventually those will be down scaled, but that would take time, this is described in the following section, which resolves the issue with</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># thrashing, the process of spinning up and down pods way too quickly over a short period of time , which OpenShift tries to avoid, do not</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co"># wonder why those 3 or 4 pods that were auto-scaled might stay 3 or 4 and take some time to get back to the original 2 or 1 replicas that</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="co"># your deployment describes</span></span></code></pre></div>
<h3 id="avoiding-thrashing">Avoiding thrashing</h3>
<p>When the Apache benchmark test kicked off, the OpenShift auto-scaler
detected very high CPU usage on the deployed pods which violated the HAP
constraints. This caused a new pods to be spun up on demand. Behind the
scenes the deployment was modified creating a new number of replicas.
After the tests were completed the CPU usage on the pods went back when
the CPU spiked and the new pods spun up quickly, it tool several minutes
for new pods to spin down. By default HAP synchronizes with the
Prometheus metrics every 30 seconds. You can modify this sync period in
the <code>master-config.yml</code></p>
<p>This time window is by design to avoid something called thrashing, in
OpenShift that is the constant starting and sopping of pods
unnecessarily, Thrashing can cause wasted resource consumption because
deploying new pods uses resources to schedule and deploy a pod on a new
node, which often includes things like loading apps and libraries into
memory. After OpenShift triggers an initial scale there is a forbidden
window of time to prevent trashing. The rationale is that in practice if
there is a need to constantly scale up and scale down within a matter of
minutes, it is probably less expensive to keep the pods running than it
is to continuously trigger scaling changes. In versions of OpenShift up
to 3.6, the forbidden window is hard coded at 5 minutes to scale down
the pods and 3 minutes to scale up the pods, in OpenShift versions
higher than that, the default values are still 5 and 3 minutes
respectively, but they can be modified via the
<code>controllerManagerArgs</code>, and the
<code>horizontal-pod-autoscaler-downscale-delay</code>.</p>
<h2 id="continuous-integration-deployment">Continuous integration &amp;
deployment</h2>
<p>Deploying software into production is difficult one major challenge
is adequately testing apps before they make it into production. And
adequate testing requires one of the longest standing challenges in IT -
consistent environments. For many organizations, it is time consuming to
stand up new environments are for development, testing and QA and more.
When the environments are finally in place they are often inconsistent.
These inconsistencies develop over time due to poor configuration,
management, partial fixes and fixing problems upstream such as directly
making a patch in a production environment. Inconsistent environment can
lead to unpredictable software. To eliminate the risk, organizations
often schedule maintenance window during software deployments and then
cross their fingers.</p>
<p>Over the last 15 years there have been many attempts to improve
software processes. Most notable has been the industry wide effort to
move from the waterfall method of deploying software to flexible
approaches such as Agile that attempts to eliminate risk of performing
many small iterative deployments as opposed to the massive software
rollouts common with the waterfall method. But Agile falls short in
several areas, because it focuses on software development and does not
address the efficiency of the rest of the stakeholders in the
organization. For example code may get to operations mode quickly but a
massive bottleneck may result because operations now has to deploy code
more frequently.</p>
<p>Many organizations are trying to solve these problems with a modern
DevOps approach that brings together all the stakeholders to work
jointly throughout the software development lifecycle. DevOps is now
almost synonymous with automation, and Continuous integration (CI) &amp;
Continuous deployment (CD). Often shorthanded to CI/CD. The delivery
mechanism for implementing this is often called a software deployment
pipeline in addition to technology that is cheaper focused largely on
the technology aspect and how it related to containers</p>
<h3 id="container-images-are-the-centerpiece">Container images are the
centerpiece</h3>
<p>From a technology perspective containers are becoming the most
important technology in the software deployment pipeline. Developers can
code apps and services without the need to design or even care about the
underlying infrastructure. Operations teams can spend fewer resources
designing the installation of apps. Apps and services can easily be
moved not only between environments like QA testing and so on, in the
software development pipeline but also between on premises and public
cloud environments such as Amazon Web Services - <code>AWS</code>,
<code>Microsoft Azure</code> and
<code>Google Compute Platform (GCP)</code>.</p>
<p>When apps need to be modified developers package new container
images, which include the app, configuration and runtime dependencies.
The container then goes through the software deployment pipeline,
automated, testing and processing. Using container images in a software
deployment pipeline reduces risk because the exact same binary is run in
every environment. If a change need to be made then it begin in the
sandbox or development environments and the entire deployment process
starts over. Because running containers are created from the container
images, there is no such ting as fixing things upstream. If a developer
operator attempted to circumvent the deployment process and path
directly into production the change would not persist. The change must
be made to the underlying container image. By making the container the
centerpiece of the deployment pipeline system stability and app
resiliency are greatly increased. When failures occur, identifying
issues and rolling back software is quicker because the container can be
rolled back to a previous version. This is much different than in
previous approaches, where entire app servers and databases may need to
be reconfigured in parallel to make the rollback of the entire system
possible.</p>
<p>In addition, containers let developer run more meaningful testing
earlier in the development cycle, because they have environments that
mimic production, on their laptops. A developer can reasonable simulate
a production environment load and performance testing on the container
during development. The result is higher quality more reliable software
updated. Better more efficient, testing also leads to less work in
progress and fewer bottle necks, which means faster updates.</p>
<h3 id="promoting-images">Promoting images</h3>
<p>In this section you will build a full pipeline in OpenShift. To keep
the promise of using the same binary in every environment you will build
your image just once in your development environment. You will then use
image tagging to indicate that the image is ready to be promoted to
other projects. To facilitate this process you will use Jenkins and some
additional OpenShift concepts which you will learn about as you go.
Jenkins is an open source automation server that is commonly used as the
backbone for CI/CD pipelines because it has many plugins for existing
tools and technologies. Jenkins often becomes a Swiss army knife that is
used to integrate disparate technologies into one pipeline</p>
<h3 id="cicd1-creating-an-environment---todo-this-needs-rewrite">CI/CD:1
Creating an environment - TODO: this needs rewrite</h3>
<p>The first part of any CI/CD pipeline is the development environment.
Here, container images are built tested and then tagged if they pass
their tests. All container build happen in this environment. You will
use pre-built template to spin a up a simple app that runs on
<code>Python</code>, and uses <code>MongoDB</code>, as a database. The
template also provides an open source Git repository called
<code>Gogs</code>, which comes pre-installed with the app already in it.
<code>PostgresSQL</code>is also provided as a database for
<code>Gogs</code>.</p>
<p>This section will make heavy use of OpenShift templates to install
apps. An OpenShift template is essentially an array of objects, that can
be parameterized and pun up on demand. In most cases the API objects
created as part of the template are all part of the same app, but that
is not a hard requirement. Using OpenShift templates provides several
features that are not available if you manually import objects:</p>
<ul>
<li>Parametrized values can be provided at creation time.</li>
<li>Values can be created dynamically based on regex values, such as
randomly generated database password</li>
<li>Messages can be displayed to the user in the console or on the CLI.
Typically message include information on how to use the app</li>
<li>You can create labels that can applied to all objects in the
template.</li>
<li>Part of the OpenShift API allows templates to be instantiated
programmatically, and without a local copy of the template</li>
</ul>
<p>OpenShift comes with many templates out of the box that you can see
through the service catalog or by running
<code>oc get templates - n openshift</code>. To see the raw templates
files navigate to <code>/usr/share/openshit/examples</code></p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here is the abridged version of the list of the command mentioned above, that pulls the list of templates which are</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># installed in the default OpenShift distribution, note that this list is about 1/3rd of the default templates, but</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co"># there are even more on platforms like github distributed by regular people that can be installed and setup in your</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># OpenShift cluster instance, proprietary ones certainly also exist</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                                          DESCRIPTION                                                                        PARAMETERS        OBJECTS</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="ex">jenkins-ephemeral</span>                             Jenkins service, without persistent storage....                                    12 <span class="er">(</span><span class="ex">all</span> set<span class="kw">)</span>      <span class="ex">7</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="ex">jenkins-ephemeral-monitored</span>                   Jenkins service, without persistent storage. ...                                   13 <span class="er">(</span><span class="ex">all</span> set<span class="kw">)</span>      <span class="ex">8</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="ex">jenkins-persistent</span>                            Jenkins service, with persistent storage....                                       14 <span class="er">(</span><span class="ex">all</span> set<span class="kw">)</span>      <span class="ex">8</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="ex">jenkins-persistent-monitored</span>                  Jenkins service, with persistent storage. ...                                      15 <span class="er">(</span><span class="ex">all</span> set<span class="kw">)</span>      <span class="ex">9</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="ex">mariadb-ephemeral</span>                             MariaDB database service, without persistent storage. For more information ab...   8 <span class="er">(</span><span class="ex">3</span> generated<span class="kw">)</span>   <span class="ex">3</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="ex">mariadb-persistent</span>                            MariaDB database service, with persistent storage. For more information about...   9 <span class="er">(</span><span class="ex">3</span> generated<span class="kw">)</span>   <span class="ex">4</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="ex">mysql-ephemeral</span>                               MySQL database service, without persistent storage. For more information abou...   8 <span class="er">(</span><span class="ex">3</span> generated<span class="kw">)</span>   <span class="ex">3</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="ex">mysql-persistent</span>                              MySQL database service, with persistent storage. For more information about u...   9 <span class="er">(</span><span class="ex">3</span> generated<span class="kw">)</span>   <span class="ex">4</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="ex">nginx-example</span>                                 An example Nginx HTTP server and a reverse proxy <span class="er">(</span><span class="ex">nginx</span><span class="kw">)</span> <span class="ex">application</span> that ser...   10 <span class="er">(</span><span class="ex">3</span> blank<span class="kw">)</span>      <span class="ex">5</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="ex">nodejs-postgresql-example</span>                     An example Node.js application with a PostgreSQL database. For more informati...   18 <span class="er">(</span><span class="ex">4</span> blank<span class="kw">)</span>      <span class="ex">8</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="ex">nodejs-postgresql-persistent</span>                  An example Node.js application with a PostgreSQL database. For more informati...   19 <span class="er">(</span><span class="ex">4</span> blank<span class="kw">)</span>      <span class="ex">9</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="ex">openjdk-web-basic-s2i</span>                         An example Java application using OpenJDK. For more information about using t...   9 <span class="er">(</span><span class="ex">1</span> blank<span class="kw">)</span>       <span class="ex">5</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="ex">postgresql-ephemeral</span>                          PostgreSQL database service, without persistent storage. For more information...   7 <span class="er">(</span><span class="ex">2</span> generated<span class="kw">)</span>   <span class="ex">3</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="ex">postgresql-persistent</span>                         PostgreSQL database service, with persistent storage. For more information ab...   8 <span class="er">(</span><span class="ex">2</span> generated<span class="kw">)</span>   <span class="ex">4</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="ex">rails-pgsql-persistent</span>                        An example Rails application with a PostgreSQL database. For more information...   23 <span class="er">(</span><span class="ex">4</span> blank<span class="kw">)</span>      <span class="ex">9</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="ex">rails-postgresql-example</span>                      An example Rails application with a PostgreSQL database. For more information...   22 <span class="er">(</span><span class="ex">4</span> blank<span class="kw">)</span>      <span class="ex">8</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="ex">react-web-app-example</span>                         Build a basic React Web Application                                                9 <span class="er">(</span><span class="ex">1</span> blank<span class="kw">)</span>       <span class="ex">5</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a><span class="ex">redis-ephemeral</span>                               Redis in-memory data structure store, without persistent storage. For more in...   5 <span class="er">(</span><span class="ex">1</span> generated<span class="kw">)</span>   <span class="ex">3</span></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="ex">.....</span></span></code></pre></div>
<p>At the command line lets create your development environment, by
running the following, this will create the project and also on top of
that will make sure to setup the necessary template to our CI/CD
pipeline.</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first create the new project, that will be used throughout this section</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-project dev <span class="at">--display-name</span><span class="op">=</span><span class="st">&quot;ToDo App - DEV&quot;</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to configure the template, this template is included in a file which we can simply apply</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc create <span class="at">-f</span> openshift/dev-todo-app-template.json <span class="at">-n</span> dev</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># now to instantiate the template itself we can simply do this will basically create a new app using all the objects</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># defined in the template, the template is nice because we can re-use it to duplicate apps easily, or use it as a way to</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># replicate common app setups and configurations</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-app <span class="at">--template</span><span class="op">=</span><span class="st">&quot;dev-todo-app-flask-mongo-gogs&quot;</span></span></code></pre></div>
<h3 id="deployment-strategies">Deployment strategies</h3>
<p>So far in this section, you have learned how to build a container
image and automate the promotion, of that image across different
environments using native OpenShift automation in addition to Jenkins
integration. But we have not discussed the exact sequence for ow the new
version of the application is rolled out, The way you update your
application in OpenShift is called a deployment strategy it is a
critical component of supporting a wide variety of application in the
platform. OpenShift supports several deployment strategies including the
following:</p>
<ul>
<li><p>Rolling - the default strategy. When pods consisting of the new
image become ready by passing their readiness checks, they slowly
replace the old images, one by one, setting this deployment strategy is
done in the deployment config object</p></li>
<li><p>Re-create - scales down to zero pods consisting of the old image
and then begin to deploy the new pods. This strategy has the cost of a
brief downtime while waiting for the new pods to be spun up. Similar to
rolling in order to use this strategy it must be set in the deployment
configuration object.</p></li>
<li><p>Blue/Green focuses on reducing risk by standing up the pods
consisting of new images while the pods with the old images remain
running, this allows the user to test their code in a production
environment. When the code has been fully tested all new requests are
sent to the new deployment. OpenShift implements this strategy using
routes.</p></li>
<li><p>Canary - adds checkpoints to the blue/green strategy by rolling
out a fraction of the new images at a time and stopping. The user can
test the application adequately before rolling out more pods. As with
blue/green deployments this strategy is implemented using OpenShift
routes.</p></li>
<li><p>Dark launches - rolls out new code but does not make it available
to users. By testing how the new code works in production, the users can
then later enable the features when its determined to be safe. This
strategy has been made popular at place like Facebook and Google. To
accomplish dark launches the application code must have checks for
certain environment variables that are used to enable the new features.
In OpenShift you can take advantage of that code by toggling the new
features on or off by setting the appropriate environment variables for
the application deployment.</p>
<p>There are many considerations for choosing a deployment strategy. The
rolling strategy upgrades your application the most quickly while
avoiding downtime, but it runs your old code side-by-side with your new
code. For many stateful application such as clustered application and
databases this can be problematic. For example imagine that your new
deployment has any of the following characteristics</p></li>
<li><p>It is rolling out a new database schema</p></li>
<li><p>it has a long running transaction</p></li>
<li><p>it shared persistent storage among all the pods in the
deployment</p></li>
<li><p>it uses clustering to dynamically discover other pods,</p></li>
</ul>
<p>In these cases it makes sense to use a re-create strategy instead of
a rolling update. Databases almost always use the re-create strategy.
You can check the strategy for the pod by doing a quick inspect of the
deployment object for the target app. Stateless applications is a good
fir for a rolling upgrade whereas databases make more sense to run using
the re-create strategy. Both the rolling and the re-create strategies
have extensible options including various parameters to determine the
timing of the rollouts they also provide lifecycle hooks which allow
code to be injected during the deployment process. Many users also
choose to add blue green and canary style deployment strategies by
combining the power of OpenShift routes with a rolling update or re
create strategies, for application using rolling deployment adding a
blue/green or canary style deployment allows the OpenShift user to
reduce risk by providing a more controlled rollout using checkpoints.
For application using the re-create deployment strategy adding
blue/green or canary features lets the application avoid downtime. Both
the blue/green and canary deployments use OpenShift routes to manage
traffic across multiple services. To implement these deployment
strategies an entire copy of the application is created with the new
code. This copy includes the objects to run the application the
deployment, service, replication controller, pods and so on. When
adequate testing has been performed on the new code, the OpenShift route
is patched to point to the service containing the new code. A blue/green
deployment has the added benefit of testing code in production and
because the old code is still running the app can be rolled back to the
old code, if something breaks. Once downside to using blue/green
deployments is that they require more infrastructure, because your
application needs to double the resources while both versions of the
code are running at the same time. A canary deployment is similar to a
blue/green one except that whereas blue/green switches the route between
services all at once, canary uses, weights to determine what percentage
of traffic should go to the new and old services. You can modify the
weights for the rollout using the OpenShift command line interface tool
or the console.</p>
<p>Here is a brief summary of the topics we have covered, we can say:
That image streams enable automation and consistency for container
images, you can use OpenShift triggers for even based image builds and
deploys you can use DNS for service discovery. You can use environment
variables for service discovery if dependencies are installed first.
Image tagging automates the promotion of images between environments.
Secrets mask sensitive data that needs to be decoupled from an image.
Config maps provide startup arguments environment variables or files
mounted in an image, OpenShift provides a Jenkins instance or a template
with many useful plugins pre-installed, allowing regular Jenkinsfile
pipelines to be executed and monitored form the OpenShift console,
OpenShift supports many types of deployment strategies for a wide
variety of applications.</p>
<h2 id="stateful-applications">Stateful applications</h2>
<p>The very first application we have deployed in OpenShift, the php web
app that we have deployed, is the <code>image uploader</code>, Here are
a few additional application features:</p>
<ul>
<li>Shows you the full size image when you click it.</li>
<li>Shows you these images as thumbnails on the application page</li>
<li>Uploads images from your workstation</li>
<li>Verifies that what you are uploading is a standard image format</li>
</ul>
<p>It is not the next Instagram, but it is a simple enough to live in a
couple of files of source code and be easy to edit and manipulate in
OpenShift. You will use that simplicity to your advantage in this
section. If you have not already go ahead and test out the image app,
and upload a few images. After you do that your application should show
these images on the home page as thumbnails</p>
<p>When you deploy an application in OpenShift you can specify the
minimum number of replicas instances of the application to keep running
all the times If you do not specify a number, OpenShift will always keep
at least one instance of your app running at all times. We initially
discussed this in earlier sections, and used the feature in a more
recent sections to scale up the image to more than one replicas. None of
these replicas had persistent storage though. If one of the application
pods was deleted or scaled down any data it had written would be gone as
well. We can test this one.</p>
<h3 id="container-storage-1">Container storage</h3>
<p>After logging into your OpenShift cluster from the command line with
the oc command line tool you can use the <code>oc get pods</code>
command to get a list of all your running pods. The output of that will
show that the app-cli has a few running pods, and a few completed ones,
which are the build ones. The running ones are the actual application
pods, as we already know. After that, lets delete one of the pods, or
even both, using the <code>oc delete pod &lt;pod-id&gt;</code></p>
<p>Even after we delete one OpenShift will make sure that the desired
state, meaning at least 2 replicas is fulfiled, therefore it will scale
up the application, creating a new pod in place of the one we have just
deleted, If we run the <code>oc get pods</code> one more time we will
see that the new pod has a new <code>age</code> which is about 10 or so
seconds, this is the time it took us to list the pods again, and in
between that time OpenShift already created the new pod in place of the
one we deleted</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here is a short snippet that demonstrates the steps from above.</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc login <span class="op">&lt;</span>cluster-ip<span class="op">&gt;</span> -u <span class="op">&lt;</span>kubeadmin<span class="op">&gt;</span> -p <span class="op">&lt;</span>adminpassword<span class="op">&gt;</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc delete pod</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span></code></pre></div>
<p>Now that would seem to be the answer to just about everything
wouldn’t it. Applications automatically restart themselves when they go
down. But before we close up shop take another look at your application
web page. We can see that the images we have initially uploaded earlier
are nowhere to be found , When a pod is deployed in OpenShift the
storage that it used for the file system is ephemeral - it does not
carry over from one instance of the application to the next. When
application needs to be more permanent, you need to set up a persistent
storage for use in OpenShift</p>
<h2 id="handling-permanency">Handling permanency</h2>
<p>In OpenShift persistent storage is available for data that needs to
be shared with other pods or needs to persist pas the lifetime of any
particular pod. Creating such permanent storage in pods is handles by
persistent volumes - PV. These <code>PVs</code> in OpenShift use
industry standard network based storage solutions to manage persistent
data. OpenShift can use a long list of storage solutions to create
persistent volumes - including, but not limited to the following:</p>
<ul>
<li><p><code>CSI</code> - the container storage interface defines a
standard interface for container orchestration systems, like Kubernetes,
to expose arbitrary storage systems to their container workloads. Once a
CSI compatible volume driver is deployed on a Kubernetes cluster, users
may use the CSI volume type to attach or mount the volumes exposed by
the CSI driver. A CSI volume can be used in a Pod just like any other,
we can have the pod reference a persistent volume claim, which itself is
referencing a persistent volume which itself defines the CSI driver to
use.</p></li>
<li><p><code>NFS</code> - these types of volumes allow an existing NFS
(Networking File System) share to be mounted into a Pod, Unlike
<code>emptyDir</code> which is erased when a Pod is removed the contents
of an NFS volume are preserved and the volume is merely unmounted. This
means that an NFS volume can be pre-populated with data and that data
can be shared between pods. NFS can be mounted by multiple writers
simultaneously. You must have your own NFS server running with the share
exported before you can use it, you can not specify NFS mount options in
the Pod spec, these have to be specified in the persistent volume
document itself, for the volume.</p></li>
<li><p><code>HostPath</code> - local directories on the OpenShift nodes
themselves, which are NOT ephemeral, <code>hostPath</code> volume mounts
a file or directory from the host node file system into your pod. That
is not something that most Pods will need, but it offers a powerful
escape hatch for some application</p></li>
<li><p><code>Local</code> - a local volume represents a mounted local
storage device such as a disk partition or directory, Local volumes can
only be used as statically created persistent volume objects, dynamic
provisioning is not supported, compared to <code>hostPath</code>
volumes, local volumes are used in a durable and portable manner without
manually scheduling pods to nodes. These system is aware of the volume’s
node constraints by looking at the node affinity on the Persistent
volume, however local volumes are subject to the availability of the
underlying node and are not suitable for all applications. The pod using
this volume is unable to run. Applications using the local volumes must
be able to tolerate this reduced availability as well as potential data
loss, depending on the durability of the underlying disk</p></li>
</ul>
<p>Note that using the <code>HostPath</code> volume type presents many
security risk. If you can avoid using a <code>hostPath</code> volume you
should. For example define a local <code>PersistentVolume</code>
instead, and use that. If you are restricting the access to specific
directories on the node, using admission time validation, that
restriction is only effective when you additionally require that any
mounts of that <code>hostPath</code> volume are read only. If you allow
a read-write mount of any host path by an untrusted Pod, the containers
in that pod may be able to subvert the read-write host mount. Take care
when using <code>hostPath</code> volumes, whether these are mounted as
read-only or as read-write because:</p>
<ul>
<li>Access to the host file system can expose privileged system
credentials such as for the kubelet or privileged API keys.</li>
<li>Pod with identical configuration such as created from a Pod
template, may behave differently on different nodes due to different
file on the nodes</li>
<li><code>hostPath</code> volume usage is not treated as ephemeral
storage usage. You need to monitor the disk usage by yourself because
excessive <code>hostPath</code> disk usage will lead to disk pressure on
the node</li>
</ul>
<p>In the next chapter we will configure a persistent volume in
OpenShift using the network file system - HostPath</p>
<h2 id="creating-resources">Creating resources</h2>
<p>In OpenShift they rely on the listed types of network storage to make
the storage available across all nodes in a cluster. For the examples in
the next few sections you will use a persistent volume built with NFS
storage. First we have to export a NFS volume on your OpenShift master.
As we discussed early on in the sections your OpenShift cluster is
currently configured to allow any user to log into the system as long as
their password is not empty. Each new username is added to a local
database, at first login. You created a user named dev and used that
user to create a project and deployed the app-cli. The dev user can
create projects and deploy apps but it does not have the proper
permissions to make a cluster wide changes like attaching a persistent
volume. We will take a much deeper look at how users are managed in
OpenShift in future sections, but to create a persistent volume we need
an admin level access user in the OpenShift cluster, luckily we can get
that.</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># just to ensure we do not run into issues with some commands</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-i</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># list the available disks, this is just for information purposes, we can see that we have one main partition, which</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co"># is vda4, the rest is system storage, for the virtual machine, such as /boot</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> lsblk</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda1</span> 252:1    0    1M  0 part</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda2</span> 252:2    0  127M  0 part</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="ex">├─vda3</span> 252:3    0  384M  0 part /boot</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="ex">└─vda4</span> 252:4    0 99.5G  0 part /var/lib/kubelet/pods/6994529f-6383-4141-8f1c-48cd7189ee3c/volumes/kubernetes.io~csi/pvc-3849f616-7f27-47d4-b0f8-7c98897aa529/mount</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/var/lib/kubelet/pods/14c7b420-5ed8-49d6-829f-af7f14474200/volume-subpaths/nginx-conf/networking-console-plugin/1</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/var</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/sysroot/ostree/deploy/rhcos/var</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/usr</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/etc</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>                                <span class="ex">/sysroot</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># make the directory where we will store the network file system data, also remount /var to be extra sure</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mount <span class="at">-o</span> remount,rw /var</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mkdir /var/nfs-share</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="co"># we care about the block identifier for our storage, in this case as already mentioned that would be vda4</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> blkid <span class="kw">|</span> <span class="fu">grep</span> <span class="at">-i</span> vda4</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="ex">/dev/vda4:</span> LABEL=<span class="st">&quot;root&quot;</span> UUID=<span class="st">&quot;3ee0fdbd-7a71-4158-a0f8-db54b07fa6af&quot;</span> TYPE=<span class="st">&quot;xfs&quot;</span> PARTLABEL=<span class="st">&quot;root&quot;</span> PARTUUID=<span class="st">&quot;a8432eef-31a4-7b42-a1fd-768a79c7c61d&quot;</span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="co"># check the firewall rules from iptable, if you do not get any output, the second command, that will expose the NFS</span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="co"># server port in the firewall rules</span></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> iptables <span class="at">-L</span> <span class="at">-v</span> <span class="at">-n</span> <span class="kw">|</span> <span class="fu">grep</span> 2049</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> iptables <span class="at">-I</span> INPUT <span class="at">-p</span> tcp <span class="at">--dport</span> 2049 <span class="at">-j</span> ACCEPT</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="co"># NFS is actually a collection of four services that you need to enable and start:</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="co"># - rpcbind—NFS uses the RPC protocol to transfer data.</span></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a><span class="co"># - nfs-server—The NFS server service.</span></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="co"># - nfs-lock—Handles file locking for NFS volumes.</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a><span class="co"># - nfs-idmap—Handles user and group mapping for NFS volumes.</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a><span class="ex">systemctl</span> enable rpcbind nfs-server nfs-lock nfs-idmap<span class="kw">;</span></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="ex">systemctl</span> start rpcbind nfs-server nfs-lock nfs-idmap<span class="kw">;</span></span></code></pre></div>
<h3 id="creating-storage">Creating storage</h3>
<p>Now that we have created the persistent volumes it is time to take
advantage of them in OpenShift application consume persistent storage
using persistent volume claims. A persistent volume claim - PVC - can
bedded into an application as volume using the command line or through
the web interface, let us create one PVC first, on the command line and
add it to the application</p>
<p>The way the persistent volume claims match up with persistent
volumes, first you need to know how persistent volumes and persistent
volume claims match up to each other. In OpenShift PV represent the
available storage, while the PVC, represent an application need for that
storage - the claim for the storage. When you create a PVC, OpenShift
look for the best fit among the available PV and reserves it for use by
the PVC. In the example environment matches are based on two criteria
:</p>
<ul>
<li><p>Persistent Volume Size - OpenShift tries to take best advantage
of available resources, when a PVC is created it reserves the smallest
PV available that satisfies its needs.</p></li>
<li><p>Data Access Mode - when a PVC is created OpenShift look for an
available PV with at least the level of access required if an exact
match is not available it reserves a PV with more privileges that still
satisfies the requirements for example if a PVC is looking for a PV with
a RWO (read/write) access mode it will use a PV with a RWX (read/write
many) access mode if one is available.</p></li>
</ul>
<p>Because all the persistent volumes in your environment are the same
size matching them to a PVC will be straightforward, next you will
create a PVC for your application to use.</p>
<p>First have to enable the NFS capabilities on the server, meaning that
we have to create, and then enable the mount paths, that we are going to
be using in the persistent volume. The script below does just that, you
will notice that it creates 5 directories which can be used by 5
different PVC, they are simply named starting from <code>pv01</code> to
<code>pv05</code>, and are under the directory -
<code>/var/nfs-share</code>.</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/sh</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a file with the following contents, on the crc node itself, name it nfs.sh, and execute it, make the file</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># executable with chmod # +x nfs.sh, run this script as sudo, like so sudo ./nfs.sh. This will make the needed</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co"># directories and export them as nfs volumes which can later on be mounted and used by our persistent volume and the</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># persistent claims</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="va">NUM_PVS</span><span class="op">=</span>5</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="va">PREFIX</span><span class="op">=</span><span class="st">&quot;app-cli&quot;</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># go over the predefined number of persistent volumes to create, make a new directory for each one of them, under the</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co"># specified path, note that this path is matched exactly in the creation of persistent volume object above</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="va">$(</span><span class="fu">seq</span> <span class="at">-f</span> <span class="st">&quot;%02g&quot;</span> 1 <span class="va">${NUM_PVS})</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="cf">do</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mkdir</span> <span class="at">-p</span> <span class="st">&quot;/var/nfs-share/</span><span class="va">${PREFIX}</span><span class="st">-pv</span><span class="va">${i}</span><span class="st">&quot;</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">echo</span> <span class="st">&quot;/var/nfs-share/</span><span class="va">${PREFIX}</span><span class="st">-pv</span><span class="va">${i}</span><span class="st"> *(rw,sync,no_root_squash)&quot;</span> <span class="op">&gt;&gt;</span> /etc/exports</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure that the permissions for the directories are lax, to ensure that the directories can be written to and read</span></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="co"># from, otherwise the container might not be able to save or read any data at all</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> <span class="at">-R</span> 777 /var/nfs-share</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a><span class="fu">chown</span> <span class="at">-R</span> nfsnobody:nfsnobody /var/nfs-share</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a><span class="co"># show what was exported at the end, you should see all the directories that we exported above, note that these are</span></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># also going to be used later on in the manifest files for storage</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a><span class="ex">exportfs</span></span></code></pre></div>
<div class="sourceCode" id="cb46"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure to make the script executable after which use sudo to execute the script as shown below</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> chmod +x nfs.sh</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo ./nfs.sh</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># to checkout what was exported, you can cat out the file exports, and see all the volumes in there</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat /etc/exports</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/nfs-share/app-cli-pv01</span> <span class="pp">*(</span>rw,sync,no_root_squash<span class="pp">)</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/nfs-share/app-cli-pv02</span> <span class="pp">*(</span>rw,sync,no_root_squash<span class="pp">)</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/nfs-share/app-cli-pv03</span> <span class="pp">*(</span>rw,sync,no_root_squash<span class="pp">)</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/nfs-share/app-cli-pv04</span> <span class="pp">*(</span>rw,sync,no_root_squash<span class="pp">)</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/nfs-share/app-cli-pv05</span> <span class="pp">*(</span>rw,sync,no_root_squash<span class="pp">)</span></span></code></pre></div>
<h3 id="logging-in-as-kubeadmin">Logging in as
<code>kubeadmin</code></h3>
<p>When an OpenShift cluster is installed it creates a config file for a
special user names kubeadmin or system:admin, depending on which
OpenShift distribution we are using. The admin user is authenticated
using an SSL certificate regardless of the authentication provider that
is configured. Admin user has full administrative privileges on an
OpenShift cluster. The key certificate for admin are placed in the
Kubernetes config files in <code>~/.kube</code>, when the OpenShift
cluster is installed., this makes it easier to run commands as admin. It
is also possible to list the credentials of the users from the command
line directly, or you can even observe the credentials being logged out
to the terminal when the cluster is being started</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a># you might see something like this when the cluster is being started for the first time, otherwise refer to the help</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a># documentation of the cluster tool you are using, that will point you to the right sub command to use to correctly</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a># extract the credentials for the admin user</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>The server is accessible via web console at:</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>  https://console-openshift-console.apps-crc.testing</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>Log in as administrator:</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>  Username: kubeadmin</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>  Password: pottH-ZrwmV-KscNd-CZheg</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>Log in as user:</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>  Username: developer</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>  Password: developer</span></code></pre></div>
<p>The example above, shows the example output for the <code>crc</code>
deployment method of the OpenShift cluster, but similar output will be
observed for the <code>minishift</code> deployment method, you have to
take a note of the IP address of the cluster, since that will your point
of entry to the API server and the user interface and console</p>
<p>If you are unable to locate the password for the admin or developer,
and you are using the <code>crc</code> or <code>minishift</code>
methods, checkout your home directory for folders named
<code>.crc</code> and <code>.minishift</code>, therein you will be able
to find the details about the credentials stored in files. These are
usually located in the following locations on your host machine -
<code>$HOME/.crc/machines/crc/kubeadmin-password</code> or
<code>$HOME/.minishift/machines/minishift/TODO</code></p>
<h3 id="physical-volume">Physical volume</h3>
<p>To create a resources from a YAML template use the
<code>oc create</code> or <code>oc apply</code> command along with the
<code>-f parameter</code>, which will specify the template file you want
to apply or process. To create the persistent volume for this example
you will use the template block above, save it to a file, and run the
command. Therefore the command might look something like that -
<code>oc apply -f &lt;directory/pv01.yml&gt;</code>. What are some of
the options used in the template above.</p>
<p>The Access mode of the persistent volume, dictates how the different
pods can access the persistent volume we have created, this is mostly to
be able to reasonably deal with race conditions and resource locks, that
might occur while data is being accessed - in any capacity, read or
write</p>
<ul>
<li><p>Read/Write (RWO) - This volume can be mounted as read write by a
single node, in the OpenShift cluster, this is a useful when you have
workloads where a single application pod will be writing data. An
example is a relational database, when you know that all the writes to
the persistent data will come from a single pod, that would be the pod
that is running database server/service</p></li>
<li><p>Read/Only Many (ROX) - This is for volumes where the volume can
be mounted as read only by multiple OpenShift nodes. An example of where
this type of access mode is useful is when a horizontally scalable web
application needs access to the same static content such as
images.</p></li>
<li><p>Read/Write Many (RWX) - That access mode is the option you will
use for the persistent volume in this section. It allows multiple nodes
to mount this volume read from it and write to it. The
<code>image uploader</code> application is a good example. When you
scale up the application in the next section multiple nodes will need to
be able to read and write to the persistent storage you are about to
create</p></li>
</ul>
<p>A reclaim policy dictates how the persistent volume reclaims space
after a storage claim on the persistent volume is no longer required.
Two options are available:</p>
<ul>
<li>Retain - with this reclaim policy all data is retained in the
volume, reclaiming space is a manual process</li>
<li>Recycle - this reclaim policy automatically removed data when the
claim is deleted, this one is the one we will be using for the
persistent volume created for this section</li>
</ul>
<h3 id="creating-volumes">Creating volumes</h3>
<p>OpenShift makes extensive use of the configuration files written in
YAML. These files are a human readable language that is often used for
configuration files and to serialize data in a way that is easy for both
humans and computers to consume. YAML is the default way to push data
into and get data into Kubernetes and by proxy into OpenShift. In
previous sections we have talked about OpenShift resources that are
created when an application is built and deployed. These resources have
documented YAML formatted templates so you can create and manage the
resources easily. In later sections you will use several of these
templates to create or change resources in OpenShift. For the
application deployments you created in earlier sections, these templates
were automatically generated and stored by OpenShift, when we created
the new application or in other words when we run the
<code>new-app</code> command.</p>
<p>In this section you will use the template to create a persistent
volume, this template is more like a specification, that even provides a
version, that version tells Kubernetes which version of the
specification revision we are using to create the given resource object,
different revision versions might have some differences in the general
layout and structure of the YAML specification file.</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolume</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pv01</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">capacity</span><span class="kw">:</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 2Gi</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteMany</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">nfs</span><span class="kw">:</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="co">        # The IP address here refers to the IP address of the NFS server</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co">        # In our case we can use the address of the cluster node we</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co">        # can try nslookup from the node - nslookup api.crc.testing</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co">        # server: &lt;cluster-ip-address&gt;</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="co">        # path: &lt;path-inside-cluster-node&gt;</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">server</span><span class="kw">:</span><span class="at"> </span><span class="fl">192.168.127.2</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /var/nfs-share/app-cli-pv01</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">persistentVolumeReclaimPolicy</span><span class="kw">:</span><span class="at"> Recycle</span></span></code></pre></div>
<div class="sourceCode" id="cb49"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can now see the newly created persistent volume which points to the nfs-directory which would be setup in the</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co"># next section, make sure that these directories indeed exist after completing the next section where they are setup</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pv</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pv01</span>  2Gi        RWX            Recycle          Available</span></code></pre></div>
<p>So what type of information does this file or template contain
really:</p>
<ul>
<li><p>At the very start as we already mentioned, is the version of the
object, in this case it is under revision <code>v1</code>.</p></li>
<li><p>First it is the type of resource the template will create
different resources have different templates configurations in this case
you are creating a persistent volume</p></li>
<li><p>A name for the resource to be created, this is simply
<code>pv01</code>, this is mandatory from a Kubernetes point of view,
each object of a given type/kind needs a unique name</p></li>
<li><p>Storage capacity for the persistent volume, that will be created
measured in GB, in this example, each of the persistent volumes is
2GB</p></li>
<li><p>Next is the access mode of the volume, we will dig into deeper
later on</p></li>
<li><p>NFS path for this persistent volume</p></li>
<li><p>NFS server for this persistent volume , if you used another IP
address for your master or used another server, you will need to edit
this value,</p></li>
<li><p>Recycle policy for the persistent volume that will be created.
These policies dictate what and how data will be disposed of once it is
no longer being used by an application, we will dig deeper into those in
the next sections</p></li>
</ul>
<h3 id="creating-claims">Creating claims</h3>
<p>Now we will do something very similar to what we have already done
for the persistent volume, create it from a template, again using YAML
as the base manifest format, that will be fill up all the necessary
fields that are required to attach our PVC to the application, and by
proxy match that PVC with the persistent volume that we have already
created for the cluster. There are few important parameters to take a
note of:</p>
<ul>
<li><p>The name of the PVC to be created, this is important, as it is
core parameter for all Kubernetes and by proxy/extension OpenShift
objects, we can not really make one without an unique name</p></li>
<li><p>The access mode of the PVC, in this example the PVC will request
the RWX (read/write many) this aligns with the volumes we have already
crated in previous section, which were all RWX.</p></li>
<li><p>The size of the storage required, this example creates a 2GB
storage request which matches the size of the persistent volume that you
created in the previous section</p></li>
</ul>
<div class="sourceCode" id="cb50"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc01</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co">    # we are forcing this claim to bind to the pv01, this is usually not needed, but we would like to make sure that the</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co">    # claim will correctly be using the right persistent volume we have already created, the storageClassName is mandatory</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co">    # since otherwise this will error out on running apply</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumeName</span><span class="kw">:</span><span class="at"> pv01</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;&quot;</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteMany</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 2Gi</span></span></code></pre></div>
<p>We can then use the regular <code>oc</code> command to apply this
configuration, first save that one into a file, and then we can simply
run the <code>oc apply -f pv01.yml</code>, note that unlike the
persistent volumes we are not required to use an admin user, the volumes
are meant to be created by admins, but the claims are created for pods,
by developers, usually what happens when a new deployment config is
created is that the pod will refer to the volume claim by name, which
should be created by the developers first.</p>
<p>The one rule to remember is that the PVC needs to be created in the
same project as the project for which it will provide storage. When the
PVC is created it queries OpenShift to get available persistent volumes.
It uses the criteria described to find the best match and then reserves
that persistent volume, once that is done it can take a minute or so
depending on the size of your cluster, for the PVC to become available
to be used in an app as persistent storage. The following command shows
how you can use the <code>oc</code> command line tool to provide
information about the active persistent volume claims, in an OpenShift
project.</p>
<pre class="txh"><code># Note the following event, once the persistent volume claim is created, it will be lazily initialized, meaning that it
# will not be &#39;created&#39; per se, until at least one pod is actually using it, therefore it will stay in pending status
# until that happens

  Type    Reason                Age                  From                         Message
  ----    ------                ----                 ----                         -------
  Normal  WaitForFirstConsumer  7s (x14 over 3m16s)  persistentvolume-controller  waiting for first consumer to be created before binding</code></pre>
<div class="sourceCode" id="cb52"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that we have to first make sure that the currently set project is the one we are going to work with, either use</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the -n flag when calling the command below, or just simply use oc project &lt;project-name&gt;, to first set the currently</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># active project context</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pvc</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>          STORAGECLASS                   VOLUMEATTRIBUTESCLASS   AGE</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="ex">pvc-app-cli</span>   crc-csi-hostpath-provisioner   <span class="op">&lt;</span>unset<span class="op">&gt;</span>                 27s</span></code></pre></div>
<p>A PVC represents reserved storage available to the applications in
your project. But it is not yet mounted into an active application. To
accomplish that you need to mount your newly created PVC into an
application as a volume.</p>
<h3 id="modifying-deployment">Modifying deployment</h3>
<p>In OpenShift a volume is any filesystem file or data mounted into an
application pod to provide persistent data. In this section we are
concerned with persistent storage volumes. Volumes also are used to
provide encrypted data, application configuration and other types of
data as you saw in earlier sections. To add a volume you use the
<code>oc volumes command</code>, The following example takes the newly
created PVC and adds it into the app-cli application - the command is
broken over multiple lines to make it more readable and
understandable</p>
<div class="sourceCode" id="cb53"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that we need 3 primary things here, first is we need to tell it the type of volume, the name of the volume, and</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># where the volume will be mounted, meaning which path in the container will be linked to the volume, reading and writing</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># under that path will actually directly write/read to the persistent volume in this case the NFS server</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc set volume deployment/app-cli <span class="at">--add</span> <span class="dt">\</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">--claim-name</span><span class="op">=</span>pvc01 <span class="dt">\</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">--type</span><span class="op">=</span>PersistentVolumeClaim <span class="dt">\</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">--mount-path</span><span class="op">=</span>/opt/app-root/src/uploads</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co"># after this if we do describe the deployment we will be able to see the following output, the rest of it is removed, and only the volume information is shown, which allows us to see which claim our deployment is working with, and also the name of the volume which is as mentioned automatically generated</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe deployment/app-cli</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Volumes:</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>   <span class="ex">volume-rpw2z:</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Type:</span>          PersistentVolumeClaim <span class="er">(</span><span class="ex">a</span> reference to a PersistentVolumeClaim in the same namespace<span class="kw">)</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ClaimName:</span>     pvc01</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ReadOnly:</span>      false</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code></pre></div>
<p>By applying the volume to the deployment, for app-cli you can trigger
a redeployment of the application automatically to incorporate the new
persistent volume, the following parameters specified above are required
and mandatory, as mentioned in the comment, they define the minimal
number of known arguments that need to be specified for the volume to be
created for our application.</p>
<p>Optionally you can also specify a name of the volume itself, with the
–name parameter. If it is not set, OpenShift creates on dynamically,
typically simply as a sequence of letters and numbers</p>
<p>Using the <code>oc</code> describe command to list the details of the
deployment we can now see that the volume is attached to the deployment
and more precisely to the pod itself</p>
<div class="sourceCode" id="cb54"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can see below that the volume is attached to the pod, through the deployment</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe deployment/app-cli</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Here is an excerpt from from the describe above, we can see that the volume was created, and it was linked to the</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># persistent volume claim we have already created, and in turn that claim is linked to the persistent volume object</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># which is an NFS volume configured on the cluster node</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Volumes:</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>   <span class="ex">volume-plc22:</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Type:</span>          PersistentVolumeClaim <span class="er">(</span><span class="ex">a</span> reference to a PersistentVolumeClaim in the same namespace<span class="kw">)</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ClaimName:</span>     app-cli</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ReadOnly:</span>      false</span></code></pre></div>
<div class="sourceCode" id="cb55"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and this is how part of the spec of the deployment for the pod will look like, a volume is attached to the deployment, in this case a volume is given an automatic name, what is important here is that the volume is linked to a claim, the claim we already created above</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> volume-plc22</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> app-cli</span></span></code></pre></div>
<p>Because we have created the persistent volume and the claims to allow
for multiple reads and writes by using the RWX access mode, when this
application scale horizontally each new pod will mount the same PVC, and
be able to read data from and write data to it. To sum up you just
modified your containerized application to provide horizontally scalable
persistent storage.</p>
<p><code>Finally go into the application through your browser, visit the following web page for our image upload application - http://app-cli-image-uploader.apps-crc.testing/, upload a new image in there, and follow up with the next sections, remember up the name of the file you upload we will use that as reference to validate that indeed the file was correctly uploaded to the host / node, under the correct directory we specified in the persistent claim</code></p>
<h2 id="volume-mounts">Volume mounts</h2>
<p>Because we are using NFS server, exports as the source for the
persistent volume, it stands to reason that somewhere on the OpenShift
nodes, those NFS volumes are mounted and created. You can see that this
is the case by looking at the following example. SSH into the OpenShift
node, locally where the containers are running, run the mount command
and search for the mounted volumes from the IP address of the NFS
server. The IP address of the OpenShift cluster should be format of
<code>192.168.XXX.XXX</code></p>
<div class="sourceCode" id="cb56"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># after having ssh access to the node, we can run the mount command, replace the cluster-node-ipaddress with the ip</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># address of the cluster node, we can obtain the cluster node ip address as shown above, using nslookup against</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the api.crc.testing host, since the /etc/resolv.conf is configured with the cluster dns service, therefore the</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># host will be resolved to a correct host</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mount <span class="kw">|</span> <span class="fu">grep</span> 192.168.127.2</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="ex">192.168.127.2:/var/nfs-share/app-cli-pv01</span> on /var/lib/kubelet/pods/952b5bd5-c8f2-4da5-9b62-5b58d355f6e5/volumes/kubernetes.io~nfs/pv01 type nfs4 <span class="er">(</span><span class="ex">rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,ret</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="va">rans</span><span class="op">=</span>2,sec=sys,clientaddr=192.168.127.2,local_lock=none,addr=192.168.127.2<span class="kw">)</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="ex">192.168.127.2:/var/nfs-share/app-cli-pv01</span> on /var/lib/kubelet/pods/861fcda4-d641-4f8b-9bdf-8c012f5199f2/volumes/kubernetes.io~nfs/pv01 type nfs4 <span class="er">(</span><span class="ex">rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,ret</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="va">rans</span><span class="op">=</span>2,sec=sys,clientaddr=192.168.127.2,local_lock=none,addr=192.168.127.2<span class="kw">)</span></span></code></pre></div>
<p>You will get multiple results, each of those will be for each pod
that is running for the application, earlier in this section you used
the <code>oc get pv</code> command and confirmed that the PV was being
used by the app-cli, but that does not explain how the NFS volume is
made available in the app-cli container’s mount namespace.</p>
<p>Earlier in previous section, we had a look at how the file system in
a container is isolated from the rest of the containers that are running
on the cluster host, and the cluster host/node itself, through the use
of namespaces and more precisely the mount namespace which is one of the
corner stone namespaces that the Linux kernel uses when dealing with
containerized applications. The persistent volume mount however, is not
added to the mount namespace of the app-cli containers. Instead the NFS
mount is made available in the container using a technology called
<code>bind mount</code></p>
<p>A bind mount, in a Linux is a special type of mounted volume where
part fo the file system is mounted in a new new additional location. For
app-cli the NFS mount for the persistent volume is mounted using a bind
mount at /opt/app-root/src/uploads in the container mount namespace,
using ta bind mount makes the content available simultaneously in two
locations. A change in one location is automatically reflected in other
location.</p>
<p>Bind mounts are used for volumes for two primary reasons. First
creating a bind volume mount on a Linux system is a lightweight
operation in terms of performance and CPU requirements. That means
redeploying a new container to replace an old one does not involve
remounting a remote volume. This keeps container creation time low.</p>
<p>Second this approach separates concerns for persistent storage, using
bind mounted the container definition does not have to include specific
information about the remote volume. The container only needs to define
the name of the volume to mount. OpenShift abstract how to access the
remote volume and make it available in the containers. The separation of
concerns between administration and usage of a cluster is consistent
OpenShift design feature.</p>
<div class="sourceCode" id="cb57"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># finally to show the magic in play, from the cluster node, you can navigate to the directory /var/nfs-share/app-cli-pv01,</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co"># there you will find the uploaded file / image, now  we have successfully mounted a persistent volume claim from the</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># host through the use of a persistent volume object</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cd /var/nfs-share/app-cli-pv01 <span class="kw">&amp;&amp;</span> <span class="fu">ls</span> <span class="at">-la</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="ex">total</span> 3972</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxrwxrwx.</span> 2 nfsnobody  nfsnobody      48 Jun 11 14:23 .</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxrwxrwx.</span> 7 nfsnobody  nfsnobody     106 Jun 11 13:40 ..</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span> 1 1000660000 root      4063862 Jun 11 14:23 image.jpg</span></code></pre></div>
<p>The goal of this section was to walk you through configuring the
components that make a persistent storage available to a container in
OpenShift. In the following sections, we will use persistent storage to
create more scalable and resilient applications.</p>
<h1 id="stateful-applications-1">Stateful applications</h1>
<p>In previous sections we have created persistent storage for the image
upload pods, which allowed data to persist past the lifecycle of a
single pod. When a pod failed a new pod spun up in its place and mounted
the existing persistent volume locally. Persistent storage in OpenShift
allows many stateful applications to run in containers. Many other
stateful applications still have requirements that are unsatisfied by
persistent storage alone, for instance many workloads distribute data
through replication which required application level clustering. In
OpenShift this type of data replication requires direct pod to pod
networking without going through the service layer. It is also very
common for stateful applications such as databases to have their own
custom load balancing and discovery algorithms, which require direct pod
to pod access. Other common requirements for stateful applications
include the ability to support sticky services and sessions as well as
implement a predictable graceful shutdown.</p>
<p>One of the main goals of the OpenShift container platform is to be a
world class platform for stateless and stateful applications. In order
to support stateful applications a variety of tools are available to
make virtually any application container native. This chapter will walk
you through the most popular tools including headless services, sticky
sessions pod discovery techniques and stateful sets, just to name a few.
At the end of this section you will walk through the power the stateful
set, which bring many stateful applications to life on OpenShift</p>
<h3 id="enabling-a-headless-services">Enabling a headless services</h3>
<p>A good example of application clustering in everyday life is
demonstrated by amazon’s virtual shopping cart. Amazon customers browse
for items and add them to a virtual shopping cart so they can
potentially be purchased later. If an Amazon user is signed in to their
account their virtual shopping cart will be persisted permanently
because the data is stored in a database. But for users who are not
signed in to an account the shopping cart is temporary. The temporary
cart is implemented as in-memory cache in amazon’s data centers. By
taking advantage of an in-memory caching end users get fast performance
which results in a better user experience. Once downside of using
in-memory caching is that if a server crashed that data is lost. A
common solution to this problem is data replication - when an
application puts data in memory that data can be replicated to many
different caches, which result in fast performance and redundancy.</p>
<p>Before applications can replicate data among one another, they need a
way to dynamically find each other. In previous sections, this concept
was covered through the use of the service discovery in which pods use
OpenShift service object. The OpenShift service object provides a stable
IP and port that can be used to access one or more pods. For most cases
having a stable IP and port to access one or more replicated pods is all
that is required. But many types of applications such as thsoe that
replicate data require the ability to find all the pods in a service and
access each one directly, on demand.</p>
<p>One working solution would be to use a single service object for each
pod, giving the application a stable IP and port for each pod. Although
this works nicely, it is not ideal because it can generate many service
object which can become difficult to manage. A better solution is to
implement a headless service and discover the application pod using an
application specific discovery metrics. A headless service is a service
object that does not load balance or proxy between backend pods. it is
implemented by the setting - <code>spec.clusterIP</code> field to None,
in the service API object.</p>
<p>Headless services are most often used for application that need to
access specific pods directly without going through the service proxy.
Two common examples of headless service are clustered databases and
applications that have a client side load balancing login built into
them or the code. Later in this chapter we will explore an example of a
headless service using MongoDB a popular NoSQL database.</p>
<h3 id="application-clustering-with-wildfly">Application clustering with
WildFly</h3>
<p>In this section we will deploy a classic example of application level
clustering in OpenShift using WildFly, a popular application server for
Java based application runtimes. You will be deploying new application
as part of this chapter so create a new project as follows</p>
<div class="sourceCode" id="cb58"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-project stateful-apps</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Now</span> using project <span class="st">&quot;stateful-apps&quot;</span> on server <span class="st">&quot;https://api.crc.testing:6443&quot;</span>.</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="ex">You</span> can add applications to this project with the <span class="st">&#39;new-app&#39;</span> command. For example, try:</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="ex">oc</span> new-app rails-postgresql-example</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="ex">to</span> build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="ex">kubectl</span> create deployment hello-node <span class="at">--image</span><span class="op">=</span>registry.k8s.io/e2e-test-images/agnhost:2.43 <span class="at">--</span> /agnhost serve-hostname</span></code></pre></div>
<p>It is important to note that this new example uses cookies stored in
your browser to track your session, Cookies aer small pieces of data
that servers ask your browser to hold to make your experience better. In
this case a cookie will be stored in your browser with a simple unique
identifier, a randomly generated string called <code>JSESSIONID</code>.
When the user initially accesses the web application the server will
reply with a cookie containing that java session id field and a unique
identifier as the value. Subsequent access to the application will use
the <code>JSESSIONID</code> to look up all information about the user’s
session, which is stored in a replication cache. It doe not matter which
pod is accessed - the user experience will be the same</p>
<p>The WildFly application that you will deploy will replicate the user
data among all the pods in its service. The application will track which
user the request comes from by checking the <code>JSESSIONID</code>,
that is passed from the user’s browser cookie. Because the user data
will be replicated the end user will have consistent experience even if
some pods die and new pods are accessed. Run the following
<code>oc</code> command to create the new application.</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will create the template that we will use to create the application and all required dependencies below</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc create <span class="at">-f</span> openshift/stateful-app-template.yml <span class="at">-n</span> stateful-apps</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># to see the list of templates, we can now do the following, and use the name of our newly created template to create</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co"># the new application from it, as done below</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get templates</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co"># use the template to directly create the components of our new application, like so, specify the name of the template</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-app <span class="at">--template</span><span class="op">=</span><span class="st">&quot;wildfly-oia-s2i&quot;</span></span></code></pre></div>
<p>Now that the application is deployed let us explore application
clustering with WildFly on OpenShift. To demonstrate this we will do the
following</p>
<ul>
<li>add data to your session by registering users on the application
page</li>
<li>make a note of the pod name</li>
<li>scale the service to two replicated pods. The WildFly application
will then automatically replicate your session data in memory between
pods</li>
<li>delete the original pod</li>
<li>verify that your session data is still active</li>
</ul>
<h3 id="querying-the-openshift-server">Querying the OpenShift
server</h3>
<p>Before we get started we need to modify some permissions for the
default service account in your project which will be responsible for
running the application pods. From the stateful app project run the
following command to add the view role to the default service account.
The view role will allow the pods running in the project to query the
OpenShift API server directly. In this case the application will take
advantage of the ability to query the OpenShift API server to find other
WildFly application pods in the project. These other instances will send
pod-to-pod traffic directly to each other and use their own application
specific service discovery method and load balancing features for
communication</p>
<div class="sourceCode" id="cb60"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will make sure to add to the service account the view role, the service accounts are named based on the project</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># name and you can see that in our command below too, that would be system:serviceaccount:stateful-apps:default</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc policy <span class="dt">\</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    add-role-to-user <span class="dt">\</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    view <span class="dt">\</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    system:serviceaccount:<span class="va">$(</span><span class="ex">oc</span> project <span class="at">-q</span><span class="va">)</span>:default <span class="dt">\</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">-n</span> <span class="va">$(</span><span class="ex">oc</span> project <span class="at">-q</span><span class="va">)</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="ex">clusterrole.rbac.authorization.k8s.io/view</span> added: <span class="st">&quot;system:serviceaccount:stateful-apps:default&quot;</span></span></code></pre></div>
<p>Start by registering a few users in the WildFly application page.
List the pods for the project and remember their ids name. Then scale up
these pods with the</p>
<div class="sourceCode" id="cb61"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we first would like to the pods that we have, we can see that we have one running pod, which is where the application</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># lives, the rest are the build and deploy pods which are completed we do not care about them, we can see that the</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># application lives in a pod with id - wildfly-app-1-vr2kz</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   READY   STATUS             RESTARTS   AGE</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-build</span>    0/1     Completed          0          3h25m</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-deploy</span>   0/1     Completed          0          3h24m</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-vr2kz</span>    1/1     Running            0          3h24m</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># in between here, create a few Wildfly, accounts by visiting the application URL, first we can list the routes created</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co"># for our app, and then describe one of the routes, to find where this is exposed, after which action we can visit that</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co"># link</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get routes</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   HOST/PORT                                             PATH   SERVICES               PORT       TERMINATION     WILDCARD</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app</span>            wildfly-app-stateful-apps.apps-crc.testing                   wildfly-app            <span class="op">&lt;</span>all<span class="op">&gt;</span>                      None</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="co"># from the description of the route we can see the request host, which in this case point to</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># http://wildfly-app-stateful-apps.apps-crc.testing, visit that URL and create a few accounts first, so we can see how the</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="co"># data we create will persist over when later on we scale the pods</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe route route/wildfly-app</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                   wildfly-app</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>              stateful-apps</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>                 app=wildfly-oia-s2i</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>                        <span class="ex">app.kubernetes.io/component=wildfly-oia-s2i</span></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>                        <span class="ex">app.kubernetes.io/instance=wildfly-oia-s2i</span></span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>                        <span class="va">application</span><span class="op">=</span>wildfly-app</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>                        <span class="va">template</span><span class="op">=</span>wildfly-oia-s2i</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a><span class="ex">Requested</span> Host:         http://wildfly-app-stateful-apps.apps-crc.testing</span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a><span class="co"># we can now scale up the deployment/deployment-config, which will cause the old pod to get deleted, and two new ones</span></span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a><span class="co"># will get created</span></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale dc/wildfly-app <span class="at">--replicas</span><span class="op">=</span>2</span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a><span class="co"># here we can see that the pods are now two, the old one is still there that would be the one with suffix vr2kz, but a</span></span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a><span class="co"># new with with a suffix 47nwb has now spun up</span></span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   READY   STATUS             RESTARTS   AGE</span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-build</span>    0/1     Completed          0          3h33m</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-deploy</span>   0/1     Completed          0          3h31m</span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-vr2kz</span>    1/1     Running            0          3h31m</span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-47nwb</span>    1/1     Running            0          31s</span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can delete the original pod, the one ending with vr2kz, if your application works correctly, during the scaling</span></span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a><span class="co"># procedure, the application would have cloned / replicated over the data from the vr2kz original pod, to the new one</span></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a><span class="co"># which ends with 47nwb, meaning that no data will be lost when the first pod get deleted</span></span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc delete pod wildfly-app-1-vr2kz</span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a><span class="co"># after we delete this pod, since our deployment config was modified to require two replicas a new one would be spun up</span></span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a><span class="co"># in its place, to keep the pod replicas at two (2) which is okay, the desired state is fulfiled, there is one new one</span></span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a><span class="co"># with suffix k7g28 in place of the one we deleted vr2kz</span></span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   READY   STATUS             RESTARTS   AGE</span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-build</span>    0/1     Completed              0          3h37m</span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-deploy</span>   0/1     Completed              0          3h36m</span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-k7g28</span>    1/1     Running                0          6s</span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-47nwb</span>    1/1     Running                0          4m41s</span></code></pre></div>
<p>The two pods discovered each other with the help of a WildFly
specific discovery mechanism designed for Kubernetes, the implementation
is called KUBE_PING and its part of the <code>JGroups</code> project.
When the second pod was started it queried the OpenShift API for all the
pods in the current project. The API server then returned a list of pods
in the current project. The KUBE_PING code in the WildFly server
filtered the list of pods for those with special ports labeled ping. If
any of the pods in the result set returned from the API server match the
filter then the <code>JGroups</code> code in WildFly will attempt to
join any existing clusters among the pods in the list.</p>
<p>Take a moment to examine the result set from the pod perspective by
navigating to any of the pods in the OpenShift console and clicking the
terminal tab, then run this command to query the API server for a list
of pods in the project matching the label
<code>application=wildfly-app</code></p>
<div class="sourceCode" id="cb62"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first we can simply login into or remote ssh into one of the pods and inspect the contents of the environment which</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># shows what is the OpenShift config that WildFly would be using to query the server, in this case we run the following</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># inside the ssh session of one of the pods - env | grep -i kube, and we can see that we have these env variables, which</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># are used to determine which namespace to query, which is really the project name in OpenShift terms, and the labels to</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># filter on the pods, in our case to make sure it is looking only for pods that are the actual application that we care</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co"># about.</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="va">OPENSHIFT_KUBE_PING_NAMESPACE</span><span class="op">=</span>stateful-apps</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="va">OPENSHIFT_KUBE_PING_LABELS</span><span class="op">=</span>application=wildfly-app</span></code></pre></div>
<h3 id="verify-the-data-replication">Verify the data replication</h3>
<p>Now that two pods a re successfully clustered together delete
original pod from the OpenShift console or by the command line. The
OpenShift replication controller will notice that a pod has been deleted
and will spin up a new one in its place to ensure that there are still
two replicas. If clustering gs working properly the original data you
entered will still be available, even though it was originally stored in
memory in a pod that no longer exists. Double check by refreshing the
application in your browser. If the data is not longer there make sure
that the policy command above was run correctly, it has to be run first
before you do any other changes and before we start scaling and deleting
pods</p>
<h3 id="other-cases-for-direct-pod-access">Other cases for direct pod
access</h3>
<p>A java application server that needs to cluster application is just
one common use case for direct pod discovery and access. Another example
is an application that has its own load balancing or routing mechanisms
such as shared data base. A shared database is one in which large data
sets are stored i many small databases as opposed to one large database.
Many sharded databases have intelligence built into their clients and
drivers that allow for direct access to the correct shard without
querying where the data resides. Sharded databases work well with
OpenShift and have been implemented using MongoDB and Infinispan</p>
<p>A typical sharded database implementation may include creating the
service object as headless service. Once a headless service object is
created DNS can be used as another service discovery mechanism or
method. A DNS query for a given headless service will return A records
for all the pods in the service. More information on the DNS and A
records is available in next sections. Applications can then implement
custom login to determine which pod to access. One popular application
that uses DNS queries to determine which instances to access is Apache
Kafka a fast open source messaging broker. Most implementations of Kafka
on OpenShift and other kubernetes base platforms use headless services
so the messaging brokers can access each other directly to send and
replicate messages. The brokers find each other using DNS queries which
are made possible by implementing a headless service.</p>
<p>Other common use cases for direct access include more mundane IT
workloads such as software agents that are used for backups and
monitoring. Backup agents are often run with many traditional database
workloads and implement features such as scheduled snapshots and point
in time recovery of data. A monitoring agent often provides features
such as real time alerting and visualization of an application. Often
these agents may either run locally embedded as instrumented code in the
application or communicate through direct network access. For many cases
direct network access is required because the agents may communicate
with more than one application across many servers. In these scenarios,
the agents require consistent direct access to applications in order to
fulfill their daily functions.</p>
<h3 id="describing-sticky-sessions">Describing sticky sessions</h3>
<p>In the WildFly example data is replicated between the WildFly server
instances. A cookie with a unique identifier is generated automatically
by the application and stored in your browser. By using a cookie the
application can track which end user is accessing the application. This
approach works well but has several drawbacks. The most obvious is that
if the WildFly server did not support application clustering or did not
have a discovery mechanism that works in OpenShift that application
would produce uneven user experience. Without application clustering if
there were two application pods one with user data and one without user
data then the user would see their data like 50% of the time because
requests are send round-robin manner between pods in a service</p>
<p>Once common solution to this problem is to use sticky session. In the
context of OpenShift enabling sticky sessions ensures that a user making
requests into the cluster will consistently receive responses from the
same pod for the duration of their session.</p>
<p>This added consistently helps ensure a smooth user experience and
allows many applications that store temporarily data locally in the
container to be run in OpenShift. By default in OpenShift sticky session
are implemented using cookies for HTTP based routes and some types of
HTTPS based routes. The OpenShift router can reuse existing cookies or
create new cookies. The WildFly application you created earlier created
its own cookie so the router will use that cookie for the sticky session
implementation. If cookies are disable or can not e used for the route
sticky sessions are implemented using a load balancing scheme called
source that uses the client’s IP address as part of its
implementation.</p>
<h3 id="toggling-sticky-sessions">Toggling sticky sessions</h3>
<p>Let us see how sticky session work by toggling cookies on and off
using the Linux curl command line tool, which can make HTTP requests to
a server eight times and print the result. The WildFly application you
have deployed has a couple of REST endpoints that have not been explored
yet. The first endpoint can be used to print out the pod IP and
hostname. Enter the following command to print out that information from
8 sequential HTTP requests to the app’s route</p>
<div class="sourceCode" id="cb63"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here we call curl 8 times against the route of our application, using the /rest/serverdata/ip endpoint</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> I <span class="kw">in</span> <span class="va">$(</span><span class="fu">seq</span> 1 8<span class="va">)</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="cf">do</span> <span class="dt">\</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="ex">curl</span> <span class="at">-s</span> <span class="dt">\</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;</span><span class="va">$(</span><span class="ex">oc</span> get route wildfly-app <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.spec.host}&#39;</span><span class="va">)</span><span class="st">/rest/serverdata/ip&quot;</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">printf</span> <span class="st">&quot;\n&quot;</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co"># you will get something like that, as an output, we can see the host names which are really also corresponding to the</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co"># pod names as well as the IP addresses of those pods,</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-tmkqj&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.164&quot;</span><span class="dt">}</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-tmkqj&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.164&quot;</span><span class="dt">}</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-tmkqj&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.164&quot;</span><span class="dt">}</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-tmkqj&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.164&quot;</span><span class="dt">}</span></span></code></pre></div>
<p>The output prints the hostname and the IP address of each pod four
times, alternating back and forth between pods in a round robin pattern.
That is as you would expect from the default behavior, the curl command
does not provide any cookie or identifier method to tell OpenShift to
not do the default round robin routing approach.</p>
<p>Fortunately curl can save cookies locally in a text file that can be
used for future HTTP requests, to the server. Use the following command
to grab the cookie from the WildFly application and save it to a local
file called <code>cookie.txt</code></p>
<div class="sourceCode" id="cb64"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure to save the cookie by doing a regular request, then we will use that file to send it over for our 8</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># requests we are going to be doing, just as above, and inspect what we get as a result</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> curl <span class="at">-o</span> /dev/null <span class="dt">\</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--cookie-jar</span> cookies.txt <span class="dt">\</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">$(</span><span class="ex">oc</span> get route wildfly-app <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.spec.host}&#39;</span><span class="va">)</span>/rest/serverdata/ip</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># here is the content of that file after we created it with the request above, we can see that this is the data of a</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co"># regular browser cookie, it contains the type of the cookie - http-only, and the host we are hitting as well, and some</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co"># other fields which are of no interest at the moment, along with a couple of cookie identifiers at the end</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat cookie.txt</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Netscape HTTP Cookie File</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co">#HttpOnly_wildfly-app-stateful-apps.apps-crc.testing FALSE / FALSE 0 e5cd79b5768b9bd942ee273490b6e8ec 38affa384fdb33244245b19937c8d6e8</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="co"># here we first also save any cookies that the server provides us back and then send them back, to the server with each</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co"># HTTP request, this will ensure that the requests are pinned to a pod</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> I <span class="kw">in</span> <span class="va">$(</span><span class="fu">seq</span> 1 8<span class="va">)</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="cf">do</span> <span class="dt">\</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">curl</span> <span class="at">-s</span> <span class="at">--cookie</span> cookies.txt <span class="dt">\</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;</span><span class="va">$(</span><span class="ex">oc</span> get route wildfly-app <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.spec.host}&#39;</span><span class="va">)</span><span class="st">/rest/serverdata/ip&quot;</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">printf</span> <span class="st">&quot;\n&quot;</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="co"># and here is the output, now we can see that only one pod is being hit, there is no round robin pattern of access for the pods</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;hostname&quot;</span><span class="dt">:</span><span class="st">&quot;wildfly-app-1-bhtbm&quot;</span><span class="op">,</span><span class="st">&quot;ip&quot;</span><span class="dt">:</span><span class="st">&quot;10.217.0.165&quot;</span><span class="dt">}</span></span></code></pre></div>
<h3 id="limitations-of-cookies">Limitations of cookies</h3>
<p>One limitation of using cookies for load balancing is that they do
not work for HTTPS connections that use the pass-through routing. In
pass through routing there is an encrypted connection from the client
typically a browser all the way to the application pod. In this scenario
cookies wont work, because the connection is encrypted and OpenShift has
no way of decrypting it, there is no way for the routing layer in
OpenShift to see the request. To solve this problem OpenShift uses the
client IP address to implement sticky sessions. But this option has a
couple of drawbacks.</p>
<p>First many client IP addresses get translated using Network Address
Translation (NAT), before reaching their destination. When a request is
translated using NAT it replaces the often private IP address of the
client with that of a public IP address. This frequently makes the
client IP address the same for all users on a particular home or
business network. Imagine a scenario in which you ran three pods to run
an application for everyone in your office, but everyone in your office
was being routed to the same pod because the requests are translated by
NAT and are seen or appeared to be show the same source IP address.</p>
<p>Second OpenShift uses an internal hashing schema based on the client
IP address and the number of pods to determine is load balancing schema.
When the number of replicas changes such as when you are using auto
scaling it is possible to lose sticky sessions.</p>
<p>For the rest of this section you will not need to instances of the
WildFly application. So let us scale it back down</p>
<div class="sourceCode" id="cb65"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale dc/wildfly-app <span class="at">--replicas</span><span class="op">=</span>2</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="ex">deploymentconfig.apps.openshift.io</span> <span class="st">&quot;wildfly-app&quot;</span> scaled</span></code></pre></div>
<h2 id="shutting-down-applications">Shutting down applications</h2>
<p>So far in this section you have learned how to use sticky sessions to
ensure that users have consistent experience in OpenShift. You have also
learned how to use custom load balancing and service discovery in
OpenShift services. To demonstrate custom load balancing you deployed an
application that keeps users data in memory and replicates its data to
other pods. When looking at clustering you entered data and then scaled
up to two pods that replicated the data you entered. You then killed the
original pod and verified that your data was still there. This approach
worked well but in a controlled and limited capacity. Imagine a scenario
in which autoscaling was enabled and the pods were spinning up and down
more quickly. How would you know the application data had been
replicated before the particular pod was killed</p>
<ul>
<li>or even which pod was killed. OpenShift has several ways to solve
this issue</li>
</ul>
<h3 id="application-grace-period">Application grace period</h3>
<p>The easiest and most straightforward solution is to use a grace
period for the pod to gracefully shut down. Normally when an OpenShift
deletes a pod it sends the pod a Linux TERM signal, often abbreviated as
SIGTERM. The SIGTERM acts as a notification to the process that it needs
to finish what it is doing and then exit and pass control to the parent
process. One caveat is that the application needs custom code to catch
the signal and handle the shutdown sequence. Fortunately many
application servers have this code built in. If the container does not
exit within a given grace period, OpenShift sends a Linux KILL signal or
SIGKILL, that immediately terminates the application.</p>
<p>In this section, we will deploy a new a to demonstrate how OpenShift
grace period works. In the same stateful apps project that you are
already in run the following command to build and deploy the
application</p>
<div class="sourceCode" id="cb66"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will create the project directly from the github using the resource template</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-app <span class="dt">\</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">-l</span> app=graceful <span class="dt">\</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--context-dir</span><span class="op">=</span>dockerfile-graceful-shutdown <span class="dt">\</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    https://github.com/OpenShiftInAction/chapter8</span></code></pre></div>
<p>The application may take a minute to build because it may need to
pull down the new base image to build the application. Once the
application is successfully built and running delete it with a grace
period of 10 seconds.</p>
<div class="sourceCode" id="cb67"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will prompt the container runtime to wait 10 seconds before forcefully killing the container process, this gives</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># us a window of 10 seconds, within which we will take a look at the logs produces by the pod, see below</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc delete pod <span class="at">-l</span> app=graceful <span class="at">--graceperiod</span><span class="op">=</span>10</span></code></pre></div>
<p>When you run delete with a grace period of 10 seconds, OpenShift
sends a SIGTERM signal immediately to the pod and then forcibly kills it
in 10 seconds if it has not exited by itself within that time period.
Quickly run the following command to see this plays our in the logs for
the pod</p>
<div class="sourceCode" id="cb68"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will pull the logs for the pods that are running our app, we are using a sub shell command to select the pod-id</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co"># dynamically to make sure we do not lose time tracking it down, since we have 10 seconds to do this, to see the logs,</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># there is a way to see the logs of a killed pod as well, but that we will leave for future excessive</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc logs <span class="at">-f</span> <span class="va">$(</span><span class="ex">oc</span> get pods <span class="at">-l</span> app=graceful <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.items[].metadat.nam}&#39;</span><span class="va">)</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we can see that these are the logs the pod is printing while it is running, since it is an example application purely</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># created to test the SIGTERM it is waiting for us to send that signal, through the delete command, then after the signal</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="co"># was caught it is now running the code that is in the signal hook or callback section</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Waiting</span> for SIGTERM, sleeping for 5 seconds now...</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Waiting</span> for SIGTERM, sleeping for 5 seconds now...</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Waiting</span> for SIGTERM, sleeping for 5 seconds now...</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Caught</span> SIGTERM! Gracefully shutting down now</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 0 seconds</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 1 seconds</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 2 seconds</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 3 seconds</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 4 seconds</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 5 seconds</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 6 seconds</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 7 seconds</span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 8 seconds</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a><span class="ex">Gracefully</span> shutting down for 9 seconds</span></code></pre></div>
<p>The process that is running is a simple bash script that waits for a
SIGTERM signal and then prints a message to standard out until it is
killed. In this case the pod was given a grace period of 10 seconds and
the pods printed logs for approximately 10 seconds before it was
forcibly killed. By default, the grace period is set to 30 seconds. If
you have an important container, that you never want to kill or be
killed, you must set the <code>terminationGracePeriodSeconds</code>
field in the deployment to -1</p>
<p>As we know in a container the main process runs as process PID - 1.
This is very important and when handling linux signals, because only PID
1, receives the signal, although most containers have a single process
many containers have multiple processes. In this scenario the main
process needs to catch the signal and notify the other process in the
container - <code>systemd</code> can also be used as a seamless
solution. For containers with multiple processes that all need to handle
linux signals it is best to use <code>systemd</code>, for this
implementation.</p>
<p>We can now proceed and delete this example app demo, bu doing
<code>oc delete all -l app=graceful</code>, which will delete all
resources, tagged with the label - graceful</p>
<h3 id="container-lifecycle-hooks">Container lifecycle hooks</h3>
<p>Although catching basic linux signal such as SIGTERM is a best
practice many application are not equipped to handle the Linux signal.s
A nice way to externalize the logic from the application is to use the
<code>preStop</code> hook and one of two container lifecycle hooks
available in OpenShift. Container lifecycle hooks allow users to take
predetermined actions during a container management lifecycle event. The
two events available in OpenShift are as follows</p>
<ul>
<li><code>PreStop</code> - Executes a handler before the container is
terminated, this event is blocking, meaning it must finish before the
pod is terminated.</li>
<li><code>PostStart</code> - Executes a handler immediately after the
container is started.</li>
</ul>
<p>Similar to readiness probes and liveness probes the handler can be a
command that execute in the container, or it can be an HTTP call to an
endpoint exposed by the container. Container lifecycle hooks can be used
in conjunction with pod grace periods. If the <code>preStop</code> hook
are used they take precedence over the pod deletion. SIGTERM will not be
sent to the container until the <code>preStop</code> hook finishes
executing.</p>
<p>Container lifecycle hooks and linux signal handling are often used
together but in many cases users decide which method to use for their
application. The main benefit of using linux signal handling is that the
application will always behave the same way, no matter where the image
is run. It guarantees consistent and predictable shutdown behavior
because the behavior is coded in the application itself. Sending SIGTERM
signals on delete is fundamental not only to all kubernetes or OpenShift
platforms but to all runtimes like docker, contaienrd and so on. If the
user handles the SIGTERM signal in their application the image will
behave consistently even if it is moved outside of OpenShift. Because
the <code>preStop</code> hooks need to be explicitly added to the
deployment or template there is no guarantee that the image will behave
the same way in other environments. Many application such as third party
application do not handle SIGTERM properly and the end user can not
easily modify the code. In this case a <code>preStop</code> hook must be
used. A good example of this is the NGINX server, a popular and
lightweight HTTP server. When NGINX is sent a SIGTERM it exits
immediately rather than forking NGINX image itself, and adding code to
handle the linux SIGTERM signal an easy solution is to add a
<code>preStop</code> hook that gracefully shuts down the NGINX server
form the command line. A general rule to follow is that i you control
the code, you should code your application to handle the SIGTERM if you
do not control the code use a <code>preStop</code> hook if needed</p>
<h2 id="stateful-sets">Stateful sets</h2>
<p>So far in this chapter you have learned that OpenShift has many
capabilities to support stateful application. These let users make
traditional workloads first class citizens on OpenShift, but some
application also require even more predictable startup and shutdown
sequencing as well as predictable storage and networking identifying
information. Imaging a scenario with the WildFly application in which
data replication is critical to the user experience but a massive
scaling event destroys too many pods at one time while replication is
happening, how will the application recover ? Where will the data be
replicated to ? Is it going to be lost forever ?</p>
<p>To solve this problem OpenShift has a special object called a
stateful set, known as pet set or older versions of OpenShift. A
stateful set is a powerful tool in the OpenShift users toolbox to
facilitate many traditional workloads in a modern environment. A
stateful set object is used in place of a replication controller or
replica set (in newer version of OpenShift platform) and it is the
underlying implementation to ensure replicas in a service, but it does
so in a more controlled way.</p>
<p>A replication controller can not control the order of how pods are
created or destroyed. Normally if a user configures a deployment to go
from one to five replicas in OpenShift that task is passed to an replica
set or controller that starts four new pods all at once. The order in
which they are started, and marked as ready, is completely random and
unknown</p>
<h3 id="deterministic-sequence">Deterministic sequence</h3>
<p>A stateful set brings a deterministic sequential order to pod
creation and deletion. Each pod that is created also has an ordinal
index number associated with it. The ordinal index indicates the startup
order. For instance if the application WildFly application was using a
stateful set with three replicas the pods would bee started and names in
this order - <code>wildfly-app-0</code>, <code>wildfly-app-1</code>,
<code>wildfly-app-2</code>. A stateful set also ensures that each pod is
running and ready, (has passed the readiness probe) before the next pod
is started, it is sequential. In the previous scenario
<code>wildfly-app-2</code> would not be started until the
<code>wildfly-app-1</code> was running and ready</p>
<p>The reverse is also true, a replication set or controller will delete
pods at random, when a command is given to reduce the number of
replicas. A stateful set can also be used to controller shutdown
sequence, it starts with the pod that has the highest ordinal index (n-1
replica) and works its way backward to meet the new replica
requirements. A pod will not be shut down until the previous pod has
been fully terminate. Refer back to the previous section regarding
SIGTERM and lifecycle hooks which will also affect this behavior</p>
<p>The controller shutdown sequence can be critical for many stateful
application. In the case of the WildFly application user data is being
shared between a number of pod. When the WildFly application is shut
down gracefully a data synchronizes process may occur between the
remaining pods in the application cluster. This process often be
interrupted without the use of a stateful set because the pods are shut
down in parallel. By using a predictable one-at-a-time shutdown
sequence, the application is less likely to lose any data which results
in a better user experience.</p>
<h3 id="examining-a-stateful-set">Examining a stateful set</h3>
<p>To see how stateful sets work first create a new project</p>
<div class="sourceCode" id="cb69"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first create a new namespace project that we will be using to do our testing in</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-project statefulset</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co"># then we can create the template from the provided file, which will be later used to create the actual app</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc create <span class="dt">\</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">-f</span> openshift/stateful-set-template.yml <span class="dt">\</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">-n</span> statefulset</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="ex">template.template.openshift.io/mongodb-statefulset-replication-emptydir</span> created</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="co"># now create the application from the template we have created above</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc new-app <span class="at">--template</span><span class="op">=</span><span class="st">&quot;mongodb-statefulset-replication-emptydir&quot;</span></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a><span class="co"># note how our two pods are correctly labeled with -0 and -1, these are the ordinal values of the pods, they were</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a><span class="co"># created in that exact same order by the stateful set/controller</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>      READY STATUS  RESTARTS AGE</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-0</span> 1/1   Running 0        68s</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-1</span> 1/1   Running 0        37s</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can see the logs of the very first pod with index 0, and we can actually see how the pod is configured to look for other pods such as in this case pod with index 1, to replicate the data, that was added to the database, in this case a single entry of user was added, then that data was replicated to the other pod, you can clearly see how the mongodb-0 pod is trying to access the other pod through the deterministic IP/host address which in this case is the mongodb-1.mongodb-internal.statefulset.svc.cluster.local</span></span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc logs mongodb-0</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a><span class="ex">Successfully</span> added user: { <span class="st">&quot;user&quot;</span> : <span class="st">&quot;oiauser&quot;</span>, <span class="st">&quot;roles&quot;</span> : [ <span class="st">&quot;readWrite&quot;</span> ] }</span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a><span class="ex">bye</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">conn12</span><span class="pp">]</span> replSetReconfig admin command received from client</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">conn12</span><span class="pp">]</span> replSetReconfig config object with 2 members parses ok</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Connecting to mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Successfully connected to mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> New replica set config in use: { _id: <span class="st">&quot;rs0&quot;</span>, version: 2, protocolVersion: 1, members: [ { _id: 0, host: <span class="st">&quot;mongodb-0.mongodb-internal.statefulset.svc.cluster.local:27017&quot;</span>, arbi</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a><span class="ex">terOnly:</span> false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: <span class="st">&quot;mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017&quot;</span>, arbiterOnly: false, buildIndexes: true, hidden: fal</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a><span class="ex">se,</span> priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wti</span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a><span class="ex">meout:</span> 0 }, replicaSetId: ObjectId<span class="er">(</span><span class="st">&#39;6846cec0fa37b01dd820333d&#39;</span><span class="kw">)</span> <span class="er">}</span> <span class="er">}</span></span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> This node is mongodb-0.mongodb-internal.statefulset.svc.cluster.local:27017 in the config</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> Member mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017 is now in state STARTUP</span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> Member mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017 is now in state SECONDARY</span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a><span class="co"># start scaling to 3 replicas, which will simply add one more, to the list above, notice that our pods are in the</span></span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a><span class="co"># ready state, which means that we will be able to scale them up, had the mongodb-1 pod been in a non ready 0/1 state,</span></span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a><span class="co"># our scale command would have failed, that is because the stateful set will not allow us to scale up when we have</span></span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a><span class="co"># pods that are not ready</span></span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale statefulset/mongodb <span class="at">--replicas</span><span class="op">=</span>3 <span class="kw">&amp;&amp;</span> <span class="ex">oc</span> get pods</span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>      READY STATUS  RESTARTS AGE</span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-0</span> 1/1   Running 0        4m16s</span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-1</span> 1/1   Running 0        4m15s</span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-2</span> 1/1   Running 0        4s</span>
<span id="cb69-45"><a href="#cb69-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-46"><a href="#cb69-46" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can also look up the logs of the mongodb-0, we can see that now the newest pod which was the mongodb-2, is detected by the mongodb-0 pod, which then replicates its data to the mongodb-2 pod, using the same process as it did for the pod with ordinal index 1 above.</span></span>
<span id="cb69-47"><a href="#cb69-47" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc logs mongodb-0</span>
<span id="cb69-48"><a href="#cb69-48" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Connecting to mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-49"><a href="#cb69-49" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> New replica set config in use: { _id: <span class="st">&quot;rs0&quot;</span>, version: 3, protocolVersion: 1, members: [ { _id: 0, host: <span class="st">&quot;mongodb-0.mongodb-internal.statefulset.svc.cluster.local:27017&quot;</span>, arbi</span>
<span id="cb69-50"><a href="#cb69-50" aria-hidden="true" tabindex="-1"></a><span class="ex">terOnly:</span> false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: <span class="st">&quot;mongodb-1.mongodb-internal.statefulset.svc.cluster.local:27017&quot;</span>, arbiterOnly: false, buildIndexes: true, hidden: fal</span>
<span id="cb69-51"><a href="#cb69-51" aria-hidden="true" tabindex="-1"></a><span class="ex">se,</span> priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: <span class="st">&quot;mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017&quot;</span>, arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, vot</span>
<span id="cb69-52"><a href="#cb69-52" aria-hidden="true" tabindex="-1"></a><span class="ex">es:</span> 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId<span class="er">(</span><span class="st">&#39;6846cec0fa3</span></span>
<span id="cb69-53"><a href="#cb69-53" aria-hidden="true" tabindex="-1"></a><span class="st">7b01dd820333d&#39;</span><span class="kw">)</span> <span class="er">}</span> <span class="er">}</span></span>
<span id="cb69-54"><a href="#cb69-54" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> This node is mongodb-0.mongodb-internal.statefulset.svc.cluster.local:27017 in the config</span>
<span id="cb69-55"><a href="#cb69-55" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Connecting to mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-56"><a href="#cb69-56" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Successfully connected to mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-57"><a href="#cb69-57" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">ASIO</span>     <span class="pp">[</span><span class="ss">NetworkInterfaceASIO</span><span class="pp">-</span><span class="ss">Replication</span><span class="pp">-</span><span class="ss">0</span><span class="pp">]</span> Successfully connected to mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017</span>
<span id="cb69-58"><a href="#cb69-58" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> Member mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017 is now in state STARTUP</span>
<span id="cb69-59"><a href="#cb69-59" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>event-date<span class="op">&gt;</span> I <span class="ex">REPL</span>     <span class="pp">[</span><span class="ss">ReplicationExecutor</span><span class="pp">]</span> Member mongodb-2.mongodb-internal.statefulset.svc.cluster.local:27017 is now in state SECONDARY</span></code></pre></div>
<p>Unlike previous use of the scale command this time you need to
explicitly state that you are scaling a stateful set. In OpenShift
console notice that the new pod that was created has a deterministic pod
name with the ordinal index associated with it. Also the reason we are
using the <code>statefulset</code> object to scale, instead of
deployment, is because the <code>statefulset</code> is a replacement for
the regular deployment object when working with stateful application
contexts, such as this one. The <code>StatefulSet</code> has similar but
also different properties in its specification than the
<code>Delpoyment</code> this is to reflect the features that the
<code>StatefulSet</code> exposes to the OpenShift users, related to
managing application state</p>
<p>Similar to the WildFly application the three MongoDB pods are
replicating data to each other. To check that this replication is fully
functional click any of the pods on the bottom of the mongodb stateful
set Details page and then click the Terminal tab. Any commands executed
here will execute in the pod. First log into the mongodb as the admin
user, using <code>mongo admin -u admin -p oiaadminpassword</code>. After
that check the status of the MongoDB replica set by typing
<code>rs.status()</code> after the login was successful</p>
<h3 id="constant-network-identity">Constant Network identity</h3>
<p>Stateful sets also provide a consistent means of determining the
host-naming scheme for each pod in the set. Each predictable hostname is
also associated with a predictable DNS entry. Examine the pod hostname
for mongodb-0 by executing this command -</p>
<div class="sourceCode" id="cb70"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we are again pulling the id of each pod or its name to exec a cat command from the context of each node, printing the</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="co"># contents of the /etc/hostname file, which will show us the actual hostname of the pod</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> for statefulpod in <span class="va">$(</span><span class="ex">oc</span> get pods <span class="at">-l</span> name=mongodb <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.items[*].metadata.name}&#39;</span><span class="va">)</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">do</span> <span class="dt">\</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>        <span class="ex">oc</span> exec <span class="va">$statefulpod</span> <span class="at">--</span> cat /etc/hostname<span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">done</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="co"># these are the hostnames, these will correspond to the names of the pods themselves as we know, from the very firs</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="co"># sections when we discussed the relationship between the container, container runtime, hostname and the orchestrator</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-0</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-1</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a><span class="ex">mongodb-2</span></span></code></pre></div>
<p>We know that the hostname is the same as the pod name, that is
something we have seen and investigated earlier in previous sections.
The stateful set also ensures a DNS entry for each pod running in a
stateful set. This can be found by executing the dig command using the
DNS entry name for each pod. Find the IP addresses by executing the
following command from one of the OpenShift nodes. Because the command
relies on the OpenShift provided DNS it must be run from within the
OpenShift environment to work properly</p>
<p><code>When you are using stateful sets the pod hostname in the DNS is listed in the format &lt;pod name&gt;.&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code></p>
<p>Because this example also contains a headless service, there are DNS
A records for the pods associated with the headless service. Ensure that
the pod IP into DNS match the previous listing by running this command
from the OpenShift nodes</p>
<div class="sourceCode" id="cb71"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># login into the node itself first, through ssh</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh <span class="op">&lt;</span>cluster-node-host<span class="op">&gt;</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># from within the cluster node, we can then execute the following piece of shell script which will use dig - DNS lookup</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="co"># utility. Is a flexible tool for interrogating DNS name servers. It performs DNS lookups and displays the answers that</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="co"># are returned from the name server(s) that were queried. Most DNS administrators use dig to troubleshoot DNS problems</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="co"># because of its flexibility, ease of use, and clarity of output. Other lookup tools tend to have less</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="co"># functionality than d</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> for statefulpod in <span class="va">$(</span><span class="ex">oc</span> get pods <span class="at">-l</span> name=mongodb <span class="at">-o</span><span class="op">=</span>jsonpath=<span class="st">&#39;{.items[*].metadata.name}&#39;</span><span class="va">)</span><span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">do</span> <span class="dt">\</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        <span class="ex">dig</span> +short <span class="va">$statefulpod</span>.mongodb-internal.statefulset.svc.cluster.local<span class="kw">;</span> <span class="dt">\</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">done</span></span></code></pre></div>
<h3 id="consistent-persistent-storage">Consistent persistent
storage</h3>
<p>Pods running as part of a stateful set can also have their own
persistent volume claims associated with each pod. But unlike a normal
persistent volume claim, they remain associated with a pod and its
ordinal index, as long as the stateful set exists. In the previous
example you deployed an ephemeral stateful set without persistent
storage. Imagine that the previous example was using persistent storage,
and the pods were writing log files that included the pod hostname. You
wold not want the persistent volume claim to later be mapped to the
volume of a different pod with a different hostname because it would be
hard to make sense of those log files, for debugging and auditing
purposes. Stateful sets solve this problem by providing a consistent
mapping through the use of a volume claim template which is a template
the persistent volume associates with each pod. If a pod dies or is
reschedules to a different node, then the persistent volume claim will
be mapped only to the new pod that starts in its place with the same
hostname as the old pod. Providing a separate and dedicate persistent
volume claim for each pod in the stateful set is crucial for many
different types of stateful applications which cannot use the typical
deployment config model of sharing the same persistent volume claims
across many applications instances.</p>
<h3 id="the-stateful-set-limitations">The Stateful set limitations</h3>
<p>Under normal circumstances pods controller by a stateful set should
not need to be deleted manually. But there are a few scenarios in which
a pod being controller by a stateful set could be deleted by an outside
force. For instance if the kubelet or node is unresponsive, then the API
server may remove the pod after a given amount of time and restart it
somewhere else in the cluster. A pod could also exit accidentally or
could be manually removed by a user. In those cases it is likely that
the ordinal index will be broken, New pods will be created with the same
hostname and DNS entries, as the old pods but the IP addresses may be
different. For this reason any application that relies on hard coded IP
addresses is not a good idea, or fit for stateful sets. If the
application can not be modified to use DNS or hostnames instead of IP
addresses you should use a single service per pod for a stable IP
address.</p>
<p>Another limitation is that all the pods in a stateful set are
replicas of each other which of course makes sense when you want to
scale, But that would not help any situation in which disparate
applications need to be started in a particular order. A classic example
is a Java or NET application that throws errors if a database is
unavailable, once the database is started then the application also
needs to be restarted to refresh the connections. In that scenario, a
stateful set would not help the order between the two disparate
services</p>
<h3 id="non-native-stateful-applications">Non-native stateful
applications</h3>
<p>One of the reasons OpenShift has gained so much market adoption is
that traditional IT workloads work just as well as modern stateless
applications. Yet there is still work to be done, one of the biggest
promises of using containers is that applications will behave the same
way between environments. Containers start form well known image
binaries that contain the application and the configuration it needs to
run. If a container dies a new one is started form the previous image
binary that is identical to how the previous container was started. Once
major problem with this model occurs for applications that are changed
on the fly and store their information in a way that makes it difficult
to re-create</p>
<p>A good example of this issue can be seen with <code>wordpress</code>
an extremely popular blogging application that was designed many years
before containers became popular. In a given <code>wordpress</code>
workflow a blogger might go to the admin portion of their web-site add
some text and then save it, <code>wordpress</code> saves all that text
in a database, along with any HTML and styling. When the blogger has
completed this action the container has drifted form its original image.
Container drift is normal for most applications but in this case, if the
container crashed the blog would be lost, persistent storage can be used
to ensure that the data is persisted. When a new <code>wordpress</code>
pod starts it could map to the database and would have all the blogs
available.</p>
<p>But promoting such a snapshot of a database among various
environments is a major challenge. There are many examples of using an
event driven workflow that can be triggered to export and import a
database after a blogger publishes content, but it is not easy nor is it
native to the platform. Containers start from well known immutable
container images, but engineering a reverse workflow in which images are
created form running container instances is more error-prone and rigid.
Other examples that have worked with some engineering include -
applications that open a large number of ports applications that rely on
hard coded IP addresses, and other legacy applications that rely on
older Linux technologies</p>
<h3 id="cleanup-resource-limits-quotas">Cleanup resource limits &amp;
quotas</h3>
<p>After you are done with this chapter, make sure to either delete the
resource quota and limit ranges objects we have created or edit them to
be more lenient, in case you wish to up-scale your replicas for the
app-cli project, this will vastly restrict your capabilities to do so
since the limits are not enough for more than 2 replicas for this
project. Be wary of this !</p>
<h1 id="operations-security">Operations &amp; Security</h1>
<p>This section focuses on cluster wide concepts and knowledge you will
need to effectively manage an OpenShift cluster at scale. These are some
of the skills required for any operations teams managing an OpenShift
cluster. In previous chapter is all about working with OpenShift
integrated role based access control, you will change the authentication
provider for your cluster, users and work with the system accounts build
into OpenShift along with the default roles for different user types.
Other sections focus on the software define network that deployed as
part of OpenShift this is how containers communicate with each other and
how service discovery works in an OpenShift cluster. The we will bring
everything together and look at OpenShift from security perspective. We
will discuss how SELinux is used in OpenShift and how you can work with
security policies to provide the most effective level of access for your
application.</p>
<h2 id="permissions-vs-wild-west">Permissions vs Wild-West</h2>
<p>A platforms like OpenShift are not effective for multiple users
without robust access and permissions management for various
applications in OpenShift components. If every user had a full access to
all your OpenShift resources it would truly be the wild west. Conversely
if it was difficult to access resources, OpenShift would not be a good
for much either. OpenShift has a robust authentication and access
control system, that provides a good balance of self-service workflows
to keep productivity up while limiting users to only what they need to
get their job done. When you first deployed OpenShift the default
configuration allowed any user name and non empty password field to log
in. This authentication method uses the allow-all identity provider that
comes with OpenShift.</p>
<p>In OpenShift the identity provider is a plugin that defines how users
can authenticate and the backend service that you want to connect for
managing user information. Although the allow all provider is good
enough when you are learning to use OpenShift when you need to enforce
access rules you will need to change to a more secure authentication
method. In the next section you will replace the allow-all provider with
one that uses a local data base file</p>
<p>We are going to use and configure the OpenShift cluster with an
Apache htpasswd database for user access and set up a few users to use
with that authentication source You will create the following users:</p>
<ul>
<li>developer a user with permissions typical to those given to a
developer in an OpenShift cluster</li>
<li>project-admin - a user with permissions typical of a developer or
team lead in an OpenShift cluster</li>
<li>admin - a user with administrative control over the entire OpenShift
cluster</li>
</ul>
<p>Please go through that now, and then continue with this section.
After configuring OpenShift in that way, if you attempt to log in with
your original dev or other user that user can not be authenticated
because it is not in your htpasswd database, but if you log into using
the new developer or any of the new users, you will no longer have
access to the previous projects we have created - like the
<code>image-uploader</code> and so forth, that is due to the fact that
the old dev user would still own that namespace or project</p>
<h3 id="setting-up-authentication">Setting up authentication</h3>
<p>Many different user databases are available to the IO professionals
for managing access and authentication. To interoperate with as many of
these as possible, OpenShift provides 11 identity providers that
interface with various user databases, including the allow all provider
that you have been using so far in the cluster. These providers are as
follows</p>
<ul>
<li>allow all - allows any username and non-empty password to log
in</li>
<li>deny all - does not allow any usernames and passwords to log in</li>
<li>htpasswd - authenticates with Apache htpasswd database files</li>
<li>keystone - user OpenStack Keystone as the authentication source</li>
<li>LDAP - authenticates using an LDAP provider like OpenLDAP</li>
<li>Basic - uses the Apache Basic authentication on a remote server to
authenticate users</li>
<li>Request Header - uses custom http header for user
authentication</li>
<li>GitHub - authenticates with github, using OAuth2</li>
<li>Gitlab - authenticates with gitlab, using OAuth2</li>
<li>Google - uses google OpenID connect for authentication</li>
</ul>
<p>Different authentication providers have different options that are
specific to each provider unique format, for example the options
available for the htpasswd provider are different than those required
for the github provider, because these providers access such different
user databases</p>
<p>What is htpasswd - that is an utility from the old days, it goes all
the way back to the first versions of the Apache web server in the later
90s, back then computers had so much less memory that the name of an
application could affect system performance, applications names were
typically limited to eight characters, to fit this tight requirement
characters were often removed or abbreviated - and thus htpasswd was
born.</p>
<h3 id="introduction-to-htpasswd">Introduction to htpasswd</h3>
<p>The htpasswd provider uses the Apache style htpasswd files for
authentication. These are simple databases that contain a list of
usernames and their corresponding password in an encrypted format. Each
line in the file represents a user. The user and password sections are
separated with a colon (:). The password section includes the algorithms
that was used to encrypt the password encapsulated with $ characters,
and the encrypted password itself. Here is an example htpasswd file wit
two users, admin and developer</p>
<pre class="htpasswd"><code>admin:$apr1$vUqfPZ/D$sTL5RCy1m5kS73bC8GA3F1
developer:$apr1$oKuOUw1t$CEJSFcVXDH5Jcq7VDF5pU/</code></pre>
<p>You create htpasswd files using the htpasswd command line tool, by
default the htpasswd tool uses a custom encryption algorithms base on
the <code>md5</code> hashing.</p>
<h3 id="creating-htpasswd-files">Creating htpasswd files</h3>
<p>To create an htpasswd database file, you need to ssh into your master
server or cluster node, on the master server the configuration files for
the OpenShift master process are in the /etc/origin/master. There you
will create the htpasswd file called <code>openshift.htpasswd</code>
with three users - developer, project-admin, and admin - to act as the
database for the htpasswd provider to interact with</p>
<p>You need to run the htpasswd command to add each user, the first time
you run the command be sure to include the -c option to create the new
htpasswd file. First make sure that the htpasswd utility is installed,
you can do these operations on your host machine, since it is the file
we are interested in, the file that will be produced by the commands
below</p>
<div class="sourceCode" id="cb73"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># execute the following command to add the different users, and also create the htpasswd database file, you see that we</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># still keep the kubeadmin here, just change the password of that user, this is because that user is the only</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="co"># cluster-admin, until we have another one we can not remove it from the htpasswd file, otherwise we will not be able to</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># execute any cluster level operations which we need to still, to add roles to the other users and the new admin, which is</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="co"># going to be named just - admin, after that we can safely delete the kubeadmin user.</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> mkdir <span class="at">-p</span> /etc/origin/master <span class="kw">&amp;&amp;</span> <span class="fu">touch</span> openshift.htpasswd</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> htpasswd <span class="at">-b</span> <span class="at">-c</span> <span class="at">-B</span> /etc/origin/master/openshift.htpasswd kubeadmin admin</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> htpasswd <span class="at">-b</span> <span class="at">-B</span> /etc/origin/master/openshift.htpasswd admin admin</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> htpasswd <span class="at">-b</span> <span class="at">-B</span> /etc/origin/master/openshift.htpasswd developer developer</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> htpasswd <span class="at">-b</span> <span class="at">-B</span> /etc/origin/master/openshift.htpasswd project-admin project-admin</span></code></pre></div>
<h3 id="changing-the-provider">Changing the provider</h3>
<p>Before we add the new database with username and password, we can
actually take a look at how the <code>crc</code> does it by default, it
is using the htpasswd file database, too and we can actually see how
with this command</p>
<div class="sourceCode" id="cb74"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># by default we can see that there is an object called htpass-secret, that is the default one which actually contains</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the two users, and their username and password, with the command below we can extract the contents of the secret to the</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co"># standard output the secret is an opaque object that contains the htpasswd file encoded in base64, meaning it is by no</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="co"># means encrypted and we can see the content of the file</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc extract secret/htpass-secret <span class="at">--to</span><span class="op">=</span>- <span class="at">--confirm</span> <span class="at">-n</span> openshift-config</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="ex">developer:</span><span class="va">$2</span><span class="ex">a</span><span class="va">$1</span><span class="ex">0</span><span class="va">$QVaaReC08iHwTJ5fUv2Kj</span><span class="ex">.YdVbjtuNKgf89X/0uwsmQN17cUyR.vW</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="ex">kubeadmin:</span><span class="va">$2</span><span class="ex">a</span><span class="va">$1</span><span class="ex">0</span><span class="va">$GskveI9lKitkLXj</span><span class="ex">.yoB46ueCoHp50ytLofjhQN5/LSy7bFs.UW8ta</span></span></code></pre></div>
<p>First we need to define a new secret, we can use the contents of the
htpasswd file after which we can apply that to the cluster like so, we
will use a new secret name to avoid having to delete the old one, but
the process will follow the same configuration steps</p>
<div class="sourceCode" id="cb75"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that we are creating a new secret in the openshift-config namespace, this is where all the openshift config</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="co"># lives, we will use the database file we created above, the secret command will base64 encode the file and create the</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co"># secret from it, we use the new file we generated with the new users and passwords</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc create secret generic htpass-secret-custom <span class="at">--from-file</span><span class="op">=</span>htpasswd=openshift/htpasswd-new-users <span class="at">-n</span> openshift-config</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="ex">secret/htpass-secret-custom</span> created</span></code></pre></div>
<p>Here is the document we have to actually update, by default there is
already a cluster level OAuth provider, if we run an –apply with this
new provider content, what will happen is that it will get updated and
the name of the <code>fileData</code> field will be replace with the new
secret that we created, which is the
<code>htpass-secret-custom</code></p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> config.openshift.io/v1</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> OAuth</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> cluster</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">identityProviders</span><span class="kw">:</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> htpasswd_custom_provider</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">mappingMethod</span><span class="kw">:</span><span class="at"> claim</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">type</span><span class="kw">:</span><span class="at"> HTPasswd</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">htpasswd</span><span class="kw">:</span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">fileData</span><span class="kw">:</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> htpass-secret-custom</span></span></code></pre></div>
<div class="sourceCode" id="cb77"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run the following command with the content of the OAuth object above, to update the cluster config, then logout of</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co"># existing sessions and try to login again, with some of the users, we have set the passwords to be the same as the</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># usernames, by default all users will be developers, and will have access to no projects, we will fix this in next</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="co"># section where we give them permissions from the cluster node itself</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc apply <span class="at">-f</span> misc-tech-topic/openshift/oauth-cluster-htpass.yml</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="ex">oauth.config.openshift.io/cluster</span> configured</span></code></pre></div>
<p>Make sure to log-out and log back in after the changes, use the new
password for the <code>kubeadmin</code> user which is simply
<code>admin</code></p>
<h2 id="working-with-roles">Working with roles</h2>
<p>Roles are used to define permission for all users in an OpenShift
cluster, in previous sections, you used the special admin to configure
physical volumes on your cluster, the admin is a special user account.
To work with roles you will use a new command line tool name
<code>oadm</code> (short for OpenShift administration). It is installed
by default on your cluster</p>
<p>On your master node, the OpenShift deployment program setup up the
root user, we can see the user information set up for the root user by
running the following command as the root user on your master node
OpenShift server - <code>oadm config view</code>. This allows
administrators with access to the root user on an OpenShift master node
server to have cluster administration access by default. It is useful,
but it also means you have to make sure everyone who has root access ot
your master server should be able to control your OpenShift cluster. For
a smaller cluster like the one you have built this will work fine. But
for a large cluster, the people who should have root access to your
server and the people who should be able to administer OpenShift
probably wont match. You can distribute this administrative certificate
as needed for your cluster administrator workstations.</p>
<h3 id="assigning-user-roles">Assigning user roles</h3>
<p>Remember those users you created ? The developer user need
permissions to view and add new content to the
<code>image-uploader</code> project. To accomplish that first make sure
you are working in the context of the <code>image-uploader</code>
project by running the following command
<code>oc project image-uploader</code> In the project namespace you need
to add the edit role to your developer user. This role gives user
permission to add. Adding a role to a new user for a project or even the
entire OpenShift cluster is called binding a role to a user.</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure that your are first logged in the cluster node, do not execute this form the host machine, rather it has to</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># be done directly from the node / master cluster server</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oadm policy add-role-to-user edit developer</span></code></pre></div>
<p>To confirm that your new role is applied log in again through the web
UI or the command line as the developer user. You should now have access
to the <code>image-uploader</code> project, and the deployed application
there. That takes care of the developer user. Let us give your admin
user a little more access in the next section you will give the admin
user administrator level access to your entire OpenShift cluster</p>
<h3 id="creating-administrators-user">Creating administrators user</h3>
<p>So far the OpenShift cluster has a single project, as an OpenShift
cluster grows it typically has dozens or even hundreds of projects at
any given time. To manage this effectively you need users who can
administer a project or even hundreds of projects at any given time.</p>
<p>Creating the project admin - for the <code>image-uploader</code>
project you will make the project admin user an administrator for the
project only. You can do so much the same way you have the developer
user the ability to edit, instead of binding the edit role to the
project admin user, however you need to bind the admin role. This role
will give the project admin user full administrative privileges in the
<code>image-uploder</code> namespace project. Run the following command
as root on your master server</p>
<div class="sourceCode" id="cb79"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this gives the</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oadm policy add-role-to-user admin project-admin</span></code></pre></div>
<p>You now have developer user who can work in the
<code>image-uploader</code> project and a project-admin who can
administer the project. The next user role you need is one who can
manage the entire OpenShift cluster.</p>
<p>Creating the cluster admin - the cluster admin role is important. A
cluster admin can not only administer projects but also manage all
OpenShift internal configuration and state, To create a cluster admin
run the following command as root on your master node:</p>
<div class="sourceCode" id="cb80"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note the difference here, we are giving a cluster-role, not just a role to the user, and the cluster role name is -</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cluster-admin, that should be enough for you to tell the difference between this command and the one we did for the other</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="co"># two users above.</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oadm policy add-cluster-role-to-user cluster-admin admin</span></code></pre></div>
<p>This command binds the admin role to the admin user we already
created in our htpasswd database file, instead of binding that role for
a single project, it binds it for every project or namespace in
OpenShift. Everything you have donein this section until now will hep
you edit existing users and make sure they have the correct privileges
to access what their job requires. But what happens when you add new
users ? In the next section we will configure OpenShift bind the edit
role to new users by default when they are created</p>
<h3 id="setting-default-roles">Setting default roles</h3>
<p>OpenShift has three default groups. These groups are configured
during OpenShift installation and define whether a user is
authenticated. You can use these groups to target users for additional
actions, but the groups themselves can not be modified.</p>
<ul>
<li>system:authenticated - any user who has successfully authenticated
through the web UI or command line, or via the API</li>
<li>system:authenticated:oauth - any user who has been authenticated by
OpenShift internal OAuth2 server. This excludes system accounts</li>
<li>system:unauthenticated - users who have failed authentication or not
attempted to do one.</li>
</ul>
<p>In your cluster it will be helpful to allow any authenticated user to
access the <code>image-uploader</code> project. You can accomplish this
by running the following <code>oadm</code> policy command which binds
the edit role for the <code>image-uploader</code> project specified by
the -n option to the system:authenticated group</p>
<div class="sourceCode" id="cb81"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now any user who has successfully authenticated and logged in will now have direct access to this project.</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oadm policy add-role-to-group edit <span class="at">-n</span> image-uploader system:authenticated</span></code></pre></div>
<p>When do we use other default groups - this example uses the
system:authenticated group, depending on what you need to accomplish the
other groups can be used in a similar fashion. The
<code>system:authenticated:oauth</code> groups excludes the system
accounts that are used to build and deploy applications in OpenShift. We
will cover those in future sections, in short this group consists of all
the humans and external services accessing OpenShift.
System:unauthenticated can be used if you want to provide a level of
anonymous access in your cluster. Its most common use however is to
route any user currently in that group to the OpenShift login page</p>
<p>To confirm that your new default user role has taken effect add an
additional user named <code>user1</code> to your htpasswd database file
with the following command -
<code>echo user1 | htpasswd --stdin /etc/origin/master/openshift/htpasswd user1</code></p>
<p>Log into your cluster with that user and confirm that your new user
can use the <code>image-uploader</code> project by default, that user
should have the ability to work in the <code>image-uploader</code>
project from the first login</p>
<p>Any time you have a shared environment you need processes in place to
ensure that one user or project does not take up too many resources, or
privileges in your cluster- either accidentally or on purpose. Limit
ranges and resource quotas are the processes that manage this potential
problem. In OpenShift these resources constraints are different for each
deployment depending on whether explicit resources quotas are
requested.</p>
<p>For previous applications that we deployed we did not specify any
resources, either processor or memory to be allocated for either
deployment these best-effort deployments do not request such specific
resources and are assigned a best-effort quality of service can govern
default values at the project level in OpenShift by using limit ranges.
In the next section, we will discuss limit ranges in more depth and you
will create your own and apply them to the <code>image-uploader</code>
project.</p>
<h2 id="limit-ranges">Limit ranges</h2>
<p>For each project in OpenShift a limit range defined as a
<code>LimitRange</code> when working with the OpenShift API provides
resources constraints, for most objects that exist in a project. The
objects are the types of OpenShift components that users deploy to serve
applications and data. Limit ranges apply to the maximum processing and
memory resources and total object count for each components. The limits
for each component are outlined here</p>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 88%" />
</colgroup>
<thead>
<tr class="header">
<th>Project component</th>
<th>Limits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pod</td>
<td>CPU and memory per pod, and total pods per project</td>
</tr>
<tr class="even">
<td>Container</td>
<td>CPU and memory per container, default memory and CPU, maximum
burstable ratio per container, and total containers per project</td>
</tr>
<tr class="odd">
<td><code>ImagesMaximum</code></td>
<td>image size for the internal registry</td>
</tr>
<tr class="even">
<td>Image</td>
<td><code>streamMaximum</code> image tag references and image references
per image stream</td>
</tr>
<tr class="odd">
<td>Persistent</td>
<td>volume <code>claimsMinimum</code> and maximum storage request size
per PVC</td>
</tr>
</tbody>
</table>
<p>Before an application is deployed or scaled up the project limit
ranges are analyzed to confirm that the request is within the limit
range. If a project limit range does not allow the desired action then
it does not happen.</p>
<p>For example if a project limit range defines the memory per pod as
being between 50 MB and 1,000 MB a request for a new application
deployment with a defined quota of 1,500 MB will fail because it is
outside the pod memory limit range for that project. Limit ranges have
the additional benefit of being able to define default compute resource
values for a project. When you deployed <code>app-gui</code> and
<code>app-cli</code> you had not yet defined a limit range for the
<code>image-uploader</code> project and did not specify the resources
for each deployment so each application pod was deployed with no
resource constraints. In OpenShift a deployment with no defined
resources quota.</p>
<p>If users start accessing the gui heavily it can consume resources to
the point that the performance of the app-cli deployment is affected for
a busy cluster with multiple users and running application that is a
major problem, with limit ranges you can define the default compute
resources for a project that does not specify a quota to prevent this
from happening in your cluster.</p>
<h3 id="define-resource-limit-ranges">Define resource limit ranges</h3>
<p>Limit ranges define the minimum and maximum RAM and CPU an
application can be allocated when its deployed. In addition to the top
and bottom of the range you can specify default request value and
limits. The difference between an application requested value, and its
maximum value limit is called the burstable range</p>
<p>Let us create a limit range for the <code>image-uploader</code>
project using the template, create the file containing that content and
apply it to the</p>
<div class="sourceCode" id="cb82"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;v1&quot;</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;LimitRange&quot;</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;core-resource-limits&quot;</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">limits</span><span class="kw">:</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">type</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;Pod&quot;</span></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">max</span><span class="kw">:</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;2&quot;</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1Gi&quot;</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">min</span><span class="kw">:</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;100m&quot;</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;4Mi&quot;</span></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">type</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;Container&quot;</span></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">max</span><span class="kw">:</span></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;2&quot;</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1Gi&quot;</span></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">min</span><span class="kw">:</span></span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;100m&quot;</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;4Mi&quot;</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">default</span><span class="kw">:</span></span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;300m&quot;</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;200Mi&quot;</span></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">defaultRequest</span><span class="kw">:</span></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;200m&quot;</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;100Mi&quot;</span></span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">maxLimitRequestRatio</span><span class="kw">:</span></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;10&quot;</span></span></code></pre></div>
<p>To define a limit range a user need to have the cluster admin role.
To log in as the admin user run the following command, since we have
already setup a user like that, we can use the following command to do
so</p>
<div class="sourceCode" id="cb83"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure that we are logged in with a user that has a cluster-admin role</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc login <span class="at">-u</span> kubeadmin <span class="at">-p</span> <span class="op">&lt;</span>kubeadmin-password<span class="op">&gt;</span> <span class="op">&lt;</span>cluster-host-address<span class="op">&gt;</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the resource limits for the project or namespace alone</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc apply <span class="at">-f</span> openshift/limit-ranges.yml <span class="at">-n</span> image-uploader</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># we can then list them and optionally describe the object for more details</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get limitranges <span class="at">-n</span> image-uploader</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   CREATED AT</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a><span class="ex">core-resource-limits</span>   <span class="op">&lt;</span>date<span class="op">&gt;</span></span></code></pre></div>
<p>After which we can simply run the apply to create the new resource
limit range for the project or namespace, in this case we are using the
<code>image-uploader</code> project, other project can be used instead
just change the value of the -n argument in the command.</p>
<p>Throughout this section and the rest of the document we will need to
create YAML template files that are referenced in the <code>oc</code>
commands .We use relative file name paths to keep the examples easy to
read, but if you are not running <code>oc</code> from the directory
where those files are created, be sure to reference the full path when
you run the command</p>
<p>You can use the command line to confirm that the
<code>image-uploader</code> project limit range was created and to
confirm that the settings you specified in the template were accurately
read. As with every other resource in the OpenShift cluster we can do
this by using the get command - <code>oc get limitrange</code>. Those
can be combined with a describe command to list the details for each of
these resource limits</p>
<p>You can also use the web interface to confirm that the limit range
you just set. Using the Resources &gt; Quotas. Limit ranges act on a
components in a project. They also provide default resource limits for
deployments that do not provide any specific values themselves But they
do not provide project wide limits to specify maximum resource mounts.
For that you will need to define a resource quota for
<code>image-uploader</code> project. In the next section that is exactly
what will be done</p>
<h2 id="resource-quotas">Resource quotas</h2>
<p>Nobody likes noise neighbor, and OpenShift users are no different. If
one project users were able to consume more than their fair share of the
resources in an OpenShift cluster all manner of resource availability
issues wold occur, for example a resource hungry development project
could stop applications in a production level project in the same
cluster from scaling up when their traffic is increased. To solve this
problem OpenShift uses project quotas to provide resource caps and
limits at the project level. Whereas limit ranges provide maximum
resource limits for an entire project, quotas on the other hand fall
into three primary categories:</p>
<ul>
<li>compute &amp; storage resources - memory, processing (CPU) etc</li>
<li>object counts - services, storage claims, config maps, secrets,
replication controllers etc</li>
</ul>
<p>In one of the very first sections, we discussed the pod life-cycles,
project quotas apply only to pods that are not in a terminal phase.
Quotas apply to any pod in a pending running or unknown state. Before an
application deployment is started or a deployed application is changed
OpenShift evaluates the project quotas.</p>
<p>In the next section you will create a compute resource quota for the
<code>image-uploader</code> project</p>
<h3 id="creating-compute-quotas">Creating compute quotas</h3>
<p>Compute resource quotas apply to CPU and memory allocation, They are
related to limit ranges because they represent quotas against totals for
requests and limits for all application in a project. You can set the
following six values with compute resource quotas.</p>
<ul>
<li>cpu, request.cpu - total of all CPU requests in a project typically
measured in cores or millicores CPU and requests.cpu are synonyms and
can be used interchangeable.</li>
<li>memory, request.memory - total of all memory requests in a project
typically expressed in mega or gigabytes or memory and requests.memory
are synonyms and can be used interchangeable</li>
<li>limits.cpu - total for all CPU limits in a project</li>
<li>limits.memory - total for all memory limits in a project</li>
</ul>
<p>In addition to the quotas you can also specify the scope the quota
applies to. There are four quotas scopes in OpenShift.</p>
<p><code>- Terminating</code> - Pods that have a defined life cycle.
Typically these are builder and deployment pods.</p>
<ul>
<li><code>NotTerminating</code> - Pods that do not have a defined life
cycles. This scopes include application pods like the app-gui and
app-cli and most other applications you will deploy in OpenShift.</li>
<li><code>BestEffort</code> - Pods that have a best-effort quality of
service, for processing and memory. Best-effort deployment are those
that did not specify a request or limit when they were created.</li>
<li><code>NotBestEffort</code> - Pods that do not have a best effort
quality of service, for processing and memory, that is the inverse of
<code>BestEffort</code> this scope is useful when you have a mixture of
low priority transient workloads that have been deployed with best
effort quality of service and higher priority workloads with dedicated
resources</li>
</ul>
<p>To create an new quota for a project cluster admin privileges are
required. That means you need to be logged in as the admin user to run
this command, because the developer user has only the edit role bound to
it, for the <code>image-uploader</code> project and has no privileges
for the rest of the cluster. To log in as the admin user follow the
previous section, where we already did that for the range limits</p>
<div class="sourceCode" id="cb84"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ResourceQuota</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> compute-resources</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">hard</span><span class="kw">:</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">pods</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;10&quot;</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests.cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;2&quot;</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests.memory</span><span class="kw">:</span><span class="at"> 2Gi</span></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">limits.cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;3&quot;</span></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">limits.memory</span><span class="kw">:</span><span class="at"> 3Gi</span></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">scopes</span><span class="kw">:</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> NotTerminating</span></span></code></pre></div>
<p>Save the template to a file, and execute the commands below to apply
the resource quota object, this will ensure that the
<code>image-uploader</code> project is now properly restricted by both
limit ranges, and now by the resource quotas. Which will prevent us
from:</p>
<ul>
<li><p>due to the limit ranges - we will never be able to deploy
something that might hog or request too many resources in the
<code>image-uploader</code> project</p></li>
<li><p>due to the resource quotas - during the active execution of our
containers inside the <code>image-uploader</code> project they will
never be able to take more runtime resources than allowed</p></li>
</ul>
<div class="sourceCode" id="cb85"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure that we are logged in with a user that has a cluster-admin role</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc login <span class="at">-u</span> kubeadmin <span class="at">-p</span> <span class="op">&lt;</span>kubeadmin-password<span class="op">&gt;</span> <span class="op">&lt;</span>cluster-host-address<span class="op">&gt;</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="co"># first create the resource quota from the template</span></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc apply <span class="at">-f</span> openshift/resource-quotas.yml <span class="at">-n</span> image-uploader</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co"># we can then list them and optionally describe the object for more details</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get resourcequotas <span class="at">-n</span> image-uploader</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                AGE    REQUEST                                                 LIMIT</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a><span class="ex">compute-resources</span>   106s   pods: 2/10, requests.cpu: 0/2, requests.memory: 0/2Gi   limits.cpu: 0/3, limits.memory: 0/3Gi</span></code></pre></div>
<h3 id="creating-resource-quotas">Creating resource quotas</h3>
<p>Resource quotas track all resources in a project that are deployed by
Kubernetes. Core components in OpenShift like deployment configs and
build configurations are not covered by quotas, that is because these
components are created on demand for each deployment and controller by
OpenShift.</p>
<p>The components that are managed by resource quotas are the primary
resources in an OpenShift cluster that consume storage and compute
resources, keeping track of a project’s resources is important when you
need to plan how to grow and manage your OpenShift cluster to
accommodate your application. The following components are tracked by
resource quotas</p>
<ul>
<li>config maps - we discussed config maps in previous sections, they
provide a way to configure and define data for containers</li>
<li>persistent volume claims - applications requests for persistent
storage</li>
<li>resource quotas - the total number of quotas per project</li>
<li>replication controller - the number of controllers in a project.
This is typically equal to the number of deployed applications but you
can also manually deploy applications using different workloads that
could make this number change.</li>
<li>secrets - we discussed them in previous section, they are a
variation of the config maps</li>
<li>services - the total number of services in a project</li>
<li>image streams - the total number of image streams in a project</li>
</ul>
<p>Most of the items in this list should look familiar, we have been
discussing them for several sections at this point. The following
listing shows the resource quotas template that you need to apply to the
<code>image-uploader</code> project to do this apply the following
file</p>
<div class="sourceCode" id="cb86"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ResourceQuota</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> object-counts</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">hard</span><span class="kw">:</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">configmaps</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;10&quot;</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">persistentvolumeclaims</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;5&quot;</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">resourcequotas</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;5&quot;</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">replicationcontrollers</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;20&quot;</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">secrets</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;50&quot;</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">services</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;10&quot;</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">openshift.io/Image streams</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;10&quot;</span></span></code></pre></div>
<p>Save the template to a file, and execute the commands below to apply
the resource quota object, this will ensure that the
<code>image-uploader</code> project is now properly restricted by
OpenShift object or resource counts as well</p>
<div class="sourceCode" id="cb87"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first create the resource quota from the template</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc apply <span class="at">-f</span> openshift/storage-resource-quotas.yml <span class="at">-n</span> image-uploader</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we can then list them and optionally describe the object for more details</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get resourcequotas <span class="at">-n</span> image-uploader</span></code></pre></div>
<h2 id="working-with-quotas-limits">Working with quotas &amp;
limits</h2>
<p>Now that the <code>image-uploader</code> project has limit ranges and
quotas it is time to put them through their paces. The compute quota for
the app is not being reflected yet and your first task is to fix
that</p>
<h3 id="quotas-to-existing-applications">Quotas to existing
applications</h3>
<p>When you deployed the apps in previous sections no quotas or limits
were defined for the <code>image-uploader</code> project. As we
mentioned when you were creating limit ranges back then you cluster was
essentially the wild west and any deployed application could consume any
amount of resources in the cluster. If an application is created and
there are no limit ranges to reference an no resources were requested as
when you deployed the metrics pod. The linux kernel components that
define the resource constraints for each container are created with
unlimited values for the resources limits. This is what happened when
you deployed the app-cli and the app-gui and why their CPU and memory
quotas are not reflected in OpenShift.</p>
<p>Now that you have applied limit ranges and quotas to the
<code>image-uploader</code> project you have OpenShift to re-create the
containers for these applications to include these constraints, The
easiest way to to do this is to delete the current pods for each
application. When you run the following <code>oc delete command</code>
OpenShift will automatically deploy new pods that contain the default
limit ranges, that you defined in the previous section.</p>
<div class="sourceCode" id="cb88"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will delete all pod resources related to those labels, note that  we are only deleting the pods, nothing else,</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the deployment and in particular the replication set will ensure that the pods are re-created with the correct limits</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc delete pod <span class="at">-l</span> deployment=app-cli</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc delete pod <span class="at">-l</span> deployment=app-gui</span></code></pre></div>
<p>Because you did not specify specific resource values your new app-gui
and app-cli pods inherit the default request values defined in the
core-resource-limits limit range object. Each pod was assigned 200
millicores and 100 MIB of RAM. You can see that in the previous output
that the consumed CPU and memory quotas for the
<code>image-uploader</code> project are twice the default request.</p>
<p>It is definitely not a best practice to start using projects without
having set limits and resources first, but we had to start somewhere,
and if the very first few sections was all about quotas you would never
have gotten to this section, so for teaching purposes, we began using
OpenShift without discussing proper configuration rules</p>
<h3 id="changing-quotas-for-deployed-applications">Changing quotas for
deployed applications</h3>
<p>When you deploy a new application you can specify limits and quotas
as part of its definition. You can also edit the YAML definition for an
existing deployment config directly from the command line. To edit the
resource limits for your deployment run the following
<code>oc edit command</code> which lets you edit the current YAML
definition for the application.</p>
<div class="sourceCode" id="cb89"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will open your default system editor, and allow you to edit the contents of the manifest as if it was being done</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># directly and interactively</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc edit deployment/app-cli</span></code></pre></div>
<p>To edit the resource limits you need to find the
<code>spec.containers.resources</code> section of the configuration.
This section is currently empty, because nothing was defined for the
application when it was initially deployed, we will change that.</p>
<div class="sourceCode" id="cb90"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;750m&quot;</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;500Mi&quot;</span></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">limits</span><span class="kw">:</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1&quot;</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1000Mi&quot;</span></span></code></pre></div>
<p>This defines our pod as burstable, because the maximum limits that
are defined are higher than the request, meaning that the pod can burst
up to 1 CPU or that would mean 1000 millicores, as we have only
requested 3/4 of that - 750 millicores, and the memory limit is twice as
much as requested</p>
<p>Savings the new configuration will trigger a new deployment for the
app-cli this new deployment will incorporate your new resource requests
and limits. Once the build completes your deployment will be available
with more guaranteed resources, you can also verify this with the
regular describe command, or through the web UI console.</p>
<p>You can edit a deployment config to make complex changes to deployed
applications but it is manual process. For new applications deployments
your project should use the default limit ranges whenever possible to
inherit default values</p>
<p>While your resource requests and limit ranges are new and fresh in
your mind let us dig a little deeper and discuss how these constraints
are enforced in OpenShift by the Linux kernel and the container runtime
- like docker or containerd, using cgroups</p>
<h2 id="cgroups-for-managing-resources">Cgroups for managing
resources</h2>
<p>Cgroups are Linux kernel components, that provide per process limits
for CPU, memory and network bandwidth and block storage bandwidth. In an
OpenShift cluster they enforce the main limits and quotas configured for
applications and projects.</p>
<h3 id="overview-of-the-cgroups">Overview of the cgroups</h3>
<p>Cgroups are defined in a hierarchy in the
<code>/sys/fs/cgroup/</code> directory on the application node. Within
this directory is a directory for each type of cgroup controller that is
available. A controller represents a specific system resource that can
be controller by cgroups. In this section we are focusing on the cpu and
memory cgroups controllers. In the directories for the cpu and memory
controllers is a directory named <code>kubepod.slice.</code> Cgroups
slices are used to create subdivisions within the cgroups controller.
Slices are used as logical dividers in a controller and define resource
limits for groups of resources below them in the cgroup hierarchy.</p>
<div class="sourceCode" id="cb91"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># login into the node, and navigate to the directory, then we can actually see the structure of this directory</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls /sys/fs/cgroup</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 16:03 cgroup.controllers</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span>  1 root root 0 Jun 11 17:24 cgroup.max.depth</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span>  1 root root 0 Jun 11 17:24 cgroup.max.descendants</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span>  1 root root 0 Jun 11 16:03 cgroup.procs</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 cgroup.stat</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span>  1 root root 0 Jun 11 17:22 cgroup.subtree_control</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a><span class="ex">-rw-r--r--.</span>  1 root root 0 Jun 11 17:24 cgroup.threads</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 16:03 cpu.stat</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 16:03 cpuset.cpus.effective</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 cpuset.cpus.isolated</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 cpuset.mems.effective</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 dev-hugepages.mount</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 dev-mqueue.mount</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 init.scope</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 16:03 io.stat</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  4 root root 0 Jun 11 16:04 kubepods.slice <span class="op">&lt;</span>- here is the kubepods directory we care about</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  3 root root 0 Jun 11 16:03 machine.slice</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 memory.numa_stat</span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a><span class="ex">--w-------.</span>  1 root root 0 Jun 11 17:24 memory.reclaim</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 16:03 memory.stat</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 misc.capacity</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a><span class="ex">-r--r--r--.</span>  1 root root 0 Jun 11 17:24 misc.current</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:40 proc-fs-nfsd.mount</span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 sys-fs-fuse-connections.mount</span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 sys-kernel-config.mount</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 sys-kernel-debug.mount</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  2 root root 0 Jun 11 16:03 sys-kernel-tracing.mount</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span> 34 root root 0 Jun 11 17:02 system.slice</span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span>  3 root root 0 Jun 11 16:19 user.slice</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a><span class="co"># if we navigate to that directory we will see that, indeed it has two directories, which we need to look at</span></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls kubepod.slice <span class="kw">|</span> <span class="fu">grep</span> kube</span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a><span class="ex">kubepods-besteffort.slice</span></span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a><span class="ex">kubepods-burstable.slice</span></span></code></pre></div>
<p>The <code>kubepods</code> slice is where the configuration to enforce
OpenShift requests and limits are located. Within the
<code>kubepod.slice</code> are two slices
<code>kubepods-besteffort.slice</code> and
<code>kubepods-burstable.slice.</code> These two slices are how resource
limits for best-effort and burstable quality of service levels that we
have discussed in this section are enforced. Because you defined
resource requests for app-cli and app-gui they both will be defined in
<code>kubepods-burstable.slice.</code> Within the
<code>kubepod-besteffort.slice</code> and the
<code>kubepods-burstable.slice</code> are multiple additional slices.
There is not an immediate identifier to tell you which slice contains
the resource information for a give container, but you can get that
information directly from docker on your application node.</p>
<h3 id="identifying-container-cgroups">Identifying container
cgroups</h3>
<p>To determine which cgroup slice controls the resources for your
deployment, we need to get the cgroup information from the container
runtime. The cgroup slice that each container belongs to is listed in
the information from the inspect command. To obtain filter on the
<code>cgroupsPath</code> element accessor. This limits the output to
only the cgroup slice information. In your example the cgroup slice for
the app-cli is the following long id, which signifies the type is
burstable as well, we can see that from the value for the
<code>cgroupsPath</code> key in the inspect command -</p>
<div class="sourceCode" id="cb92"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here you will notice that we get two outputs that is because we have two different replicas for our deployment, but</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="co"># that will vary depending on your current state of the deployment</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl ps <span class="kw">|</span> <span class="fu">grep</span> app-cli <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1&#39;</span>} <span class="kw">|</span> <span class="fu">xargs</span> <span class="at">-I</span><span class="st">&#39;{}&#39;</span> crictl inspect {} <span class="kw">|</span> <span class="fu">grep</span> cgroupsPath</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;cgroupsPath&quot;</span><span class="ex">:</span> <span class="st">&quot;kubepods-burstable-pod70aff84f_0aa8_47ed_9b8c_cd3d6a185708.slice:crio:7f4ec5610c6a8f4110f43ac2d415d6d20449ff916bc8eb81407bfbde3438680a&quot;</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;cgroupsPath&quot;</span><span class="ex">:</span> <span class="st">&quot;kubepods-burstable-pod58448c42_a7bd_4996_9012_849df2cffa5c.slice:crio:2f1a0714812de8170f791ddf3dc008b39c9a020028f1b42418ed47133092859e&quot;</span></span></code></pre></div>
<p>As we mentioned, we are in the burstable slice still. The slice
defined in the app-cli inspect output that is. Slices do not define
resource constraints for individual containers but they can set default
values for multiple containers. That is why the hierarchy of slices look
a little excessive here. You have one more layer to go to get to the
resource constraints for the app-cli container. In the lowest slice is a
scope directory. Each scope is named after the full hash that a
container’s short IO is based on. In our example app-cli resource
constraints are defined in the scope named</p>
<div class="sourceCode" id="cb93"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># navigate to the burstable slice sub directory, and check out the output of the following ls command, we can see that</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for the two pods that we have with the respective IDs that start with 80aff and 58448, there are two slices</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cd /sys/fs/cgroup/kubepods.slice/kubepods-burstable.slice</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="co"># grep the directory content for the two pods, based on the pod id, we can see</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;pod\(70aff\|58448\)&quot;</span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="ex">kubepods-burstable-pod58448c42_a7bd_4996_9012_849df2cffa5c.slice</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="ex">kubepods-burstable-pod70aff84f_0aa8_47ed_9b8c_cd3d6a185708.slice</span></span></code></pre></div>
<p>Cgroups configurations are created on an OpenShift application nodes
using this process. It is a little complex and because cgroups are
listed according to the cgroup controller and not the PID they manage,
troubleshooting them can be a challenge on a busy system. When you need
to see the cgroup configuration for a single container, the process is
more straightforward. In the next section we will look at how the cgroup
information from the host is mounted in each container that is
created</p>
<h3 id="confirming-cgroups-limits">Confirming cgroups limits</h3>
<p>When a container runtime creates a container, it mounts the cgroup
scope that applies to it in the container, in the
<code>/sys/fs/cgroup</code> directory, it truncates the slices and scope
so the container appears to have only a single cgroup controller. We are
going to focus on the limits that enforce CPU and memory constraints for
the app-cli container. Let us begin with the limits for the CPU
consumption. To start an interactive shell prompt in your running
container, run the following command edited to reference your container
short ID</p>
<div class="sourceCode" id="cb94"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure to select one of the containers that are currently being active for the deployment, that could be</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="co"># either one of those, or you can scale down your deployment to one replica in case this is confusing</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl ps <span class="kw">|</span> <span class="fu">grep</span> app-cli <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1&#39;</span>}</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="ex">7f4ec5610c6a8</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="ex">2f1a0714812de</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="co"># we use the crictl, or the containerd runtime to log into the container through the interactive use of /bin/bash</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl exec <span class="at">-it</span> 7f4ec5610c6a8 /bin/bash</span></code></pre></div>
<p>As we discussed earlier in this chapter CPU resources in OpenShift
are allocated in millicores or one-thousands of the CPU resources
available on the server. For example if your application node has two
processors, a total of 2,000 millicores is available for the containers
on the node. The ration expressed here is what is represented in the cpu
cgroup. The actual number is not expressed in the same units, but the
ratios are always the same. The app-cli container has request of 750
millicores, with the limit of 1,000 millicores or one CPU. You need the
following two values form <code>/sys/fs/cgroup/cpu.max</code> to build a
ratio that confirms that the limit for the app-cli container is correct
configured. The file is one line, which defines two values, divided by
space, the meaning of the two values is described below:</p>
<ul>
<li><p><code>cat cpu.max | awk '{print $1}'</code> - the time period in
microseconds during which the cgroup quota for the processor access is
measured and reallocated, this can be manipulated to create different
processing quota ratios for different applications.</p></li>
<li><p><code>cat cpu.max | awk '{print $2}'</code> - the time in
microseconds that the cgroup is allowed to access the processor during
the defined period, the period of time is adjustable. For example if
that <code>value</code> value is 100, the cgroup will be allowed to
access the processor 100 microseconds during the set period, if that
period is also 100 , that means the cgroup ha unlimited access to the
processor, on the system. If the period were set to 1000, the process
would have access to the processor for 100 microseconds out of every
1,000.</p></li>
</ul>
<p>For the app-cli this cgroup limits the container access to 1 CPU
during 100,000 out of every 100,000 microseconds. If you convert these
values to a ratio app-cli is allocated a maximum of 1,000 millicores of
1 CPU. That is the limit we have set for app-cli. This is how CPU time
limits are managed for each container in an application deployment Next
let us look at how the request values are controller by cgroups.</p>
<p>The request limit for app-cli is managed by the value in
<code>/sys/fs/cgroup/cpu/cpu.weight</code>. This value is a ratio of CPU
resources relative to all the cores on the system.</p>
<p>The memory limit for the app-cli is controlled by the value in
/sys/fs/cgroup/memory/memory.current, there are some other files like
memory.max memory.min and memory.peak, which are mostly self
explanatory, the one that is more interesting is the memory.max, which
defines the maximum amount of allowed memory that the container can take
up which in this case matches perfectly with the limits configuration,
meaning that it has a value of - <code>1048576000</code> - which is as
configured a maximum amount of 1GB, The memory.current, varies and
depends on the app that is running for app-cli that value is at 350MB -
<code>34635776</code></p>
<p>Resource limits for OpenShift containers are enforced with kernel
cgroups. The only exception is hte memory request value. There is no
cgroup to control the minimum amount of RAM available to a process this
value is primarily used to determine which node a pod is assigned to in
your OpenShift cluster.</p>
<p>This section covered a lot of what is required to create and maintain
a healthy OpenShift cluster. We have gone far down into the Linux kernel
to confirm how container resources limits are enforced. Although limits
requests and quotas are not the most exciting things to work through
they are absolutely critical and essential component of OpenShift ready
to handle production workloads effectively.</p>
<p>You cluster is now connected with an authentication database and the
project you have been working on has effective resource limits and
quotas. In the following sections we will keep building on that
momentum</p>
<p>To summarize this is how the values are distributed, note that this
table is using cgroup v1 values as reference, the formula for cgroup v2
is a bit different. It is worth noting that during the last couple of
years the container runtimes have moved to the cgroups v2, which change
a few things among which are the default values for different limits,
such as the cpu.weight, which in the older cgroups v1 were called
cpu.shares. The shares value and weight have different scaling and they
are calculate very differently, most container runtimes still use the
old cgroups v1 as reference, and they adjust and scale the value to
match cgroups v2 specification when creating the containers. The reason
being that the OCI - Open container reference spec, was written with
cgroups v1 in mind</p>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 50%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>OpenShift Value</th>
<th>cgroup v1 File</th>
<th>cgroup v2 File</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cpu: 1000m limit</td>
<td>cpu.max</td>
<td>cpu.max</td>
</tr>
<tr class="even">
<td>cpu: 750m request</td>
<td>cpu.cfs_quota_us, cpu.cfs_period_us</td>
<td>cpu.weight</td>
</tr>
<tr class="odd">
<td>memory: 500Mi request</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr class="even">
<td>memory: 1000Mi limit</td>
<td>memory.max</td>
<td>memory.max</td>
</tr>
</tbody>
</table>
<h1 id="networking">Networking</h1>
<p>The importance of the network design configuration in an OpenShift
cluster can not be overstated, it is the fabric of what binds you
cluster together. With that perspective in mind OpenShift does a lot of
work to make sure its networking configuration is stable performs well
and is highly configurable and available. Those principles are what we
will cover in this section. Let us start with an overview of how the
network stack in OpenShift is designed</p>
<h2 id="managing-the-sdn">Managing the SDN</h2>
<p><code>OVS</code> is an enterprise grade scalable high performance
software defined network - in OpenShift its the default SDN used to
create the pod network in your cluster, it is installed by default when
you deploy OpenShift. <code>OVS</code> runs as a service on each node in
the cluster, you can check the status of the service by running the
following <code>systemctl</code> command on any node</p>
<div class="sourceCode" id="cb95"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># firs login into the cluster node, and then query systemd for the status of the service on the node</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-i</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> systemctl status ovs-vswitchd</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="co"># you will see something like this, which shows that the service is running and enabled out of the box</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="ex">●</span> ovs-vswitchd.service <span class="at">-</span> Open vSwitch Forwarding Unit</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>     <span class="ex">Loaded:</span> loaded <span class="er">(</span><span class="ex">/usr/lib/systemd/system/ovs-vswitchd.service</span><span class="kw">;</span> <span class="ex">static</span><span class="kw">)</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Drop-In:</span> /etc/systemd/system/ovs-vswitchd.service.d</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>             <span class="ex">└─10-ovs-vswitchd-restart.conf</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>     <span class="ex">Active:</span> active <span class="er">(</span><span class="ex">running</span><span class="kw">)</span> <span class="ex">since</span> <span class="op">&lt;</span>date<span class="op">&gt;</span><span class="kw">;</span> <span class="op">&lt;</span>time<span class="op">&gt;</span> ago</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>   <span class="ex">Main</span> PID: 1204 <span class="er">(</span><span class="ex">ovs-vswitchd</span><span class="kw">)</span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>      <span class="ex">Tasks:</span> 15 <span class="er">(</span><span class="ex">limit:</span> 152745<span class="kw">)</span></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>     <span class="ex">Memory:</span> 57.1M</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>        <span class="ex">CPU:</span> 1min 32.243s</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>     <span class="ex">CGroup:</span> /system.slice/ovs-vswitchd.service</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>             <span class="ex">└─1204</span> ovs-vswitchd unix:/var/run/openvswitch/db.sock <span class="at">-vconsole:emer</span> <span class="at">-vsyslog:err</span> <span class="at">-vfile:info</span> <span class="at">--mlockall</span> <span class="at">--user</span> openvswitch:hugetlbfs <span class="at">--no-chdir</span> <span class="at">--log-file</span><span class="op">=</span>/var/log/openvswitch/ovs-vswitchd.log <span class="at">--pidfile</span><span class="op">=</span>/var/run/openvswi<span class="op">&gt;</span></span></code></pre></div>
<p><code>The service is automatically enabled on cluster nodes as part of the OpenShift deployment. The configuration file for the OVS service is located at /etc/systconfig/opensvwitch and each node's local OVS database is located in the /etc/openswitch directory For  day-to-day operations. OVS should be transparent. Its configuration and updates are controller by OpenShift. Using OVS provides several advantages to OpenShift. jthis transparent operation is possible because OpenShift uses the Kubernetes container network interface - the container network interface provide a plugin architecture to integrate different solutions to create and mange the pod network. OpenShift uses OVS as its default but it can function with other network providers as well.</code></p>
<p>The <code>OVS</code> used in your OpenShift cluster is the
communication backbone for all your deployed pods, traffic in an out of
ever pod is affected by it, in the OpenShift cluster. For that reason
you need to know how it works and how to effectively use it for your
needs. Let us start with the network configuration of your OpenShift
application node.</p>
<h3 id="configure-application-node-network">Configure application node
network</h3>
<p>When a node is added to an OpenShift cluster several network
interfaces are created in addition to the standard loopback interface
and <code>eth0</code> physical interface. For our purposes we will call
<code>eth0</code> the physical interface even though you are using a
virtual machine for your cluster. That is because OpenShift creates the
following additional virtual interface.</p>
<ul>
<li><p><code>br0</code> - An <code>OVS</code> bridge all OpenShift
interfaces are associated with. <code>OVS</code> creates this interface
when the node is added to the OpenShift cluster.</p></li>
<li><p><code>tun0</code> attached to <code>br0</code>. Acts as the
default gateway for each node. Traffic in and out of your OpenShift
cluster is routed through this interface.</p></li>
<li><p><code>vxlan_sys_4789</code> - also attached to <code>br0</code>
this virtual extensible local area network is encrypted and used to
route traffic to containers on other nodes in your cluster. It connects
the nodes in your OpenShift cluster to create your pod network.</p></li>
</ul>
<p>Additionally each pod has a corresponding virtual Ethernet interface
that is linked to the <code>eth0</code> interface in the pod by the
Linux kernel. Any network traffic that is sent either interface in this
relationship is automatically presented to the other. All of these
relationships are</p>
<p><code>What are Linux bridges, TUN interfaces, and VXLAN - a Linux bridge is a virtual interface that is used to connect other interfaces together, If two interfaces on a host are attached to abridge they can communicate with each other without routes needing to be created. This help with communication speed as well as keeping networking configuration simple on the host and in the container. A VXLAN is a protocol that acts as an overlay network between teh nodes in your OpenShift cluster, an overlay network is a software defined networks that is deployed on top of another network. The VXLAN used in OpenShift are deployed on top of the networking configuration of the host To communicate securely between pods, the VXLAN encapsulates pod network traffic in an additional layer of network information so it can be delivered to the proper pod on the proper sever by IP address. The overlay network is the pod network in your OpenShift cluster. The VXLAN interface on each node provide access to and from that network. You can find the full definition and specification for VXLAN and at its RFC doc.</code></p>
<p>You can see these interfaces on your application nodes by running the
IP command. The following sample output has been trimmed with a little
command line magic for brevity and clarity:</p>
<div class="sourceCode" id="cb96"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ip a <span class="kw">|</span> <span class="fu">egrep</span> <span class="st">&#39;^[0-9].*:&#39;</span> <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1 $2}&#39;</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="ex">1:lo:</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="ex">2:ovs-system:</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="ex">3:ovn-k8s-mp0:</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="ex">4:br-int:</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="ex">5:eth10:</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="ex">6:tap0:</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="ex">8:br-ex:</span></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="ex">9:cf3abd918c8e682@if2:</span></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a><span class="ex">10:9943d81586b22a8@if2:</span></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="ex">12:363ff29e19ebff4@if2:</span></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="ex">14:b2cf78b2daa4f7b@if2:</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code></pre></div>
<p>The networking configuration for the master node is essentially the
same as an application node. The master node uses the pod network to
communicate with pods on the application nodes as they are deployed
deliver their application and are eventually deleted. In the next
section we will look at more deeply at how the interface in the
container is linked to a corresponding <code>veth</code> interface on
the cluster node.</p>
<h3 id="linking-containers-to-host-interfaces">Linking containers to
host interfaces</h3>
<p>In previous sections we talked about the network namespace and how
each container contains a unique loopback and <code>eth0</code>
interface for network communication. Form the perspective of application
in a container these two interface are the only networks on the host, to
get network traffic in an out of the container the <code>eth0</code>
interface in the container is linked in the Linux kernel to a
corresponding <code>veth</code> interface in the host’s default network
namespace</p>
<p>The ability to link two interfaces is a feature of the linux kernel.
To determine which <code>veth</code> interface a container is linked to
you need to log into the application node where the container is running
you can figure this out in just a few steps let us use the app-cli as an
example</p>
<div class="sourceCode" id="cb97"><pre
class="sourceCode sh"><code class="sourceCode bash"></code></pre></div>
<p>Any virtual interface on a Linux system can be linked by the kernel
to another virtual or physical interface. When an interface is linked to
another the kernel makes them essentially the same interface. If
something happens to one interface it automatically happens to its
linked interface in an interface <code>iflink</code> file a file created
and maintained by the running Linux kernel at
<code>/sys/class/net/&lt;interface-name&gt;/iflink</code> is the index
number for its linked interface. To find the linked interface number for
the app-cli container run the following <code>oc</code> exec command
making sure to use the pod ID for you app-cli deployment. This command
uses the cat command line tool to echo the contents of the
<code>iflink</code> file.</p>
<div class="sourceCode" id="cb98"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the file contains only a single number which in this case represents the number of the linked interface</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc exec app-cli-7976b4c888-rdbv7 <span class="at">--</span> cat /sys/class/net/eth0/iflink</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="ex">115</span></span></code></pre></div>
<p>The eth0 interface in the app-cli pod is linked to interface 115, on
the application node. But which veth interface is number 115 ? That
information is available in the output form the IP command. The link ID
also called the <code>ifindex</code> for each interface is the number at
the beginning of each interface listed in the command. For each eth0
interface in a container its <code>iflink</code> value is the
<code>ifindex</code> value of its corresponding veth interface.</p>
<div class="sourceCode" id="cb99"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure you have entered the cluster node in the first place</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh crc</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="co"># grep for the virtual interface, that exactly contains and starts with the number we extracted from the iflink file</span></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="co"># above, that would be 115 in our example, but you result and interface link number will be different</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ip a <span class="kw">|</span> <span class="fu">grep</span> <span class="at">-A</span> 3 <span class="st">&#39;^115.*:&#39;</span></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="ex">115:</span> 8a4aad65b71dfe8@if2: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1400 qdisc noqueue master ovs-system state UP group default</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 5a:2a:70:d2:bc:92 brd ff:ff:ff:ff:ff:ff link-netns fc30e1db-0655-46e5-bfac-157def87be33</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::582a:70ff:fed2:bc92/64 scope link</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span></code></pre></div>
<p>We have now confirmed that the app-cli pod is linked by the Linux
kernel to the virtual interface [8a4aad65b71dfe8] - on the cluster node.
This is how network traffic enters and exist containers in general. Next
let us confirm that this veth on the node is connected to the cluster’s
pod network so network traffic can get in and out of the OpenShift
cluster</p>
<h3 id="working-with-ovs">Working with OVS</h3>
<p>The command line tool to work with <code>OVS</code> directly is
<code>ovs-vsctl</code>. To use this tool you need to be logged in on to
the host cluster you are looking for information about. In these
examples we are logged in the cluster node already.</p>
<p>We mentioned earlier that all OpenShift SDN interfaces are attached
to an <code>OVS</code> ridge named <code>br0</code>. We make this
distinctions of calling it an <code>OVS</code> bridge because it is a
bridge interface that is created and controller by <code>OVS</code>
itself. You can also create a bridge interface with the Linux kernel. A
linux bridge is created and managed using the <code>brctl</code>
command. You can confirm that the <code>bridge</code> interfaces are
being controller by <code>OVS</code> by running the following
<code>ovs-vsctl</code> command to list active <code>OVS</code>
bridges</p>
<div class="sourceCode" id="cb100"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh crc</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-</span> i</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ovs-vsctl list-br</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we have two interfaces, from the names we can deduce that one is internal and the other is called external.</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="ex">ovs-vsctl</span> list-br</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="ex">br-ex</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a><span class="ex">br-int</span></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="co"># let us see what the internal one contains first, the output is abridged but these are the identities of the veth</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="co"># interface for each pod that is running in the cluster node, cluster has a lot of operator and utility pods that</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a><span class="co"># run besides our own app pods</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ovs-vsctl list-ifaces br-int</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a><span class="ex">0549f003ca15713</span></span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a><span class="ex">07ef7817039d9ff</span></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a><span class="ex">0dde308ad503bef</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a><span class="ex">...............</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a><span class="ex">1fd82e5f2d18b18</span></span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a><span class="ex">2427839e331de3f</span></span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a><span class="ex">252a3d9203d2239</span></span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a><span class="ex">363ff29e19ebff4</span></span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a><span class="ex">388c3b826f0b846</span></span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a><span class="ex">3d2158eb7ef5b90</span></span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a><span class="ex">46022cd19537541</span></span>
<span id="cb100-26"><a href="#cb100-26" aria-hidden="true" tabindex="-1"></a><span class="ex">496573f934b3956</span></span>
<span id="cb100-27"><a href="#cb100-27" aria-hidden="true" tabindex="-1"></a><span class="ex">4dc687e9870a5ac</span></span>
<span id="cb100-28"><a href="#cb100-28" aria-hidden="true" tabindex="-1"></a><span class="ex">50023d8075b388c</span></span>
<span id="cb100-29"><a href="#cb100-29" aria-hidden="true" tabindex="-1"></a><span class="ex">503b86107070900</span></span>
<span id="cb100-30"><a href="#cb100-30" aria-hidden="true" tabindex="-1"></a><span class="ex">51386e7bf3410e9</span></span>
<span id="cb100-31"><a href="#cb100-31" aria-hidden="true" tabindex="-1"></a><span class="ex">5859aa3a89d522d</span></span>
<span id="cb100-32"><a href="#cb100-32" aria-hidden="true" tabindex="-1"></a><span class="ex">646690890e9b6d1</span></span>
<span id="cb100-33"><a href="#cb100-33" aria-hidden="true" tabindex="-1"></a><span class="ex">76f5605e245b613</span></span>
<span id="cb100-34"><a href="#cb100-34" aria-hidden="true" tabindex="-1"></a><span class="ex">7c5e9807644e8cb</span></span>
<span id="cb100-35"><a href="#cb100-35" aria-hidden="true" tabindex="-1"></a><span class="ex">80c6c6f7b10f830</span></span>
<span id="cb100-36"><a href="#cb100-36" aria-hidden="true" tabindex="-1"></a><span class="ex">...............</span></span>
<span id="cb100-37"><a href="#cb100-37" aria-hidden="true" tabindex="-1"></a><span class="ex">8b4e358d3c1e190</span></span>
<span id="cb100-38"><a href="#cb100-38" aria-hidden="true" tabindex="-1"></a><span class="ex">8c88cf893b34724</span></span>
<span id="cb100-39"><a href="#cb100-39" aria-hidden="true" tabindex="-1"></a><span class="ex">8f2491e5cd6475b</span></span>
<span id="cb100-40"><a href="#cb100-40" aria-hidden="true" tabindex="-1"></a><span class="ex">ovn-k8s-mp0</span></span>
<span id="cb100-41"><a href="#cb100-41" aria-hidden="true" tabindex="-1"></a><span class="ex">patch-br-int-to-br-ex_crc</span> <span class="op">&lt;</span>- take a good note of this interface</span>
<span id="cb100-42"><a href="#cb100-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-43"><a href="#cb100-43" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ovs-vsctl list-ifaces br-ex</span>
<span id="cb100-44"><a href="#cb100-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-45"><a href="#cb100-45" aria-hidden="true" tabindex="-1"></a><span class="ex">patch-br-ex_crc-to-br-int</span> <span class="op">&lt;</span>- take a good note of this interface</span>
<span id="cb100-46"><a href="#cb100-46" aria-hidden="true" tabindex="-1"></a><span class="ex">tap0</span></span></code></pre></div>
<p>If you have not used Linux bridges before it can seem confusing when
you know a bridge should be present bu none appears when you run
<code>brctl</code>, because they are being managed by <code>OVS</code>
The node has a single <code>OVS</code> bridges named
<code>br-ex and br-int</code></p>
<p>The output of the command above, lists all interfaces connected to
this bridge br-int. This is how OpenShift SDN function, when a new pod
is deployed a new veth interface is created and attached to br-int. At
that point the pod can send and receive network traffic on the pod
network. It can communicate outside the cluster through the br-int and
br-ex</p>
<p>In the next section we will put the bridge interfaces and the SDN to
work by digging deeper into the how application traffic is routed and
how application communicate in your cluster. Let us start at the
beginning with a request for the app-cli deployment</p>
<h2 id="routing-application-requests">Routing application requests</h2>
<p>When you browse to the route of app-cli at -
http://app-cli-image-uploader.apps-crc.testing/ your request goes to the
node, first on port 80, the default HTTP port. Log in to the cluster
node and run the following netstat command to determine which service is
listening to port 80</p>
<div class="sourceCode" id="cb101"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> netstat <span class="at">-tpl</span> <span class="at">--numeric-ports</span> <span class="kw">|</span> <span class="fu">grep</span> 80</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="ex">tcp</span>        0      0 0.0.0.0:80              0.0.0.0:<span class="pp">*</span>               LISTEN      73357/haproxy</span></code></pre></div>
<p>There is an HAProxy service running on the port in the node, that is
interesting. HAProxy is the front door to your application in OpenShift.
HAProxy is an open source software defined load balancer and proxy
application. In OpenShift it takes the URL route associated with an
application and proxies those requests into the proper pod to get the
requested data back to the requesting user. Those requests into the
proper pod to get the requested data back to the requesting user. We
wont dig too much into all that HAProxy can do we are focusing on how
OpenShift uses HAProxy.</p>
<h2 id="using-the-haproxy-service">Using the HAProxy service</h2>
<p>The routed pod runs in the project named openshift-ingress in
OpenShift. The router pod handles incoming user requests for your
OpenShift cluster application and proxies them to the proper pod to be
served to the user. The router pod listens directly on the host
interface for the node its deployed on and uses the pod network to proxy
requests for different applications to the proper pod. This session then
returns to the user from the pod host through the TUN interface.</p>
<ol type="1">
<li>The user requests information for app-cli by the route’s URL which
connect to the OpenShift cluster node</li>
<li>The HAProxy pod takes the request URL and maps it to its
corresponding pod</li>
<li>HAProxy uses the pod network to proxy the connection to a node where
an app-cli pod is deployed</li>
<li>The request goes through the pod network and is passed into the
app-cli pod</li>
<li>The TUN interface attached to the bridge routes traffic to the host
network interface</li>
<li>The app-cli processes the request and sends the response through its
host TUN interface back to the user</li>
</ol>
<p>Because the router listens directly on the host interface its
configured differently than a typical pod in OpenShift. In the next
section we will investigate the HAProxy in more detail</p>
<p>How does HAProxy always deploy to the same node, in OpenShift it is
possible to tag an entire cluster application node with a label, then
when deploying specific pods you can tell them on which cluster node to
be deployed based on this label, similarly to how the names of the
namespaces can be used to deploy applications in different project
namespaces</p>
<p>This is done using the <code>nodeSelector</code> value in the
manifest file, in the deployment configuration component. The default
OpenShift router has a node selector that specifies the node with the
matching region=infra label, you can see this node selector in the
router deployment config like so</p>
<div class="sourceCode" id="cb102"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get deployment <span class="at">-n</span> openshift-ingress</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="ex">router-default</span>      1/1     1            1           87d</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="ex">routes-controller</span>   1/1     1            1           20h</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a><span class="co"># the node selector for the deployment router-default might look something like this</span></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a><span class="ex">nodeSelector:</span></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">region:</span> infra</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">kubernetes.io/os:</span> linux</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">node-role.kubernetes.io/master:</span> <span class="st">&quot;&quot;</span></span></code></pre></div>
<h3 id="investigating-the-haproxy-service">Investigating the HAProxy
service</h3>
<p>The <code>lsns</code> tool you used in previous sections displays the
namespaces associated with the HAProxy process listening on port 80. The
following <code>lsns</code> command woks in your example cluster. First
we have to find the PID of your app pod</p>
<div class="sourceCode" id="cb103"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> sudo <span class="at">-i</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># here is what we can do to extract the pid of the router-default pod, this will inspect the pod, and fetch the pid</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl ps <span class="kw">|</span> <span class="fu">grep</span> router-default <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1&#39;</span>} <span class="kw">|</span> <span class="fu">head</span> <span class="at">-n1</span> <span class="kw">|</span> <span class="fu">xargs</span> <span class="at">-I</span><span class="st">&#39;{}&#39;</span> crictl inspect {} <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;</span><span class="dt">\&quot;</span><span class="st">pid</span><span class="dt">\&quot;</span><span class="st">:&quot;</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we might get a result like that, which points to the PID of that container, we can then use it to list the namespaces</span></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="co"># bound to this process</span></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> <span class="st">&quot;pid&quot;</span>: <span class="ex">7702,</span></span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a><span class="co"># here we list all namespaces which this process has ownership of, take a good look at the first 3 namespaces, and note</span></span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a><span class="co"># that they are different than the others, that is because these reference namespaces directly on the host, you can see</span></span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a><span class="co"># that they are owned by PID 1, on the host, that is. Another giveaway is that the number of processes NPROCS is quite</span></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a><span class="co"># large for the very first 3 namespaces, which kind of should also ring some alarm bells, while the other namespaces owned</span></span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a><span class="co"># by our process 7702, has just a couple of NPROCS running inside that namespace</span></span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> lsns <span class="at">-p</span> 7702</span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>        <span class="ex">NS</span> TYPE   NPROCS   PID USER       COMMAND</span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531834</span> time      560     1 root       /usr/lib/systemd/systemd <span class="at">--system</span> <span class="at">--deserialize</span> 41</span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531837</span> user      560     1 root       /usr/lib/systemd/systemd <span class="at">--system</span> <span class="at">--deserialize</span> 41</span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a><span class="ex">4026531840</span> net       435     1 root       /usr/lib/systemd/systemd <span class="at">--system</span> <span class="at">--deserialize</span> 41</span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a><span class="ex">4026533678</span> uts         2  7702 1000560000 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a><span class="ex">4026533679</span> ipc         2  7702 1000560000 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a><span class="ex">4026533715</span> mnt         2  7702 1000560000 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a><span class="ex">4026533716</span> pid         2  7702 1000560000 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a><span class="ex">4026533722</span> cgroup      2  7702 1000560000 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span></code></pre></div>
<p>Note something strange in the output, we can see that the PID for the
time, user and net namespaces are 1, which means that they are using the
namespaces from the host, the PID column shows who owns this namespace,
PID 1 is always always the very first process that is started on the
host, that is not not the router pod PID which is 7702. Using the host
network namespace lets HAProxy listen directly on the host interfaces
for incoming requests. Listening on the host interface means HAProxy
receives application requests directly acting as OpenShift front door
for application traffic. The router pod has its own mount namespace,
which means that config files for HAProxy are isolated in the container.
To enter the router pod, run the following <code>oc rsh</code> command,
substitute the name of your router pod, this will initialize an
<code>ssh</code> like session into the pod</p>
<div class="sourceCode" id="cb104"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># change the context namespace first and then get the list of pods that are of interest to us</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc project openshift-ingress</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                                 READY   STATUS    RESTARTS   AGE</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a><span class="ex">router-default-6fcb8bbdff-p9sqk</span>      1/1     Running   0          20h</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a><span class="ex">routes-controller-75bcb6d4c4-dbhjh</span>   1/1     Running   0          20h</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a><span class="co"># this will establish an ssh like session into the pod</span></span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc rsh router-default-6fcb8bbdff-p9sqk</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a><span class="co"># now after the remote session into the pod is established run the following from the pod</span></span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ip a</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a><span class="co"># this is an abridged output, but take a good look at the very first interface, that is the interface 115, we already</span></span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a><span class="co"># saw in the host, for the app-cli pod, now the same interface is also visible from the router pod, not only that one but</span></span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a><span class="co"># actually all of them, why ? Well the ip command is executed in the namespace of the host, not the pod, that is, why we</span></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a><span class="co"># can access interfaces on the host</span></span>
<span id="cb104-18"><a href="#cb104-18" aria-hidden="true" tabindex="-1"></a><span class="ex">......</span></span>
<span id="cb104-19"><a href="#cb104-19" aria-hidden="true" tabindex="-1"></a><span class="ex">115:</span> 8a4aad65b71dfe8@if2: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1400 qdisc noqueue master ovs-system state UP group default</span>
<span id="cb104-20"><a href="#cb104-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 5a:2a:70:d2:bc:92 brd ff:ff:ff:ff:ff:ff link-netnsid 63</span>
<span id="cb104-21"><a href="#cb104-21" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::582a:70ff:fed2:bc92/64 scope link</span>
<span id="cb104-22"><a href="#cb104-22" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb104-23"><a href="#cb104-23" aria-hidden="true" tabindex="-1"></a><span class="ex">500:</span> f595c598b9740a2@if2: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1400 qdisc noqueue master ovs-system state UP group default</span>
<span id="cb104-24"><a href="#cb104-24" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> f6:f0:4d:04:bd:b9 brd ff:ff:ff:ff:ff:ff link-netnsid 44</span>
<span id="cb104-25"><a href="#cb104-25" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::f4f0:4dff:fe04:bdb9/64 scope link</span>
<span id="cb104-26"><a href="#cb104-26" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb104-27"><a href="#cb104-27" aria-hidden="true" tabindex="-1"></a><span class="ex">511:</span> d3138ecde33decd@if2: <span class="op">&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span class="op">&gt;</span> mtu 1400 qdisc noqueue master ovs-system state UP group default</span>
<span id="cb104-28"><a href="#cb104-28" aria-hidden="true" tabindex="-1"></a>    <span class="ex">link/ether</span> 42:7f:38:ab:b8:07 brd ff:ff:ff:ff:ff:ff link-netnsid 40</span>
<span id="cb104-29"><a href="#cb104-29" aria-hidden="true" tabindex="-1"></a>    <span class="ex">inet6</span> fe80::407f:38ff:feab:b807/64 scope link</span>
<span id="cb104-30"><a href="#cb104-30" aria-hidden="true" tabindex="-1"></a>       <span class="ex">valid_lft</span> forever preferred_lft forever</span>
<span id="cb104-31"><a href="#cb104-31" aria-hidden="true" tabindex="-1"></a><span class="ex">.....</span></span></code></pre></div>
<h3 id="haproxy-and-request-routing">HAProxy and request routing</h3>
<p>The config file for HAProxy is in the pod at
<code>/var/lib/haproxy/conf/haproxy.config</code>. This config file is
maintained by OpenShift cluster and operator / controllers. Any time an
application is deployed, updated or deleted, OpenShift updates this
config file and has the HAProxy process reload it. Let us see this in
action</p>
<div class="sourceCode" id="cb105"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># while still in the router pod, we can cat this file out</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat /var/lib/haproxy/conf/haproxy.config <span class="kw">|</span> <span class="fu">grep</span> app-cli</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="co"># here is the magic, you can actually see the config for both pods, that we have running for this app at the moment</span></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="ex">backend</span> be_http:image-uploader:app-cli</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">server</span> pod:app-cli-7976b4c888-zqx5l:app-cli:8080-tcp:10.217.0.86:8080 10.217.0.86:8080 cookie 24406d64b9747a68dacf13f017996cea weight 1 check inter 5000ms</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">server</span> pod:app-cli-7976b4c888-rdbv7:app-cli:8080-tcp:10.217.0.87:8080 10.217.0.87:8080 cookie a4765a9ead710ffdc2e669795c0ea2b4 weight 1 check inter 5000ms</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a><span class="co"># from your host machine, run the following command to make the pod replicas less</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc scale deployment/app-cli <span class="at">--replicas</span><span class="op">=</span>1 <span class="at">-n</span> image-uploader</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a><span class="co"># from the router pod now do execute the command again, we can see that only one entry is now present, which matches</span></span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a><span class="co"># the 1 replica we have for this project after the scaling was performed above</span></span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat /var/lib/haproxy/conf/haproxy.config <span class="kw">|</span> <span class="fu">grep</span> app-cli</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a><span class="ex">backend</span> be_http:image-uploader:app-cli</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">server</span> pod:app-cli-7976b4c888-rdbv7:app-cli:8080-tcp:10.217.0.87:8080 10.217.0.87:8080 cookie a4765a9ead710ffdc2e669795c0ea2b4 weight 1</span></code></pre></div>
<p>We will not go into depth on what all of these fields mean, however
you can clearly see that there are the ip addresses of the pods, as well
as the names and ids of these pods. HAProxy takes the request from the
user maps the requested URL to a defined route in the cluster and
proxies the request to the IP address for a pod in the service
associated with that route. All this traverses the pod network created
by OpenShift SDN.. This process works in concert with iptables on each
host. OpenShift uses a complex dynamic iptables configurable to make
sure requests on the pod network are routed to the proper application
pod. IPtables are a complex topic that we do not have the space to cover
here.</p>
<p>The method for routing requests in OpenShift works well. But it poses
a problem when you deploying applications that depend on each other to
function, if a new pod is added to an application or a pod is replaced
and it receives a new IP address the change would require all
applications that reference it to be updated and redeployed. This is not
a serviceable solution. Luckily OpenShift incorporates a DNS service on
the pod network. Let us examine that next</p>
<h2 id="locating-services-with-internal-dns">Locating services with
internal DNS</h2>
<p>Applications depend on each other to deliver information to users.
Middleware apps depend on databases. Web presentation layers depend on
middleware. In tan application spanning multiple independently scalable
pods these relationships are complex to manage to make this easier
OpenShift deployed <code>SkyDNS</code> - when the cluster is deployed,
and makes it available on the pod network, is a DNS service that uses
etcd, the primary Kubernetes database to store DNS records. Also known
as zone files, are config files where DNS records are recorded for a
domain controller by a DNS server, in OpenShift <code>SkyDNS</code>
controls the zone files for several domains that exist only on the pod
network. <code>cluster.local</code> - top level domain for everything in
your OpenShift cluster, <code>svc.cluster.local</code> - domain for all
services running in your cluster.</p>
<p>Domains for each project are also created. For example
<code>image-uploader.svc.cluster.local</code> - used to access all the
services created in the <code>image-uploader project</code>. A DNS A
record is created in <code>SkyDNS</code> for each service in OpenShift
when an application is deployed a service represents all the deployed
pods for an application. To view the services for the
<code>image-uploader project</code> run the following <code>oc</code>
command</p>
<div class="sourceCode" id="cb106"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can see the services here, however what is more interesting is the route, which was created earlier by the expose command,</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and is directly linked to the service, the router object is told which service to serve on a given path, port and so on</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get services <span class="at">-n</span> image-uploader</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class="er">(</span><span class="ex">S</span><span class="kw">)</span>             <span class="ex">AGE</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli</span>   ClusterIP   10.217.5.54   <span class="op">&lt;</span>none<span class="op">&gt;</span>        8080/TCP,8443/TCP   21h</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a><span class="co"># here we can see in the route manifest that the route is linked to the service, here is also some more interesting</span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="co"># parts of the spec, like the `targetPort`, and the `host`, which points to the &lt;app-name&gt;-&lt;project-name&gt;-&lt;cluster-host&gt;</span></span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="ex">apiVersion:</span> route.openshift.io/v1</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a><span class="ex">kind:</span> Route</span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a><span class="ex">metadata:</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>  <span class="ex">labels:</span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">app:</span> app-cli</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">app.kubernetes.io/component:</span> app-cli</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">app.kubernetes.io/instance:</span> app-cli</span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>    <span class="ex">app.kubernetes.io/name:</span> php</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>  <span class="ex">name:</span> app-cli</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a><span class="ex">spec:</span></span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>  <span class="ex">host:</span> app-cli-image-uploader.apps-crc.testing</span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>  <span class="ex">port:</span></span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a>    <span class="ex">targetPort:</span> 8080-tcp</span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">to:</span></span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the name of the service is part of the route manifest, this is what links the service with the route, and the</span></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># route is what links our service to the outside world, the pods are linked through the service, and our app/container is</span></span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># linked through the pod</span></span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a>    <span class="ex">kind:</span> Service</span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a>    <span class="ex">name:</span> app-cli</span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a>    <span class="ex">weight:</span> 100</span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a>  <span class="ex">wildcardPolicy:</span> None</span></code></pre></div>
<p>We can see that the following relationship is established between the
app and the outside world/traffic - ingress/traffic -&gt; route -&gt;
service -&gt; pod -&gt; container -&gt; application</p>
<h3 id="dns-resolution-in-pod-network">DNS resolution in pod
network</h3>
<p>When a pod is deployed the <code>/etc/resolv.conf</code> file from
the application node is mounted in the container in the same location.
In linux <code>/etc/resolv.conf</code> configures the servers and is
used for DNS service name resolution. By default
<code>/etc/resolv.conf</code> on the application node is configured with
the IP address for the node itself. DNS requests on each application
node are forwarded to the <code>SkyDNS</code> running on the master
server node.</p>
<p>The search parameter in <code>/etc/resolv.conf</code> is also updated
when its mounted in the container it is updated to include
<code>cluster.local</code> <code>svc.cluster.local</code> and all other
domains manged by the <code>SkyDNS</code> service. Any domain defined in
the search parameter in <code>resolv.conf</code> are used when a fully
qualified domain name is not used for a hostname. FQDN are defined with
an RFC document, but the general gist of what they are is that they are
a complete address on a network. The domain
<code>server.domain.com</code> is fully qualified where
<code>server</code> only is not a complete domain name. The search
parameter provides one or more domains that are automatically appended
to the non FQDN to use for DNS queries</p>
<p>When a request comes in from your cluster those requests are
automatically forwarded to the master server where <code>SkyDNS</code>
handles requests. Let us test this in action. The format is
<code>service_name.project_name.svc-cluster.local:port</code>. The
following example is run from the node, you can run the same command
from within a pod without specifying the FQDN because
<code>/etc/resolv.conf</code> has the <code>SkyDNS</code> search domains
added. Using the <code>oc rsh</code> you can enter the namespace for the
app-cli pod and use curl to download the index page from app-cli and the
default page for the router service</p>
<div class="sourceCode" id="cb107"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="ex">TODO</span></span></code></pre></div>
<h2 id="configure-openshift-sdn">Configure OpenShift SDN</h2>
<p>When you deploy OpenShift the default configuration for the pod
network topology is a single flat network. Every pod in every project is
able to communicate without restrictions. OpenShift SDN uses a plugin
architecture that provides different network topologies in OpenShift.
There are currently three OpenShift plugins that can be enabled in the
OpenShift config without making large changes to your cluster.</p>
<ul>
<li><code>ovs-subnet</code> - enabled by default, creates a flat pod
network allowing all pods in all projects to communicate with each
other.</li>
<li><code>ovs-multitenant</code> - separates the pod by project the
application deployed in a proEct can only communicate with pods deployed
in the same project, you will enable this one layer on in this
section</li>
<li><code>OVS-networkpolicy</code> - provides fine grained ingress and
egress rules for application. This plugin provides a lot of config
power, but the rules can be complex. This plugin is out of scope</li>
</ul>
<p>The Kubernetes container network interface accepts different
networking plugins. OpenShift SDN is the default CNI plugin in OpenShift
it configures and manages the pod network for your cluster, let us
review the available OpenShift SDN plugins</p>
<h3 id="using-the-ovs-subnet">Using the ovs-subnet</h3>
<p>Earlier you were able to communicate directly with an application
from the stateful-apps project from a pod in the
<code>image-uploader</code> project you could do so because of how the
<code>ovs-subsnet</code> plugins configured in the pod network. A flat
network topology for all pods in all projects lets communication happen
between any deployed applications</p>
<p>With this setup, an OpenShift cluster is deployed like a single
tenant, with all resources available to one another. If you need to
separate network traffic for multiple tenants you can use the
multitenant plugin.</p>
<h3 id="using-the-ovs-multitenant">Using the ovs-multitenant</h3>
<p>The <code>ovs-multitenant</code> network plugins isolated pod
communications at the project level. Each pod for each application
deployment can communicate only with pods and services in the same
project on the pod network. For example the app-gui and app-cli pods can
communicate directly because they are both in the same
<code>image-uploader</code> project namespace. But they are isolated
from the <code>wildfly-app</code> application in the
<code>stateful-apps</code> project in your cluster. This isolation
relies on two primary tools in <code>Open vSwitch</code></p>
<ul>
<li><p><code>VXLAN</code> - network identifier - acts like in a fashion
similar to a <code>VLAN</code> in a traditional network. It is a unique
identifier that can be associated with an interface and used to isolate
communication to interfaces with the same <code>VNID</code>.</p></li>
<li><p><code>OpenFlow</code> - is a communication protocol that can be
used to map network traffic across a network infrastructure.
<code>OpenFlow</code> is used in OpenShift to help define which
interfaces can communicate and when to route traffic through the
<code>vxlan0</code> and <code>tun0</code> interfaces on each
node.</p></li>
</ul>
<p>When the <code>ovs-multitenant</code> plugin is enabled each project
is assigned a <code>VNID</code>. The <code>VNID</code> for each project
is maintained in the etcd database on the OpenShift master node. When a
pod is created its linked veth interface is associated with the
project’s <code>VNID</code> and <code>OpenFlow</code> rules are created
to make sure it can communicate only with pods in the same project. The
router and registry pods in the default project are assigned
<code>VNID-0</code> this is a special <code>VNID</code> that can
communicate with tall other <code>VNID</code> on a system. If a pod
needs to communicate with a pod on another host, the <code>VNID</code>
is attached to each packet</p>
<p>With the <code>ovs-multitenant</code> plugin enabled if a pod needs
to communicate with a pod in another project the request must be routed
off the pod network and connect to the desired application through its
external route like any other external request this is not always the
most efficient architecture. The OpenShift SDN
<code>ovs-networkpolicy</code> plugin provides more fine grained control
over how applications communicate across projects.</p>
<h3 id="creating-advanced-network-designs">Creating advanced network
designs</h3>
<p>The <code>ovs-networkpolicy</code> plugin provides fine grained
access control for individual applications regardless of the project
they are in these rules can become complex very quickly we do not have
the space to cover this here, but you can learn more about them on the
official documentation site of OpenShift</p>
<h3 id="enabling-multi-tenant-plugin">Enabling multi tenant plugin</h3>
<p>To enable the multi tenant plugin you need to ssh into your master
node and application nodes (if you have more than one node cluster
configured) and edit a cluster network config</p>
<div class="sourceCode" id="cb108"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># on the master server, login with a correct user that has cluster-admin roles, first lets see what is in there</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe network/cluster</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="co"># this describes the network cluster object that is currently deployed on the master server, you can see some of the</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="co"># details we had already mentioned below in the output</span></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         cluster</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span></span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a><span class="ex">API</span> Version:  config.openshift.io/v1</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Kind:</span>         Network</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Metadata:</span></span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Creation</span> Timestamp:  <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Generation:</span>          6</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Resource</span> Version:    26060</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">UID:</span>                 bc505ed9-75c3-4f2a-bc0f-db8a0769e5cd</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a><span class="ex">Spec:</span></span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Cluster</span> Network:</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Cidr:</span>         10.217.0.0/22</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Host</span> Prefix:  23</span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>  <span class="ex">External</span> IP:</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Policy:</span></span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Network</span> Diagnostics:</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Mode:</span></span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Source</span> Placement:</span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Target</span> Placement:</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Network</span> Type:  OVNKubernetes</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Service</span> Network:</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>    <span class="ex">10.217.4.0/23</span></span>
<span id="cb108-30"><a href="#cb108-30" aria-hidden="true" tabindex="-1"></a><span class="ex">Status:</span></span>
<span id="cb108-31"><a href="#cb108-31" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Cluster</span> Network:</span>
<span id="cb108-32"><a href="#cb108-32" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Cidr:</span>               10.217.0.0/22</span>
<span id="cb108-33"><a href="#cb108-33" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Host</span> Prefix:        23</span>
<span id="cb108-34"><a href="#cb108-34" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Cluster</span> Network MTU:  1400</span>
<span id="cb108-35"><a href="#cb108-35" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Conditions:</span></span>
<span id="cb108-36"><a href="#cb108-36" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Last</span> Transition Time:  <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb108-37"><a href="#cb108-37" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Message:</span></span>
<span id="cb108-38"><a href="#cb108-38" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Observed</span> Generation:   0</span>
<span id="cb108-39"><a href="#cb108-39" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Reason:</span>                AsExpected</span>
<span id="cb108-40"><a href="#cb108-40" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Status:</span>                True</span>
<span id="cb108-41"><a href="#cb108-41" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Type:</span>                  NetworkDiagnosticsAvailable</span>
<span id="cb108-42"><a href="#cb108-42" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Network</span> Type:            OVNKubernetes</span>
<span id="cb108-43"><a href="#cb108-43" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Service</span> Network:</span>
<span id="cb108-44"><a href="#cb108-44" aria-hidden="true" tabindex="-1"></a>    <span class="ex">10.217.4.0/23</span></span>
<span id="cb108-45"><a href="#cb108-45" aria-hidden="true" tabindex="-1"></a><span class="ex">Events:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-46"><a href="#cb108-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-47"><a href="#cb108-47" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure to apply the following network policies, these are a prerequisite, to allow us to continue with</span></span>
<span id="cb108-48"><a href="#cb108-48" aria-hidden="true" tabindex="-1"></a><span class="co"># the creation and setup of the multitenant plugin</span></span>
<span id="cb108-49"><a href="#cb108-49" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc apply <span class="at">-f</span> openshift/network-policy-multitenant.yml</span>
<span id="cb108-50"><a href="#cb108-50" aria-hidden="true" tabindex="-1"></a><span class="ex">networkpolicy.networking.k8s.io/allow-from-openshift-ingress</span> created</span>
<span id="cb108-51"><a href="#cb108-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-52"><a href="#cb108-52" aria-hidden="true" tabindex="-1"></a><span class="co"># here after applying the configuration from above you should be able to see the effects of the net network policies</span></span>
<span id="cb108-53"><a href="#cb108-53" aria-hidden="true" tabindex="-1"></a><span class="co"># in effect, created on the master cluster node</span></span>
<span id="cb108-54"><a href="#cb108-54" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc describe networkpolic</span>
<span id="cb108-55"><a href="#cb108-55" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         allow-from-openshift-ingress</span>
<span id="cb108-56"><a href="#cb108-56" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>    openshift-ingress</span>
<span id="cb108-57"><a href="#cb108-57" aria-hidden="true" tabindex="-1"></a><span class="ex">Created</span> on:   <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb108-58"><a href="#cb108-58" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-59"><a href="#cb108-59" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-60"><a href="#cb108-60" aria-hidden="true" tabindex="-1"></a><span class="ex">Spec:</span></span>
<span id="cb108-61"><a href="#cb108-61" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PodSelector:</span>     <span class="op">&lt;</span>none<span class="op">&gt;</span> <span class="er">(</span><span class="ex">Allowing</span> the specific traffic to all pods in this namespace<span class="kw">)</span></span>
<span id="cb108-62"><a href="#cb108-62" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Allowing</span> ingress traffic:</span>
<span id="cb108-63"><a href="#cb108-63" aria-hidden="true" tabindex="-1"></a>    <span class="ex">To</span> Port: <span class="op">&lt;</span>any<span class="op">&gt;</span> <span class="er">(</span><span class="ex">traffic</span> allowed to all ports<span class="kw">)</span></span>
<span id="cb108-64"><a href="#cb108-64" aria-hidden="true" tabindex="-1"></a>    <span class="ex">From:</span></span>
<span id="cb108-65"><a href="#cb108-65" aria-hidden="true" tabindex="-1"></a>      <span class="ex">NamespaceSelector:</span> policy-group.network.openshift.io/ingress=</span>
<span id="cb108-66"><a href="#cb108-66" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Not</span> affecting egress traffic</span>
<span id="cb108-67"><a href="#cb108-67" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Policy</span> Types: Ingress</span>
<span id="cb108-68"><a href="#cb108-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-69"><a href="#cb108-69" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         allow-from-openshift-monitoring</span>
<span id="cb108-70"><a href="#cb108-70" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>    openshift-ingress</span>
<span id="cb108-71"><a href="#cb108-71" aria-hidden="true" tabindex="-1"></a><span class="ex">Created</span> on:   <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb108-72"><a href="#cb108-72" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-73"><a href="#cb108-73" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-74"><a href="#cb108-74" aria-hidden="true" tabindex="-1"></a><span class="ex">Spec:</span></span>
<span id="cb108-75"><a href="#cb108-75" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PodSelector:</span>     <span class="op">&lt;</span>none<span class="op">&gt;</span> <span class="er">(</span><span class="ex">Allowing</span> the specific traffic to all pods in this namespace<span class="kw">)</span></span>
<span id="cb108-76"><a href="#cb108-76" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Allowing</span> ingress traffic:</span>
<span id="cb108-77"><a href="#cb108-77" aria-hidden="true" tabindex="-1"></a>    <span class="ex">To</span> Port: <span class="op">&lt;</span>any<span class="op">&gt;</span> <span class="er">(</span><span class="ex">traffic</span> allowed to all ports<span class="kw">)</span></span>
<span id="cb108-78"><a href="#cb108-78" aria-hidden="true" tabindex="-1"></a>    <span class="ex">From:</span></span>
<span id="cb108-79"><a href="#cb108-79" aria-hidden="true" tabindex="-1"></a>      <span class="ex">NamespaceSelector:</span> network.openshift.io/policy-group=monitoring</span>
<span id="cb108-80"><a href="#cb108-80" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Not</span> affecting egress traffic</span>
<span id="cb108-81"><a href="#cb108-81" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Policy</span> Types: Ingress</span>
<span id="cb108-82"><a href="#cb108-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-83"><a href="#cb108-83" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         allow-same-namespace</span>
<span id="cb108-84"><a href="#cb108-84" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>    openshift-ingress</span>
<span id="cb108-85"><a href="#cb108-85" aria-hidden="true" tabindex="-1"></a><span class="ex">Created</span> on:   <span class="op">&lt;</span>date<span class="op">&gt;</span></span>
<span id="cb108-86"><a href="#cb108-86" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-87"><a href="#cb108-87" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-88"><a href="#cb108-88" aria-hidden="true" tabindex="-1"></a><span class="ex">Spec:</span></span>
<span id="cb108-89"><a href="#cb108-89" aria-hidden="true" tabindex="-1"></a>  <span class="ex">PodSelector:</span>     <span class="op">&lt;</span>none<span class="op">&gt;</span> <span class="er">(</span><span class="ex">Allowing</span> the specific traffic to all pods in this namespace<span class="kw">)</span></span>
<span id="cb108-90"><a href="#cb108-90" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Allowing</span> ingress traffic:</span>
<span id="cb108-91"><a href="#cb108-91" aria-hidden="true" tabindex="-1"></a>    <span class="ex">To</span> Port: <span class="op">&lt;</span>any<span class="op">&gt;</span> <span class="er">(</span><span class="ex">traffic</span> allowed to all ports<span class="kw">)</span></span>
<span id="cb108-92"><a href="#cb108-92" aria-hidden="true" tabindex="-1"></a>    <span class="ex">From:</span></span>
<span id="cb108-93"><a href="#cb108-93" aria-hidden="true" tabindex="-1"></a>      <span class="ex">PodSelector:</span> <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb108-94"><a href="#cb108-94" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Not</span> affecting egress traffic</span>
<span id="cb108-95"><a href="#cb108-95" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Policy</span> Types: Ingress</span></code></pre></div>
<p>After these changes are committed and done you have to make sure to
restart the master node server, you can do this with the command line on
your host machine like so <code>crc stop</code> and then followed by
<code>crc stop</code>, the changes will take effect after the server is
restarted</p>
<div class="sourceCode" id="cb109"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="ex">TODO</span></span></code></pre></div>
<h3 id="testing-the-multi-tenant-plugin">Testing the multi-tenant
plugin</h3>
<p>Previously in the section you logged in to the app-cli pod using the
<code>oc rsh</code> and downloaded the web index pages for the other
applications, now we are going to try to do the same. We will connect to
the pods again, and try to download the index pages for both
applications again, from the two different projects, one of the apps
will be from the current project <code>image-uploader</code> project and
the other from the <code>wildfly-app</code> project, but it does not
really matter which one you choose as long as its a different one than
the <code>image-uploader</code> project</p>
<h1 id="security">Security</h1>
<p>Each topic in this chapter is specific to security and to making
OpenShift a secure platform for your application. This chapter is not a
comprehensive summary of OpenShift security features that would take
many lines of words or and is a great idea for a standalone document.
What we will do in this section is walkthrough the fundamentals of
OpenShift we want to give you examples of what we think are the most
crucial concepts and we will do our best to point you in the right
direction, for the topics we do not have room to cover</p>
<p>We begin discussing important security concepts and making OpenShift
secure not long after the very first section,</p>
<ul>
<li>Understanding OpenShift role in your environment</li>
<li>Deploying applications with specific users</li>
<li>Diving deep into how container processes are isolated</li>
<li>Confirming application health and status</li>
<li>Autoscaling applications to automate resilience</li>
<li>CI/CD pipeline so humans do not have to be involved</li>
<li>Working with persistent storage</li>
<li>Controlling access to pods and handling interactions between
pods</li>
<li>Using identity providers and working with roles, limits and
quotas</li>
<li>Creating a secure stable networking</li>
</ul>
<p>We may be using a broad definition of security her but every section
in this documentation, contributes to your understanding of OpenShift
and how to deploy it in an automated and secure fashion. Automation and
security go hand in hand, because humans are not good at repetitive
task. The more you can automate task for your application the more
secure you can make those applications Even though we have already
covered a lot of ground regarding security we will still need to devote
this entire section to security specific topics</p>
<p>OpenShift has layers of security form the Linux kernel on each
application node through the routing layer that delivers applications to
end user, we will begin this discussion with the linux kernel and work
our way up through the application stack. For containers and OpenShift,
security begins in the Linux kernel with SELinux</p>
<h2 id="selinux-core-concepts">SELinux core concepts</h2>
<p>SELinux is a linux kernel module, that is used to enforce mandatory
access control - MAC. This is a set of access levels that are assigned
to users by the system. Only users with root-level permissions
privileges can alter them. For typical users including the automated
user accounts in OpenShift that deploy applications, the SELinux config
specified for a deployment is an immutable fact. MAC is a contrast to
discretionary access control - in linux. <code>DAC</code> is the system
of users and file ownership access modes that we all use every day on
Linux hosts. If only <code>DAC</code> were tin effect in your OpenShift
cluster users could allow full access to their container resources by
changing the ownership or the access mode for the container process or
storage resources. One of the key security features of OpenShift is that
SELinux automatically enforces MAC policies that can not be changed by
unprivileged users for pods and other resources even if they deployed
the application</p>
<p>We need to take a few lines to discuss some of the fundamental
information that we will use throughout the section. As with security in
general this will not be a full SELinux introduction, Entire books have
been written on that topic including the SELinux coloring book. But the
following information, will help you understand how OpenShift uses
SELinux to create a secure platform. We will focus on the following
SELinux concepts</p>
<ul>
<li>Labels - SELinux labels are applied to all objects on a linux
server</li>
<li>Context - SELinux contexts apply labels to object based on file
system location</li>
<li>Policies - SELinux policies are rules that control interactions
between objects with different SELinux labels</li>
</ul>
<h2 id="working-with-selinux-labels">Working with SELinux labels</h2>
<p>SELinux labels are applied to all objects on your OpenShift servers
as they are created. An SELinux label dictates how an object on a linux
sever interacts with the SELinux kernel module. We are defining an
object in this context as anything a user or process can create or
interact with on a server, such as the following.</p>
<ul>
<li>files</li>
<li>directories</li>
<li>TCP ports</li>
<li>unix sockets</li>
<li>shared memory resources</li>
</ul>
<p>Each object in SELinux label has four section separated by
colons:</p>
<ul>
<li>User which SELinux user has access to the object with the SELinux
label,</li>
<li>Role the SELinux role that can access the objects with the matching
SELinux</li>
<li>Type SELinux type for each label. This is the section where most
common SELinux rules are written</li>
<li>multi category security - often called the <code>MCS</code> bit,
Unique for each container what we will spend the most time on</li>
</ul>
<p><code>Open vSwitch</code> for communication on your OpenShift cluster
nodes. <code>/var/run/open-vswitch/db.sock</code>. To view this label
run the following ls command using the -Z option flag to include the
SELinux information in its output.</p>
<div class="sourceCode" id="cb110"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># besides the regular file permission, group and user owner, we can also see the labels for the sock object are also</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="co"># displayed</span></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls <span class="at">-alZ</span> /var/run/openvswitch/db.sock</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a><span class="ex">srwxr-x---.</span> 1 openvswitch hugetlbfs system_u:object_r:openvswitch_var_run_t:s0 0 Jun 11 16:03 /var/run/openvswitch/db.sock</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The SELinux label for the open v-switch socket object, the format looks something like this, we can see from above,</span></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a><span class="co"># that we have multiple targets in the label</span></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a><span class="ex">system_u:object_r:openvswitch_var_run_t:s0</span></span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a><span class="ex">system_u</span> <span class="at">-</span> the SELinux user is used for the <span class="kw">``</span>MCS<span class="kw">``</span> inmplementation</span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a><span class="ex">object_r</span> <span class="at">-</span> the SELinux role is used primarily for <span class="kw">``</span>MCS<span class="kw">``</span> implementation</span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a><span class="ex">openvswitch_var_run_t</span> <span class="at">-</span> the SELinux type is used in type enformcement policies to define interactions between objects on a Linux host</span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a><span class="ex">s0</span> <span class="at">-</span> objects are assigned an <span class="kw">``</span>MCS<span class="kw">``</span> value to distinguish between different category levels on the Linux system.</span></code></pre></div>
<p>In addition to the standard POSIX attributes of mode, owner and group
ownership, the output also includes the SELinux label for
<code>/var/run/openvswitch/db.sock.</code>. Next lets examine how
SELinux labels are applied to files and other objects, when they are
created.</p>
<div class="sourceCode" id="cb111"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Most commands have the -Z option that will include the commands in SELinux labels common command line tools like</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ls,ps,netstat and others accept the -Z option, to include the SELinux information in their output, because objects are</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co"># presented in the linux operating system as files their SELinux labels are stored in their filesystem extended</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="co"># attributes. You can view these attributes directly for the Open vSwitch socket using the following getfattr command:</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> getfattr <span class="at">-d</span> <span class="at">-m</span> <span class="at">-</span> /var/run/openvswitch/db.sock</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a><span class="co"># file: var/run/openvswitch/db.sock</span></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a><span class="ex">security.selinux=</span><span class="st">&quot;system_u:object_r:openvswitch_var_run_t:s0&quot;</span></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a><span class="co"># if you are looking for a full SELinux documentation a great place to start is the Red Hat enterprise linux 7 SELinux</span></span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a><span class="co"># guide, that you can find on the official site of RedHat</span></span></code></pre></div>
<h3 id="applying-labels-with-selinux-context">Applying labels with
SELinux context</h3>
<p>Labels are applied to files using SELinux contexts rules that are
used to apply labels, to object on a linux system, contexts use regular
expression to apply labels depending on where the object exists in the
file system. One of the worst things a system admin can hear is a
developer telling the that the SELinux breaks their application. In
reality application is almost certainly creating objects on the linux
server that do not have a defined SELinux context. If SELinux does not
know how to apply the correct label it does not know how to treat the
application objects. This often results in SELinux policy denials that
lead to frantic calls and requests to disable SELinux because its
breaking an application.</p>
<p>To query the context for a system use the <code>semanage</code>
command, and filter it using grep. You can use <code>semanage</code>to
search for context that apply to any label related to any file or
directly, including the Open<code>vSwitch</code>socket. A search
for<code>openvswitch</code>in the<code>semanage</code>output shows that
the context<code>system_u:obejct_r:poenvswitch_var_run_t:s0</code>, is
applied to any object created in the <code>/var/run/openvswitch</code>
directory</p>
<div class="sourceCode" id="cb112"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> semanage fcontext <span class="at">-l</span> <span class="kw">|</span> <span class="fu">grep</span> openvswitch</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="ex">/etc/openvswitch</span><span class="er">(</span><span class="ex">/.*</span><span class="kw">)</span><span class="ex">?</span>                             all files          system_u:object_r:openvswitch_rw_t:s0</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="ex">/run/ovn</span><span class="er">(</span><span class="ex">/.*</span><span class="kw">)</span><span class="ex">?</span>                                     all files          system_u:object_r:openvswitch_var_run_t:s0</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/bin/neutron-openvswitch-agent</span>                 regular file       system_u:object_r:neutron_exec_t:s0</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/bin/ovs-appctl</span>                                regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/bin/ovs-vsctl</span>                                 regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/bin/quantum-openvswitch-agent</span>                 regular file       system_u:object_r:neutron_exec_t:s0</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/lib/systemd/system/openvswitch.service</span>        regular file       system_u:object_r:openvswitch_unit_file_t:s0</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/sbin/ovs-vswitchd</span>                             regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/sbin/ovsdb-ctl</span>                                regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/sbin/ovsdb-server</span>                             regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/share/openvswitch/scripts/ovs-ctl</span>             regular file       system_u:object_r:openvswitch_exec_t:s0</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/share/openvswitch/scripts/ovs-kmod-ctl</span>        regular file       system_u:object_r:openvswitch_load_module_exec_t:s0</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/lib/openvswitch</span><span class="er">(</span><span class="ex">/.*</span><span class="kw">)</span><span class="ex">?</span>                         all files          system_u:object_r:openvswitch_var_lib_t:s0</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/log/openvswitch</span><span class="er">(</span><span class="ex">/.*</span><span class="kw">)</span><span class="ex">?</span>                         all files          system_u:object_r:openvswitch_log_t:s0</span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a><span class="ex">/var/run/openvswitch</span><span class="er">(</span><span class="ex">/.*</span><span class="kw">)</span><span class="ex">?</span>                         all files          system_u:object_r:openvswitch_var_run_t:s0</span></code></pre></div>
<p>Properly applied SELinux labels create policies that control how
objects with different labels can interact with each other. Let us
discuss those next</p>
<h3 id="enforcing-selinux-with-policies">Enforcing SELinux with
policies</h3>
<p>SELinux policies are a complex thing. They are heavily optimized and
compiled so they can be interpreted quickly by the linux kernel.
Creating one or looking at the code that creates one is outside the
scope here, but lets look at the basic example of what an SELinux policy
would do. For this we will use an example that most people are familiar
with, the Apache web server. Apache is a common everywhere and has long
established SELinux policies that we can use as an example</p>
<div class="sourceCode" id="cb113"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if you have followed the install setup steps from above, you will be able to see/install the httpd binary in /sbin/httpd,</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="co"># which is the Apache web server binary, if not installed, use dnf -y install httpd on your master cluster node</span></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> which httpd</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="ex">/sbin/httpd</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a><span class="co"># run the following ls command to inspect the SELinux policies on the executable, use the -Z flag, to do so</span></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls <span class="at">-zlZ</span> /sbin/httpd</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="ex">-rwxr-xr-x.</span> 1 root root system_u:object_r:httpd_exec_t:s0 589576 Jan 29 17:56 /sbin/httpd</span></code></pre></div>
<p>The executable file for the Apache web server is
<code>/usr/sbin/htpd</code>. This httpd executable has an SELinux labels
of <code>system_u:object_r:httpd_exec_t:s0</code>. On CentOS and Red Hat
systems the default Apache web content directory is
<code>/var/www/html</code>. This directory has an SELinux label of
<code>system_u:object_r:httpd_sys_content_t:s0</code>, The default
<code>cgi-script</code> directory for Apache is
<code>/var/www/cgi-bin</code>, and it has the SELinux label of
<code>system_u:object_r:httpd_sys_script_exec_t:s0</code>. There is also
the httpd_port_t label for the following TCP port numbers - 80, 8008,
8009, 8433, 9000, 81, 443, 488.an SELinux policy enforces the following
rules using these labels for the <code>httpd_exec_t</code> object
type</p>
<div class="sourceCode" id="cb114"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls <span class="at">-alZ</span> /var/www/cgi-bin</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span> 2 root root system_u:object_r:httpd_sys_script_exec_t:s0  6 Jan 29 17:56 .</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span> 4 root root system_u:object_r:httpd_sys_content_t:s0     33 Jun 15 12:40 ..</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ls <span class="at">-alZ</span> /var/www/html</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span> 2 root root system_u:object_r:httpd_sys_content_t:s0  6 Jan 29 17:56 .</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a><span class="ex">drwxr-xr-x.</span> 4 root root system_u:object_r:httpd_sys_content_t:s0 33 Jun 15 12:40 ..</span></code></pre></div>
<p>An SELinux policy enforces the following rules using these labels for
the httpd_exec_t object type:</p>
<ul>
<li><code>httpd_exec_t</code> - can write only to objects with an
<code>httpd_sys_content_t</code> type</li>
<li><code>httpd_exec_t</code> - can execute scripts only with the
<code>httpd_sys_script_exec_t</code></li>
<li><code>httpd_exec_t</code> - can read from directories with
<code>httpd_sys_script_exec_t</code></li>
<li><code>httpd_exec_t</code> - can open and bind only to ports with the
<code>httpd_port_t</code></li>
</ul>
<p>This means even if Apache is somehow compromised by a remote user it
can read content from <code>/var/www/html</code> and run scripts from
<code>/var/www/cgi-bin</code>. It also can not write to
<code>/var/www/cgi-win</code>. All of this is enforced by the Linux
kernel, regardless of the ownership or permission of the binary or these
directories, and which user owns the httpd process. The default SELinux
loaded on a Linux system is the targeted type.</p>
<p>The rules in the targeted SELinux type are applied only to objects
that have matching context. Every object on a server is assigned a
labels based on the SELinux context it matches. If an object does not
match the context it is assigned an unconfined_t type in its SELinux
labels. The unconfined_t type ha no contexts or policies associated with
it. Interactions between objects that are not covered by a policy in
targeted SELinux are allowed to run with no interference.</p>
<p>To summarize - the httpd executable can only find to specific ports
as listed above, with the matching <code>httpd_port_t</code> type. Then
the binary can only execute scripts with the
<code>httpd_sys_script_exec_t</code> type, and also the binary can only
read from directories with the <code>httpd_sys_script_exec_t</code>, and
can server content from and write to directories tagged with the
<code>httpd_sys_content_t</code> type.</p>
<p>For CentOS and Red Hat Enterprise Linux the default policies use type
enforcement. Type enforcement uses the type value from SELinux labels to
enforce the interaction between objects. Let us review what we have
talked about so far up until this point</p>
<ul>
<li><p>SELinux is used to enforce the MAC in your OpenShift cluster. MAC
provides access controls at the deeper level than a traditional
user/group ownership and access mode. It also applies to objects on the
operating system that are not traditional files and
directories.</p></li>
<li><p>Every object on an OpenShift node is assigned an SELinux label
including a type.</p></li>
<li><p>Labels are assigned according to a SELinux context as objects are
created,</p></li>
<li><p>With labels applied SELinux policies enforce interaction between
objects. SELinux uses type enforcement policies on your OpenShift
cluster to ensure proper interactions between objects</p></li>
</ul>
<p>This SELinux configuration is standard for any CentOS or RedHat
system running with SELinux in enforcing mode. Just as in the Apache web
server process we have been running discussing you know what container
is essentially a process. Each container process is assigned an SELinux
label when its created and that label dictates the policies that affect
the container. To confirm the SELinux label that is used for containers
in OpenShift get the container PID, form the container runtime, and use
the <code>ps</code> command with the -Z command parameter searching for
that PID with grep</p>
<div class="sourceCode" id="cb115"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> crictl ps <span class="kw">|</span> <span class="fu">grep</span> router-default <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1&#39;</span>} <span class="kw">|</span> <span class="fu">head</span> <span class="at">-n1</span> <span class="kw">|</span> <span class="fu">xargs</span> <span class="at">-I</span><span class="st">&#39;{}&#39;</span> crictl inspect {} <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;</span><span class="dt">\&quot;</span><span class="st">pid</span><span class="dt">\&quot;</span><span class="st">:&quot;</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;pid&quot;</span><span class="ex">:</span> 7702,</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ps <span class="at">-axZ</span> <span class="kw">|</span> <span class="fu">grep</span> 7702</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="ex">system_u:system_r:spc_t:s0</span>         7702 <span class="pp">?</span>        Ssl    2:03 /usr/bin/openshift-router <span class="at">--v</span><span class="op">=</span>2</span></code></pre></div>
<p>OpenShift hosts operate with SELinux enforcing mode. Enforcing mode
means that the policy engine that controls how objects can interact is
full activated, if an object attempts to do something that is against
the SELinux policies present on the system, that action is not allowed,
and the attempt is logged by the kernel, to confirm that the SELinux is
in enforcing mode run the following command <code>getenforce</code>
command</p>
<div class="sourceCode" id="cb116"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co"># you will get the following minimal output from this command, which basically confirms that we are indeed in enforcing</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="co"># mode running in the master node</span></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> getenforce</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> Enforcing</span></code></pre></div>
<p>In other servers, tools like virus scanners can cause issues with
SELinux. A virus scanner is designed to analyze files on a server that
are created and managed by other services. That makes writing an
effective SELinux policy for a virus scanner a significant challenge,
another typical issue is that when application and their data are placed
in location on the file system that do not match their corresponding
SELinux context. If the Apache web server is trying to access content
from /data on server, it will be denied by SELinux because /data does
not match any SELinux context associated with Apache. These sorts of
issues lead to some people deciding to disable SELinux.</p>
<p>The user and role portions of the label are not used for type
enforcement policies. The <code>svirt_lxc_net_t</code> type is used in
SELinux policies that control which resources on the system container
can interact with. We have not discussed the fourth part of the SELinux
label - the <code>MCS</code> level, which isolates pods in OpenShift
lets examine how that works next</p>
<h3 id="isolating-pods-with-levels">Isolating pods with levels</h3>
<p>The original purpose of the <code>MCS</code> bit was to implement the
<code>MCS</code> security standards on linux servers. These standards
control data access for different security levels on the same servers.
For example secret and top secret data could exist on the same server. A
top secret level process should be able to to access secret level data,
a concept called data dominance. But secret processes should never be
able to access the top secret level data, because that data has higher
<code>MCS</code> level. This is the security feature you can use to
prevent a pod from accessing data its not authorized to access on the
host.</p>
<p>OpenShift uses the <code>MCS</code> level for each container process
to enforce security as part of the pod security context. A pod security
context is all the information that describes how its secured on its
application node. Let us look at the security context for the app-cli
pod</p>
<h3 id="investigating-pod-security-context">Investigating pod security
context</h3>
<p>Each pod security context contains information about its security
posture. You can find full documentation on the possible fields that can
be defined at the official documentation in OpenShift. In OpenShift the
following parameters are configured by default.</p>
<ul>
<li><p>Capabilities - defines an application ability to perform various
tasks on the host. Capabilities can be added to or dropped from each
pod. We will look at these in depth in this section</p></li>
<li><p>Privileged - specifies whether the container is running with any
of the host namespaces</p></li>
<li><p>RunAsUser - UID with which to run the container process. This can
be configured which we will also checkout in this section, it is often
used in Dockerfile images.</p></li>
<li><p>SELinuxOptions - SELinux options for the pod, Normally the only
needed option is to st the SELinux level.</p></li>
</ul>
<p>You can view the security context for a pod in the GUI by choosing
the Pods, and then selecting the pod you want the information about, and
then choosing Actions -&gt; Edit -&gt; YAML. From the command line that
might look like that</p>
<div class="sourceCode" id="cb117"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure to specify the correct pod id, based on the output from the oc get pods, the output is abridged of course,</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="co"># but the detailed output for a pod would always include the `securityContext` field, which you can see below, it contains</span></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the `seLinuxOptions` levels</span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get <span class="at">-n</span> image-uploader <span class="at">-o</span> yaml pod/app-cli-7976b4c888-rdbv7</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="ex">apiVersion:</span> v1</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a><span class="ex">kind:</span> Pod</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a><span class="ex">metadata:</span></span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">annotations:</span></span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">creationTimestamp:</span> <span class="st">&quot;&lt;date&gt;&quot;</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>  <span class="ex">labels:</span></span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">deployment:</span> app-cli</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">pod-template-hash:</span> 7976b4c888</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">name:</span> app-cli-7976b4c888-rdbv7</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>  <span class="ex">namespace:</span> image-uploader</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a><span class="ex">spec:</span></span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">securityContext:</span></span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">fsGroup:</span> 1000650000</span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">seLinuxOptions:</span></span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a>      <span class="ex">level:</span> s0:c26,c0 <span class="co"># &lt;- take a note the following c0 `MCS` level rule</span></span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">seccompProfile:</span></span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a>      <span class="ex">type:</span> RuntimeDefault</span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">serviceAccount:</span> default</span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a>  <span class="ex">serviceAccountName:</span> default</span></code></pre></div>
<h3 id="examining-the-mcs-levels">Examining the <code>MCS</code>
levels</h3>
<p>The structure of the <code>MCS</code> level consists of sensitivity
level s0 and two categories c8 and c7 as shown in the following output
from the previous command. You may have noticed that the order of the
categories is reversed in the oc output compared with the pc command.
This makes no difference in how the Linux kernel reads and acts on the
<code>MCS</code> level.</p>
<p>A detailed discussion of how different <code>MCS</code> levels can
interact is out of scope. OpenShift assumes that application deployed in
the same project will need to interact with each other. With that in the
pods in a project have the same <code>MCS</code> level. Sharing an
<code>MCS</code> level lets applications share resources easily and
simplifies the security configuration you need to make for your
cluster.</p>
<p>Let us examine the SELinux configuration for pod in different
project. You already know the <code>MCS</code> level for app-cli.
Because the app-cli and app-gui are in the same project, they should
have the same <code>MCS</code> level. T get the <code>MCS</code> level
of the app-gui pod use the same <code>oc</code> get command</p>
<div class="sourceCode" id="cb118"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first make sure to extract the pods for the project, and then we will pull the manifest, grepping on the relevant</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="co"># field, to verify that the levels are indeed the same, in both pods in the same project, just as described above</span></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods <span class="at">-n</span> image-uploader</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="ex">app-cli-7976b4c888-rdbv7</span>   1/1     Running     0          3d20h</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a><span class="ex">app-gui-5d5dc97869-8r2wl</span>   1/1     Running     0          96s</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get <span class="at">-o</span> yaml pod/app-cli-7976b4c888-rdbv7 <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;level:&quot;</span></span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>      <span class="ex">level:</span> s0:c26,c0</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get <span class="at">-o</span> yaml pod/app-gui-5d5dc97869-8r2wl <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;level:&quot;</span></span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a>      <span class="ex">level:</span> s0:c26,c0</span></code></pre></div>
<p>This confirms what we have sated earlier the levels for the app-gui
and app-cli are the same because they are deployed in the same project.
Use the <code>wildfly-app</code> you deployed in the earlier chapter, to
get the name of the deployed pod running run the following
<code>oc</code> command, get the pods of that project and compare the
security options of the SELinux labels</p>
<div class="sourceCode" id="cb119"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can also display the abridged output for the manifest for the `stateful-apps`, `wildfly-app` as well, we can see</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="co"># that here the levels are completely different</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get pods <span class="at">-n</span> stateful-apps</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                   READY   STATUS      RESTARTS   AGE</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="ex">wildfly-app-1-snffs</span>    1/1     Running     0          108s</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the manifest file for the pod, and note that `securityContext` section, and see that the levels in this case are</span></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a><span class="co"># different for this project compared to the `image`-uploader</span></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc get <span class="at">-n</span> stateful-apps <span class="at">-o</span> yaml pod/wildfly-app-1-snffs</span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a><span class="ex">apiVersion:</span> v1</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a><span class="ex">kind:</span> Pod</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a><span class="ex">metadata:</span></span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">annotations:</span></span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">openshift.io/deployment-config.latest-version:</span> <span class="st">&quot;1&quot;</span></span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">openshift.io/deployment-config.name:</span> wildfly-app</span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>    <span class="ex">openshift.io/deployment.name:</span> wildfly-app-1</span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a>    <span class="ex">openshift.io/generated-by:</span> OpenShiftNewApp</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">openshift.io/scc:</span> restricted-v2</span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a>    <span class="ex">seccomp.security.alpha.kubernetes.io/pod:</span> runtime/default</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a>  <span class="ex">creationTimestamp:</span> <span class="st">&quot;&lt;date&gt;&quot;</span></span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a>  <span class="ex">generateName:</span> wildfly-app-1-</span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">labels:</span></span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a>    <span class="ex">application:</span> wildfly-app</span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true" tabindex="-1"></a>    <span class="ex">deployment:</span> wildfly-app-1</span>
<span id="cb119-25"><a href="#cb119-25" aria-hidden="true" tabindex="-1"></a>    <span class="ex">deploymentConfig:</span> wildfly-app</span>
<span id="cb119-26"><a href="#cb119-26" aria-hidden="true" tabindex="-1"></a>    <span class="ex">deploymentconfig:</span> wildfly-app</span>
<span id="cb119-27"><a href="#cb119-27" aria-hidden="true" tabindex="-1"></a>  <span class="ex">name:</span> wildfly-app-1-snffs</span>
<span id="cb119-28"><a href="#cb119-28" aria-hidden="true" tabindex="-1"></a><span class="ex">spec:</span></span>
<span id="cb119-29"><a href="#cb119-29" aria-hidden="true" tabindex="-1"></a>  <span class="ex">securityContext:</span></span>
<span id="cb119-30"><a href="#cb119-30" aria-hidden="true" tabindex="-1"></a>    <span class="ex">fsGroup:</span> 1000670000</span>
<span id="cb119-31"><a href="#cb119-31" aria-hidden="true" tabindex="-1"></a>    <span class="ex">seLinuxOptions:</span></span>
<span id="cb119-32"><a href="#cb119-32" aria-hidden="true" tabindex="-1"></a>      <span class="ex">level:</span> s0:c26,c10 <span class="co"># &lt;- take a note the following c10 `MCS` level rule</span></span>
<span id="cb119-33"><a href="#cb119-33" aria-hidden="true" tabindex="-1"></a>    <span class="ex">seccompProfile:</span></span>
<span id="cb119-34"><a href="#cb119-34" aria-hidden="true" tabindex="-1"></a>      <span class="ex">type:</span> RuntimeDefault</span>
<span id="cb119-35"><a href="#cb119-35" aria-hidden="true" tabindex="-1"></a>  <span class="ex">serviceAccount:</span> default</span>
<span id="cb119-36"><a href="#cb119-36" aria-hidden="true" tabindex="-1"></a>  <span class="ex">serviceAccountName:</span> default</span></code></pre></div>
<p>Each project uses a unique <code>MCS</code> level for deployed
applications this <code>MCS</code> level permits each project
applications to communicate only with resources in the same project. Let
us continue looking at pod security context components with pod
capabilities</p>
<h3 id="managing-linux-capabilities">Managing Linux capabilities</h3>
<p>The capabilities listed in the app-cli security context are Linux
capabilities that have been removed from the container process. Linux
capabilities are permissions assigned to, or removed from processes by
the Linux kernel:</p>
<div class="sourceCode" id="cb120"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>securityContext:</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>    capabilities:</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>        drop:</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>        - KILL</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>        - MKNOD</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>        - STGID</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>        - SETUID</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a>        - SYS-CHROOT</span></code></pre></div>
<p>Capabilities allow a process to perform administrative task on the
system. The root user on a Linux server can run commands with all Linux
capabilities by default. That is why the root user can perform task like
opening TCP ports below 1024 which is provided by the
<code>CAP_NET_BIND_SERVICE</code> capability, and loading modules into
the Linux kernel, which is provided by the <code>CAP_SYS_MODULE</code>
capability.</p>
<p>You can add capabilities to a pod if it needs to be able to perform a
specific type of task. Add them to the capabilities .add list in the
pod’s security context. To remove default capabilities from pods, add
the capabilities you want to remove, add them to the drop list. This is
the default action in OpenShift. The goal is to assign the fewest
possible capabilities for a pod to fully function. This least privileged
model ensures that pods can not perform tasks on the system that are not
related, to their application proper function. The default value for the
privileged option is False; setting the privileged option to True is the
same as giving the pod the capabilities of the root user on the system.
Although doing so should not be common practice privileged pods can
exist and be useful under certain circumstances. A great example is the
HAProxy pod we already talked about. It runs a s a privileged container
so it can bind to port 80 on its node to handle incoming application
requests. When an application needs access to host resources that can
not be easily provided to the pod, running a privileged container may
help.</p>
<p>The last value in the security context that we need to look at is
what controls the user ID that the pod is run with -
<code>runAsUser</code> parameter</p>
<h3 id="controlling-the-user-id">Controlling the user ID</h3>
<p>In OpenShift by default each project deploys pods using a random UID.
Just like the <code>MCS</code> level the UID is common for all pods in a
project to allow easier interactions between pods when needed. The UID
for each pod is listed in the security context in the
<code>runAsUser</code> parameter. By default OpenShift does not allow
application to be deployed using UID 0, which is the default UID for the
system’s root user. There are not any known ways for UID 0 to break out
of a container, but being UID 0 in a container means you must be
incredibly careful about taking away capabilities and ensuring proper
file ownership on the system. In an ounce of prevention that can prevent
the need for a pound of a cure down the road</p>
<p>The components in a pod or a container security context are
controller by the security context constraints <code>SCC</code> -
assigned to the pod when its deployed. An <code>SCC</code> is a
configuration applied to pods that outlines the security context
components it will operate with. We will discuss the <code>SCC</code> in
more depth in the next section when you deploy an application in your
cluster that needs a more privileged security context than the default
one. This application is a container image scanning utility that looks
for security issues in container images in your OpenShift registry.</p>
<h2 id="scanning-container-images">Scanning container images</h2>
<p>OpenShift is only as secure as the containers it deploys. Even if
your container images are built using proven vetted base images supplied
by vendors or created using your own secure workflows, you will need a
process to ensure that the image you are using do not have any security
issues as they age in your cluster. The most straightforward solution
for this challenge is to scan you container images. We are going to scan
a single container image on demand in this section in a production
environment image scanning should be an integral component in your
application deployment workflow. Companies like Black Duck Software and
<code>Twistlock</code> have image scanning and compliance tools that
integrate with OpenShift. You must be able to trust what is running in
your containers and quickly fix issues when they are found. An entire
industry has sprung up in the past few years that provides container
image scanning products to help make this an every day reality. These
scanning utilities have the capabilities to annotate or tag images with
metadata, that metadata, is then used to determine if the image is
deemed a security risk or not, in the next section we will see how we
can manually annotate images ourselves. The process is the same that an
automated tool image scanner would take.</p>
<h2 id="annotating-images-with-security-information">Annotating images
with security information</h2>
<p>OpenShift is configured with image policies that control which images
are allowed to run on your cluster. The full documentation can be found
at the official RedHat OpenShift documentation page. Annotations in the
image metadata enforce image policies you can add these annotation
manually. The deny-execution policy prevents an image from running on
the cluster under any condition. To apply this policy to the image you
can use the annotate command on the <code>oc</code> command line tool
for OpenShift</p>
<div class="sourceCode" id="cb121"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will annotate the image with the sha-id with the deny execution</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc annotate image <span class="op">&lt;</span>image-sha-id<span class="op">&gt;</span> images.openshift.io/deny-execution=true</span></code></pre></div>
<p>Image policies do not affect running pods, but they prevent an image
with the deny execution annotation from being used for deployments. To
see this in action, delete the active pods for the annotated image.
Normally the replication controller for the deployment will
automatically deploy a new version of the pod based on the correct base
image. But no new pod will be deployed in this case, the replication
controller will be stopped in its tracks , when it checks and sees that
the image is annotated therefore it will not be allowed to execute the
container runtime and start a new container for the target image.
Looking at the events for the app project you can see that the image
policies in OpenShift are reading the annotation that was added to the
image and preventing a new pod from being deployed.</p>
<div class="sourceCode" id="cb122"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see the events for the given namespace or project, you might notice something like that in the output of the events</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> oc events <span class="at">-n</span> <span class="op">&lt;</span>namespace-project<span class="op">&gt;</span></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the following output is abridged, but that is what one would expect from the events list for the namespaces / project</span></span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> Warning <span class="ex">FailedCreate</span></span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> Error <span class="ex">creating:</span> Pod <span class="st">&quot;&quot;</span> is invalid: spec.containers<span class="pp">[</span><span class="ss">0</span><span class="pp">]</span>.image: Forbidden:</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> this <span class="ex">image</span> is prohibited by policy</span></code></pre></div>
<p>This process manually scans a container image and adds an annotation
to it if security issues are found. The annotation is read by the
OpenShift image-policy engine and prevents any new pods from being
deployed using that image. Automated solutions like Black Duck and
<code>Twistlock</code> handle this dynamically, including annotations
about the security findings and information about the scan. These
annotations can be used for security reporting and to 3ensure that the
most secure application are deployed in OpenShift at all times. We
started this section with SELinux and worked our way up to the security
context that define how pods are assigned security permissions in
OpenShift. As we said at the start of this section this is not a
comprehensive list or a complete security workflow. Our goal has been to
introduce you to what are the most important security concepts in
OpenShift and give you enough information to begin to use and customize
them as you gain experience with OpenShift</p>
</body>
</html>
