<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>k8s-deep-dive</title>
  <style>
    html {
      font-size: 12pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  \usepackage{listings}
  \usepackage{xcolor}

  \lstset{
      basicstyle=\ttfamily,
      backgroundcolor=\color{black!10},
      showspaces=false,
      showstringspaces=false,
      showtabs=false,
      tabsize=2,
      captionpos=b,
      breaklines=true,
      breakautoindent=true,
      linewidth=\textwidth
  }
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#history" id="toc-history">History</a></li>
<li><a href="#meaning" id="toc-meaning">Meaning</a></li>
<li><a href="#kubernetes-and-docker"
id="toc-kubernetes-and-docker">Kubernetes and Docker</a></li>
<li><a href="#kubernetes-and-swarm"
id="toc-kubernetes-and-swarm">Kubernetes and Swarm</a>
<ul>
<li><a href="#theory" id="toc-theory">Theory</a></li>
<li><a href="#control-plane" id="toc-control-plane">Control
plane</a></li>
<li><a href="#packaging-applications"
id="toc-packaging-applications">Packaging applications</a></li>
<li><a href="#declarative-model" id="toc-declarative-model">Declarative
model</a></li>
<li><a href="#pods" id="toc-pods">Pods</a></li>
<li><a href="#deployments" id="toc-deployments">Deployments</a></li>
<li><a href="#services" id="toc-services">Services</a></li>
</ul></li>
<li><a href="#clusters" id="toc-clusters">Clusters</a></li>
<li><a href="#deployments-1" id="toc-deployments-1">Deployments</a>
<ul>
<li><a href="#theory-1" id="toc-theory-1">Theory</a></li>
<li><a href="#replicasets"
id="toc-replicasets"><code>ReplicaSets</code></a></li>
<li><a href="#pods-1" id="toc-pods-1">Pods</a></li>
<li><a href="#rollouts" id="toc-rollouts">Rollouts</a></li>
<li><a href="#rollbacks" id="toc-rollbacks">Rollbacks</a></li>
<li><a href="#labels" id="toc-labels">Labels</a></li>
<li><a href="#skeleton" id="toc-skeleton">Skeleton</a></li>
</ul></li>
<li><a href="#services-1" id="toc-services-1">Services</a>
<ul>
<li><a href="#theory-2" id="toc-theory-2">Theory</a></li>
<li><a href="#labels-1" id="toc-labels-1">Labels</a></li>
<li><a href="#endpoints" id="toc-endpoints">Endpoints</a></li>
<li><a href="#types" id="toc-types">Types</a></li>
<li><a href="#registration" id="toc-registration">Registration</a></li>
<li><a href="#discovery" id="toc-discovery">Discovery</a></li>
<li><a href="#network-magic" id="toc-network-magic">Network
magic</a></li>
<li><a href="#network-traffic" id="toc-network-traffic">Network
Traffic</a></li>
<li><a href="#namespaces" id="toc-namespaces">Namespaces</a></li>
<li><a href="#skeleton-1" id="toc-skeleton-1">Skeleton</a></li>
</ul></li>
<li><a href="#ingress" id="toc-ingress">Ingress</a>
<ul>
<li><a href="#theory-3" id="toc-theory-3">Theory</a></li>
<li><a href="#network-traffic-1" id="toc-network-traffic-1">Network
traffic</a></li>
<li><a href="#cluster-traffic" id="toc-cluster-traffic">Cluster
Traffic</a></li>
<li><a href="#skeleton-2" id="toc-skeleton-2">Skeleton</a></li>
</ul></li>
<li><a href="#storage" id="toc-storage">Storage</a>
<ul>
<li><a href="#the-big-picture" id="toc-the-big-picture">The big
picture</a></li>
<li><a href="#the-storage-providers" id="toc-the-storage-providers">The
Storage Providers</a></li>
<li><a href="#the-csi---container-storage-interface"
id="toc-the-csi---container-storage-interface">The CSI - Container
Storage Interface</a></li>
<li><a href="#the-persistent-volume-subsystem"
id="toc-the-persistent-volume-subsystem">The Persistent Volume
Subsystem</a></li>
<li><a href="#storageclass-1"
id="toc-storageclass-1"><code>StorageClass</code></a></li>
<li><a href="#application" id="toc-application">Application</a></li>
</ul></li>
<li><a href="#configmaps-secrets"
id="toc-configmaps-secrets"><code>ConfigMaps</code> &amp; Secrets</a>
<ul>
<li><a href="#the-big-picture-1" id="toc-the-big-picture-1">The big
picture</a></li>
<li><a href="#configmaps"
id="toc-configmaps"><code>ConfigMaps</code></a></li>
<li><a href="#secrets" id="toc-secrets">Secrets</a></li>
</ul></li>
<li><a href="#statefulset"
id="toc-statefulset"><code>StatefulSet</code></a>
<ul>
<li><a href="#theory-4" id="toc-theory-4">Theory</a></li>
<li><a href="#naming" id="toc-naming">Naming</a></li>
<li><a href="#creation" id="toc-creation">Creation</a></li>
<li><a href="#deleting" id="toc-deleting">Deleting</a></li>
<li><a href="#volumes" id="toc-volumes">Volumes</a></li>
<li><a href="#handling-failures" id="toc-handling-failures">Handling
Failures</a></li>
<li><a href="#services-2" id="toc-services-2">Services</a></li>
<li><a href="#network-traffic-2" id="toc-network-traffic-2">Network
traffic</a></li>
<li><a href="#skeleton-4" id="toc-skeleton-4">Skeleton</a></li>
</ul></li>
<li><a href="#security" id="toc-security">Security</a>
<ul>
<li><a href="#theory-5" id="toc-theory-5">Theory</a></li>
<li><a href="#authentication"
id="toc-authentication">Authentication</a></li>
<li><a href="#authentication-1"
id="toc-authentication-1">Authentication</a></li>
<li><a href="#authorization"
id="toc-authorization">Authorization</a></li>
<li><a href="#admission-control" id="toc-admission-control">Admission
control</a></li>
<li><a href="#general-summary" id="toc-general-summary">General
Summary</a></li>
</ul></li>
<li><a href="#kubernetes-api" id="toc-kubernetes-api">Kubernetes API</a>
<ul>
<li><a href="#theory-6" id="toc-theory-6">Theory</a></li>
<li><a href="#serialization"
id="toc-serialization">Serialization</a></li>
<li><a href="#the-api-server-1" id="toc-the-api-server-1">The API
server</a></li>
<li><a href="#the-api" id="toc-the-api">The API</a></li>
<li><a href="#the-core" id="toc-the-core">The core</a></li>
<li><a href="#named-groups" id="toc-named-groups">Named Groups</a></li>
</ul></li>
<li><a href="#thread-modeling" id="toc-thread-modeling">Thread
modeling</a>
<ul>
<li><a href="#spoofing" id="toc-spoofing">Spoofing</a></li>
<li><a href="#tampering" id="toc-tampering">Tampering</a></li>
<li><a href="#repudiation" id="toc-repudiation">Repudiation</a></li>
<li><a href="#information-disclosure"
id="toc-information-disclosure">Information Disclosure</a></li>
<li><a href="#denial-of-service" id="toc-denial-of-service">Denial of
Service</a></li>
<li><a href="#elevation-of-privilege"
id="toc-elevation-of-privilege">Elevation of privilege</a></li>
</ul></li>
<li><a href="#real-world-security" id="toc-real-world-security">Real
world security</a>
<ul>
<li><a href="#cicd-pipeline" id="toc-cicd-pipeline">CI/CD
pipeline</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p><code>Kubernetes</code> is an application <code>orchestrator</code>,
for the most part it orchestrates containerized cloud native micro
services. An orchestrator is a system that deploys and manages apps, it
can deploy your app and dynamically respond to changes, for example k8s,
can:</p>
<ol type="1">
<li>Deploy you app</li>
<li>Scale it up and down dynamically based on demand</li>
<li>Self heal when things break</li>
<li>Perform zero downtime rolling updates and rollbacks</li>
<li>Many, many more things</li>
</ol>
<p><code>What really is containerised app</code> - it is an app that
runs in a container, before we had containers, apps ran on physical
servers or virtual machines, containers are just the next iteration of
how we package and run apps, as such they are faster more lightweight
and more suited to modern business requirements than servers and virtual
machines</p>
<p><code>What is a cloud native app</code> - it is one that is designed
to meet the cloud like demands of auto scaling self healing rolling
updates, rollbacks and more, it is important to be clear that cloud
native apps are not apps that will only run in the public cloud, yes
they absolutely can run on a public clouds, but they can also run
anywhere that you have k8s even your on premise datacenter.</p>
<p><code>What are microservice apps</code> - is built from lots of
independent small specialised parts that work together to form a
meaningful app. For example you might have an e-commerce app that
comprises all of the following small components</p>
<ol type="1">
<li>Web front end</li>
<li>Catalog service</li>
<li>Shopping cart</li>
<li>Authentication service</li>
<li>Logging service</li>
<li>Persistent store</li>
</ol>
<p>Each of these individual services is called a micro service,
typically each is coded and owned by a different team, each can have its
own release cycle and can be scaled independently, for example you can
patch and scale the logging micro service without affecting any of the
others. Building apps this way is vital for cloud native features, For
the most part, each microservice runs as a container, assuming that
e-commerce app with the 6 microservice there would be one or more web
front end containers one or more catalog containers one or more shipping
cart containers etc. With all of this in mind</p>
<p><code>Kubernetes deploys and manages (orchestrates) apps that are packaged and run as containers (containerized) and that are built in ways (cloud native microservice) that allows them to scale, self heal and be updated in line with modern cloud like requirements.</code></p>
<h2 id="history">History</h2>
<p>Since amazon brought the Amazon Web Services, the world changed,
since then everyone is playing catch-up. One of the companies trying to
catch up was Google. It has its own very good cloud and needs a way to
abstract the value of AWS and make it easier for potential customers to
get off AWS and into their cloud. Google also has a lot of experience
working with containers at scale, for example huge google apps such as
Search and Gmail have been running at extreme scale on containers for a
lot of years, since way before Docker brought us easy to use containers.
To orchestrate and manage these containerised apps, Google had a couple
of in-house proprietary systems called Borg and Omega. Well Google took
the lessons learned from these systems, and created a new platform
called Kubernetes, and donated it to the newly formed
<code>Cloud Native Computing Foundation (CNCF)</code> in 2014, as an
open source project. Kubernetes enables two things Google and the rest
of the industry needs</p>
<ol type="1">
<li>It abstracts underlying infrastructure such as <code>AWS</code></li>
<li>It makes it easy to move apps on and off clouds</li>
</ol>
<p>Since its introduction in 2014, Kubernetes has become the most
important cloud native technology on the planet. Like many of the modern
cloud native projects, it’s written in Go, it is built in the open on
GitHub it is actively discussed on the IRC channels, you can follow it
everywhere on social media, there are also regular conferences and
regular meetups</p>
<h2 id="meaning">Meaning</h2>
<p>The name Kubernetes comes from the Greek word meaning Helmsman - the
person steers the seafaring ship. This theme is reflected in the logo
which is the wheel (helm control) of a sea faring ship. You will often
see it shortened to k8s - pronounced <code>kate</code>. The number 8
replaces the 8 characters between the K and the S in the name, and that
is people sometimes joke that Kubernetes has girlfriend named Kate</p>
<h2 id="kubernetes-and-docker">Kubernetes and Docker</h2>
<p>Kubernetes and Docker are two complementary technologies, Docker has
tools that build and package apps as container images. It can also run
containers, Kubernetes can’t do either of those things, Instead,
Kubernetes operates at a higher level providing orchestration services
such as self-healing, scaling and updating. It is common practice to use
Docker for build time tasks such as packaging apps as containers, but
then use a combination of Kubernetes and Docker to run them. In this
model, Kubernetes preforms the high level orchestration tasks, while
Docker performs the low level tasks such as starting and stopping
containers.</p>
<p>Assume you have a Kubernetes cluster with 10 nodes, to run your
production app. Behind the scenes each cluster node is running Docker as
its container runtime. This means Docker is the low-level technology
that starts and stops the containerised apps. Kubernetes is the higher
level technology that looks after the bigger picture, it tells docker
how to do that, also it is deciding which nodes to run containers on,
deciding when to scale up or down, and execute updates. Docker is not
the only container runtime that Kubernetes supports, it also does
support <code>gVisor</code>, <code>containerd</code> and
<code>kata</code>. Kubernetes has features which abstract the container
runtime and make it interchangeable</p>
<ol type="1">
<li><p>The container runtime interface (CRI) - is an abstraction layer
that standardizes the way 3rd party container runtimes work with
Kubernetes</p></li>
<li><p>Runtime Classes allows you to create different classes of
runtimes. For example the <code>gVisor</code> or <code>Kata</code>
Containers runtimes might provide better workload isolation than the
Docker and <code>containerd</code> runtimes</p></li>
</ol>
<h2 id="kubernetes-and-swarm">Kubernetes and Swarm</h2>
<p>In 2016 and 2017 we had the orchestrator wars, where
<code>Docker Swarm</code>, <code>Mesosphere DCOS</code>, and
<code>Kubernetes</code> completed to become the de-facto container
orchestrator. To cut a long story short, Kubernetes WON.</p>
<p>There is a good chance you will hear people talk about how Kubernetes
relates to Google’s Borg and Omega systems, as previously mentioned,
Google has been running containers at scale for a long time - apparently
crunching through billions of containers a week. So yes, Google has been
running things like <code>Search</code>, <code>Gmail</code>, and
<code>GFS</code> on lots of containers for a very longs time.
Orchestrating these containerised apps was the job of a couple of
in-house technologies called Borg and Omega. So it is not a huge stretch
to make the connection with Kubernetes - all three are in the game or
orchestration or containers at scale, and they are all related to
Google.</p>
<p><code>Kubernetes is not an opened source version of Borg and Omega, it shares common traits and technologies, common DNA, if you wish, but the Borg and Omega are still proprietary closed source projects</code></p>
<p>They are all separate but all three are related, in fact, some of the
people who built Borg and Omega were and still are involved with
Kubernetes. So although Kubernetes was built from scratch, it leverages
much of what was learned at Google with Omega and Borg</p>
<h3 id="theory">Theory</h3>
<p>At the highest level, Kubernetes is two things - A cluster to run
apps on &amp; an orchestrator of cloud native microservice apps.</p>
<h4 id="kubernetes-as-an-os">Kubernetes as an OS</h4>
<p>Kubernetes has emerged as the de-facto platform for deploying and
managing cloud native apps, in many ways it is like an operating system
for the cloud. In the same way that Linux abstracts the hardware
differences between server platforms, Kubernetes abstracts the
differences between the different private and public clouds. Net result
is that as long as you are running Kubernetes, it does not matter if the
underlying systems are on premises, in your own datacenter, edge devices
or in the public cloud or domain.</p>
<h4 id="kubernetes-as-a-cluster">Kubernetes as a cluster</h4>
<p>Kubernetes is like any other cluster - a bunch of machines to host
apps on. We call these machines nodes, and they can be physical servers,
virtual machines, cloud instances, Raspberry Pis, and more. A Kubernetes
cluster is made of a <code>control plane</code> and <code>nodes</code>.
This control plane exposes the API, has scheduler for assigning work,
and records the state of of the cluster and apps in a persistent store.
Nodes are where user apps run. It can be useful to think of the
<code>control plane</code> as the brains of the cluster and the nodes as
the muscle. In this analogy the <code>control plane</code> is the brains
because it implements the clever features such as scheduling,
auto-scaling and zero-downtime rolling updates.</p>
<h4 id="kubernetes-as-orchestrator">Kubernetes as orchestrator</h4>
<p>Orchestrator is just a fancy word for a system that takes care of
deploying and managing apps. If we take a quick analogy from the real
world, a football team is made of individuals. Every individual is
different and each has a different role to play in the team - some
defend some attack some are great at passing some tackle some shoot…
Along comes the coach and she or he gives everyone a position and
organizes them into a team with a purpose. The coach also makes sure
that the team keeps its formation sticks to the game-plan and deals with
any injuries and other changes in the circumstances. Well microservices
apps on Kubernetes are the same.</p>
<p>You start out with lots of individual specialised microservices. Some
serve web pages, some do authentication some perform searches, other
persist data. Kubernetes comes along - like the coach, organizes
everything into a useful app and keeps things running smoothly. It even
responds to events and other changes in the circumstances -
auto-scaling, updating, rolling release etc.</p>
<p>When we start out with an app, package it as a container then give it
to the cluster - Kubernetes. The cluster is made up of one or more
control plane nodes and a bunch of worker nodes. As already stated,
control plane nodes implement the cluster intelligence, worker nodes are
where user apps run.</p>
<h3 id="control-plane">Control plane</h3>
<p>As previously mentioned a Kubernetes cluster is made of control plane
nodes and worker nodes. These are Linux hosts that can be virtual
machines, bare metal servers in your datacenter or basement, instances
in a private or public cloud. You can even run Kubernetes on ARM and IoT
devices</p>
<p>A Kubernetes control plane node is a server running collection on
system services that make up the control plane of the cluster. Sometimes
we call those Masters, Heads or Head nodes. The simplest setups run a
single control plane node. However this is only suitable for labs and
test environments, for production environments multiple control plane
nodes configured for high availability is vital. Also considered a good
practice not to run user apps on control plane nodes. This frees them up
to concentrate entirely on managing the cluster. These are some of the
core components</p>
<ol type="1">
<li><strong>etcd</strong>: A distributed key-value store that stores all
cluster data, i.e called the cluster store.</li>
<li><strong>kube-apiserver</strong>: The front-end for the Kubernetes
control plane, exposing the Kubernetes API.</li>
<li><strong>kube-scheduler</strong>: Assigns workloads (Pods) to nodes
based on resource availability and constraints.</li>
<li><strong>kube-controller-manager</strong>: Runs controllers that
regulate the state of the cluster (e.g., Node Controller, Replication
Controller).</li>
</ol>
<p><code>Kubernetes is self bootstrapping - meaning that components such as the API server, just like other system components part of kubernetes, are backed by kubernetes native objets - like pods, deployments and services, located in the kube-system namespace, just the way an actual user would deploy their own images and containers, in services, deployments, and so on, pretty much the entire set of system kubernetes components, are represented with the same types of objects like services, deployments, endpoints, replicasets and so on. The kubernetes enviornment can be self inspected, meaning that we can easily see the different components, part of the contorl plane, which are deployed as native kubernetes objects, with the kubectl command, we just have to look in the kube-system namespace for them, not in the default namespace. Other system components like ingress controllers might be deployed in different than the kube-system namespace, but the general rule mentioned above still holds, kubernetes is in a way self-bootstrapping</code></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pods <span class="at">-n</span> kube-system</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                     READY STATUS  RESTARTS AGE</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">coredns-668d6bf9bc-mtpmm</span> 1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">etcd</span>                     1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-apiserver</span>           1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-controller-manager</span>  1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-proxy-z9gf9</span>         1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-scheduler</span>           1/1   Running 1        <span class="er">(</span><span class="ex">4h12m</span> ago<span class="kw">)</span> <span class="ex">4h21m</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="ex">storage-provisioner</span>      1/1   Running 3        <span class="er">(</span><span class="ex">3h7m</span>  ago<span class="kw">)</span> <span class="ex">4h21m</span></span></code></pre></div>
<p>Above we can see a very simple example of what the control plane
consists of, as we can see these are the actual pods, backed by an
image, which are responsible for the internal mechanisms and workings of
the kubernetes orchestration, we can say
<code>kubernetes is run on itself, it is a self-bootstraping system</code>.
Each of those components, will be investigated in depth, further down in
this document, but for now this is the overarching way we can look at
the kubernetes architecture.</p>
<h4 id="the-api-server">The API Server</h4>
<p>The API server is the Grand Central of Kubernetes. All communication,
between all components must go through the API server. It is important
to understand that internal system components as well as external user
components all communicate via the API server - all roads lead to the
API server</p>
<p>It exposes a RESTful API that you POST YAML configuration files to
over HTTPS. These YAML files which we sometimes call manifests describe
the desired state of an app. This desired state includes things like
which container image to use, which ports to expose and how many Pod
replicas to run. All requests to API server are subject to
authentication and authorization checks. Once these are done, the
configuration in the YAML file is validated, persisted to the cluster
store, and work is scheduled to the server</p>
<p>As mentioned already the api server is nothing more than a pod backed
by an image, and a service objects. Meaning that we can simply see those
by doing - <code>kubectl get pods -n kube-system</code>. These will show
you all pods part of the system namespace, and in there one will
immediately notice, a pod called - <code>kube-apiserver</code>.</p>
<h4 id="the-cluster-store">The Cluster Store</h4>
<p>The cluster store is the only stateful part of the control plane and
persistently stores the entire configuration and state of the cluster.
As such it is vital components of every Kubernetes cluster - no cluster
store, no cluster. The cluster store is currently based on
<code>etcd</code>, a popular distributed database. As it is the single
source of truth for a cluster, you should run between 3-5 replicas of
the <code>etcd</code> service for high-availability and you should
provide adequate ways to recover when things go bad. A default
installation of Kubernetes installs a replica of the cluster store on
every control plane node and automatically configures the high
availability (HA)</p>
<p>On the topic of availability, <code>etcd</code> prefers consistency
over availability. This means it does not tolerate split brains and will
halt updates to the cluster in order to maintain consistency. However,
if this happens user apps should continue to work, you just won’t be
able to update the cluster configuration.</p>
<p>As with all distributed databases, consistency of writes to the
database is vital. For example multiple writes to the same value
originating from different places needs to be handled. <code>etcd</code>
uses the popular RAFT consensus algorithm to accomplish this.</p>
<h4 id="the-controller-manager">The Controller Manager</h4>
<p>The controller manager implements all the background controllers that
monitor cluster components and respond to events. Architecturally, it is
a controller of controllers, meaning it spawns all the independent
controllers and monitors them. Some of the controllers include the
<code>Deployment</code> controller, the <code>StatefulSet</code>
controller and the <code>ReplicaSet</code> controller. Each one is
responsible for a small subset of cluster intelligence and runs as a
background watch loop constantly watching the API Server for
changes.</p>
<p><code>The aim of the game is to ensure the observed state of the cluster matches the desired state. The logic implemented by each controller is as follows, and is at the heart of Kubernetes and declarative design patterns</code></p>
<ol type="1">
<li>Obtain desired state</li>
<li>Observe current state</li>
<li>Determine differences</li>
<li>Reconcile differences</li>
</ol>
<p>Each controller is also extremely specialized and only interested in
its own little corner of the Kubernetes cluster. No attempts is made to
over complicate design by implementing awareness of other parts of the
system, each controller takes care of its own business and leaves
everything else alone. This is key to the distributed design of
Kubernetes and adheres to the Unix philosophy. Controllers are control
loops that watch the state of the cluster and make changes to bring the
current state closer to the desired state. Some core controller
components</p>
<p><code>Controllers run on a watch-loop  meaning that these are pods or processes in the end of the day, which watch the api server, for new changes, in the desired state, and then make sure to match that in the actual state in the kubernetes environment</code></p>
<ul>
<li><p><strong>Node Controller</strong>: Manages node
lifecycle.</p></li>
<li><p><strong>Service Controller</strong>: Ensures that traffic is
routed to the correct pods on the fly.</p></li>
<li><p><strong>Deployment Controller</strong>: Manages updates to Pods
and <code>ReplicaSets</code> objects</p></li>
<li><p><strong>Ingress Controller</strong> is a special type of
controller that handles Ingress resources. It is not part of the control
plane itself but is instead a user-deployed component that runs as a Pod
in the cluster. It watches for Ingress resources and configures external
load balancers or proxies (e.g., <code>NGINX</code>,
<code>Traefik</code>) to route traffic to the appropriate
services.</p></li>
</ul>
<p><code>K8s objects allow the definition of something called annotations, these are key value pairs/maps which are a way to store metadata or configuration information, which is a general purpose way that is used by different k8s controllers, to configure their own behavior, each object in k8s is controlled and managed by an accompanying controller, this controller is responsible for managing the objects in the control plane, and each controller has specific features which can be enabled using the annotation metadata when defining the object manifest itself, the annotation values are tied to the object, but are parsed and enforced by the controller which manages the specific type of object, this is important to understand, controllers are stateless processors of the stateful k8s environment and objects</code></p>
<p>Here is an example of a manifest snippet which shows how annotations
are defined in a manifest, each is specific to the target object in this
case the target objects are ingress and deployment, do not concern
yourselves with these types of objects, the important part to take a
note of is that these annotation values are bound to the specific object
“instance” and its manifest. The object instances are in this case
defined by the <code>metadata.name</code>, the name is a special
property in k8s which gives the created object a unique name that k8s
can use to identify the object uniquely in the k8s environment, it is
not only meant to be read by users, but the k8s control plane and
controllers as well</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> networking.k8s.io/v1</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Ingress</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> my-ingress</span><span class="co"> # defines a new name for the ingress object to create</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">nginx.ingress.kubernetes.io/rewrite-target</span><span class="kw">:</span><span class="at"> /</span><span class="co"> # Rewrite URL paths</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">nginx.ingress.kubernetes.io/ssl-redirect</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span><span class="co"> # Redirect HTTP to HTTPS</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> my-deployment</span><span class="co"> # defines a new name for the deployment object to create</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">my-controller/scale-to</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;5&quot;</span><span class="co"> # Custom annotation for scaling</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">sidecar.istio.io/inject</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span><span class="co"> # Enable istio sidecar injection</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">autoscaling.alpha.kubernetes.io/metrics</span><span class="kw">:</span><span class="at"> </span><span class="st">&#39;{&quot;type&quot;:&quot;Resource&quot;,&quot;resource&quot;:{&quot;name&quot;:&quot;cpu&quot;,&quot;targetAverageUtilization&quot;:50}}&#39;</span><span class="co"> # metrics configuration</span></span></code></pre></div>
<h4 id="the-scheduler">The Scheduler</h4>
<p>At a high level, the scheduler watches the API server for new work
tasks and assigns them to appropriate healthy worker nodes. Behind the
scenes, it implements complex logic that filters out nodes incapable of
running tasks and the ranks the nodes that are capable. The ranking
system is complex but the node with the highest ranking score is
selected to run the task.</p>
<p>When identifying nodes capable of running a task, the scheduler
performs various predicate checks. These include is the node tainted,
are there any affinity or anti affinity rules, is the required network
port available on the node does it have sufficient available resources
etc. Any node incapable of running the task is ignored and those
remaining are ranked according to things such as does it already have
the required image how much free resources does it have, how many tasks
is it currently running. Each is worth points and the node with the most
points is selected to run the task. If the scheduler does not find a
suitable node, the task is not schedule and gets marked as pending. The
scheduler is not responsible for running tasks just picking the nodes to
run them. A task is normally a Pod/container.</p>
<h4 id="the-node">The Node</h4>
<p>The kubelet is main Kubelet agent and runs on every cluster node. In
fact, it is common to use the terms node and kubelet interchangeably.
When you join a node to a cluster the process installs the kubelet which
is then responsible for registering it with the cluster. This process
registers the node’s CPU, memory, and storage into the wider cluster
pool.</p>
<p>One of the main jobs of the kubelet is to watch the API server for
new work tasks. Any time it sees one, it executed the task and maintains
a reporting channel back to the control plane. If a kubelet can’t run a
task, it reports back to the control plane and lets the control plane
decide what actions to take. For example if a kubelet can not execute a
task, it is not responsible for finding another node to run it on. It
simply reports back to the control plane and the control plane decides
what to do.</p>
<h4 id="the-proxy">The Proxy</h4>
<p>The last piece of the node puzzle is the <code>kube-proxy</code>.
This runs on every node and is responsible for local cluster networking.
It ensures each node gets its own unique IP address, and it implements
local <code>iptables</code> or <code>IPVS</code> rules to handle routing
and load balancing of traffic on the Pod network. More on all of this
later on in other chapters down below.</p>
<p><code>The kube-proxy is a crucial component in the node to node internal cluster network communication stack, which allows nodes to communicate to other nodes, through the use of iptables and IPVS, rules, providing load balancing traffic between Nodes and by proxy, Pods on different Nodes. The kube proxy is not actively participating in the actual traffic, rather it monitors for new Services and Endpoints and helps setup the state of the IPVS or iptable rules on the node, the active part happens in the actual kernel reading the ipvs rules and doing the IP translation / mapping to the other nodes</code></p>
<h4 id="the-runtime">The Runtime</h4>
<p>The kubelet needs a container runtime to perform a container related
task - things like pulling images and starting and stopping containers.
In the early days, Kubernetes had native support for Docker, More
recently it has moved to a plugin model called the
<code>Container Runtime Interface (CRI)</code>. At a high level, the
<code>CRI</code> masks the internal machinery of Kubernetes and exposes
a clean documented interface for 3rd party container runtimes to plug
into. Kubernetes is dropping support for Docker as a container runtime,
this is because Docker is bloated and does not support the
<code>CRI</code> (requires a shim instead). <code>containerd</code> is
replacing it as the most common container runtime on Kubernetes</p>
<p>Besides the kubelet and proxy, Kubernetes nodes also run other
essential management components, one key components if the container
runtime interface (CRI) which is responsible for running and managing
containers. Popular CRI implementations include containerd (used in
OpenShift). Another component is the container network interface (CNI)
which handles networking for pods, including IP address allocation and
routing, different CNI plugins such as calico, flannel, cilium, and
weave-net, enable networking capabilities based on the cluster needs</p>
<p><code>containerd is the container supervisor and runtime logic stripped out from docker engine. It was donated to the CNCF by Docker Inc, and has a lot of community support. Other CRI container runtimes also exist.</code></p>
<h4 id="the-dns">The DNS</h4>
<p>As well as the various control plane and node components, every
Kubernetes cluster has an internal <code>DNS</code> service, that is
vital to service discovery. The cluster’s <code>DNS</code> service has a
static IP address that is hard coded into every Pod on the cluster. This
ensures every container and Pod can locate it and use it for discovery.
Service registration is also automatic. This means apps do not need to
be coded with the intelligence to register with Kubernetes service
discovery. Cluster <code>DNS</code> is based on open source
<code>CoreCNS</code> project.</p>
<h4 id="control-plane-summary">Control plane summary</h4>
<p><code>Kubernetes control plane nodes are servers that run the cluster's control plane services</code>.
These services are the brains of the cluster where all the control and
scheduling decisions happen. Behind the scenes, these services include
the API server, the cluster store, scheduler and the specialised
controllers.</p>
<p><code>The API server is the front end into the control plane and all instructions and communication pass through it. By default it exposes a RESTful endpoint on port 443.</code></p>
<h3 id="packaging-applications">Packaging applications</h3>
<p>An app needs to tick a few boxes to run on a Kubernetes cluster.
These include:</p>
<ol type="1">
<li>Packages as a container image</li>
<li>Wrapped the image as a container instance in a pod</li>
<li>Deployed via a declarative config manifest file</li>
</ol>
<p>It goes like this. You write an application microservice in a
language of your choice. Then you build it into a container image and
store it in a registry. At this point the app service is containerized.
Next you define a Kubernetes Pod to run the containerized app. At the
kind of high lever we are at, a Pod is just a wrapper that allows a
container to run on a Kubernetes cluster. Once you have defined the pod,
you are ready to deploy the app to Kubernetes.</p>
<p>While it is possible to run static Pods like this on a Kubernetes
cluster, the preferred model is to deploy all Pods via a higher level
controllers. The most common controller is the Deployment. It offers
scalability, self healing and rolling updates for stateless apps. You
define Deployments in YAML manifest files that specify things how many
replicas to deploy and how to perform updates. Once everything is
defined in the Deployment YAML file, you can use the Kubernetes command
line tools to post it to the API server as the desired state of the app,
and Kubernetes will implement it</p>
<h3 id="declarative-model">Declarative model</h3>
<p>The declarative model and the concept of desired state are at the
very heart of Kubernetes. So it is vital you understand them. In
Kubernetes the declarative model works like this.</p>
<ol type="1">
<li>Declare the desired state of an app, microservice in a manifest
file</li>
<li>Post the desired state to the API server</li>
<li>Kubernetes stores it in the cluster store as the app’s desired
state</li>
<li>Kubernetes implements the target desired state in the cluster</li>
<li>Controller makes sure the observed state of the app does not vary
from the desired state</li>
</ol>
<p>Manifest files are written in simple YAML and tell Kubernetes what an
app should look like. This is called desired state. It includes things
such as which image to use, how many replicas to run, which network
ports to listen on, and how to perform updates.</p>
<p>Once you have created the manifest you post it to the API server. The
easiest way to do this is with the kubectl command line utility. This
sends the manifest to the control plane as an HTTPS POST request (on
port 443)</p>
<p>Once the request is authenticated and authorized. Kubernetes inspects
the manifest, identifies which controller to send it to
(i.e. Deployments controller) and records the configuration in the
cluster store as part of the overall desired state. Once this is done
any required work tasks get scheduled to the cluster nodes where the
kubelet co-ordinates the hard work of pulling images starting containers
attaching to networks, and starting app processes.</p>
<p>Finally controllers run as background reconciliation loops that
constantly monitor the state of things, if the observed state deviates
from the desired state, Kubernetes performs the tasks which are
necessary to reconcile the differences and bring the observed state back
in sync with the desired state</p>
<p><code>It is important to understand that what we have described is the opposite of the traditional imperative model. The imperative model is where you write long scripts of platform specific commands to build and monitor things, not only is the declarative model a lot simpler than long scripts with lots of imperative commands, it also enables self-healing, scaling and lends itself to version control and self-documentation. It does all of this by telling the cluster how thing should look like. If they start to look different, the appropriate controller notices the discrepancy and does all the hard work to reconcile the situation.</code></p>
<p>Assume you have an app with a desired state that includes 10 replicas
of a web front end Pod. If a node running two replicas fails, the
observed state will be reduced to 8 replicas but desired state will
still be 10. This will be observed by a controller and Kubernetes will
schedule two new replicas to bring the total back up to 10. The same
thing will happen if you intentionally scale the desired number of
replicas up or down. You could even change the image you want to use
(this is called a <code>rollout</code>). For example if the app is
currently using <code>v2.00</code> of an image and you update the
desired state to specify <code>v2.01</code> the relevant controller will
notice the difference and go through the process of updating the cluster
so all 10 replicas are running the new version.</p>
<p>To be clear. Instead of writing a complex script to step through the
entire process of updating every replica to the new version, you simply
tell Kubernetes you want the new version and Kubernetes does the hard
work for you.</p>
<h3 id="pods">Pods</h3>
<p>In the VMware world the atomic unit of scheduling is the virtual
machine. In the docker world it is the container, in the Kubernetes
world it is the Pod. It is true that Kubernetes runs containerized apps.
However Kubernetes demands that every container runs inside a pod.</p>
<p><code>Pods are objects in the Kubernetes API, so we capitalize the first letter. This adds clarity and the official Kubernetes docs are moving towards this standard. You can think of Pods as specs and rules for running containers, they define all kind of container specific rules and boundaries, which are used when the containers are managed in their lifecycle - started, running, terminated by the container runtime (like containerd)</code></p>
<h4 id="pods-containers">Pods &amp; Containers</h4>
<p>The very first thing to understand is that the term Pod comes from a
pod of whales - in the English language we call a group of whales a pod
of whales. As the Docker logo is a whale, Kubernetes ran with the whale
concept and that is why we have Pods. The simplest model is to run a
single container in every Pod. This is why we often use the term Pod and
container interchangeably. However there are advanced use cases that run
multiple containers in a single Pod, Powerful examples of multi
container Pods include:</p>
<ul>
<li>Service meshes</li>
<li>Containers with a tightly coupled log scraper</li>
<li>Web containers supported by a helper container pulling updated
content</li>
</ul>
<p><code>The point is that a Kubernetes Pod is a construct for running one or more containers. A pod is an object, defined declaratively in the k8s state, they are not physical entities that run on the Nodes, they are used by the kubelet service or daemon to control containers</code></p>
<h4 id="pod-anatomy">Pod anatomy</h4>
<p>At the highest level a Pod is ring fenced environment to run
containers. Pods themselves do not actually run apps, apps always run in
containers, the Pod is just a sandbox to run one or more containers.
Keeping it high level, Pods ring fence an area of the host OS, build a
network stack, create a bunch of kernel namespaces and run one or more
containers</p>
<p>If you are running multiple containers in a Pod they all share the
same Pod environment. This includes the network stack, volumes
<code>IPC</code> namespace, shared memory and more. As an example this
means all containers in the same Pod will share the same IP address. If
two containers in the same Pod want to talk to each other, they can use
the Pod’s <code>localhost</code> interface.</p>
<p>Multi container Pods are ideal when you have requirements for tightly
coupled containers that may need to share memory and storage. However if
you do not need to tightly couple containers, you should put them in
their own Pods, and loosely couple them over the network. This keeps
things clean by having each Pod dedicated to a single task. However it
creates a lot of potentially <code>un-encrypted</code> network traffic.
One must seriously consider using a service mesh to secure traffic
between Pods and app services</p>
<p>Now here is an interesting part of how the pods work internally, even
though as already mentioned pods are configuration entities, managed by
the kubelet, the kubelet itself, spawns things called pause containers,
each pod is associated with these pause containers, which are more like
a namespace holders, they are responsible for holding the network
namespace and other resources a Pod environment has “promised” to its
running containers, this is done
<code>in case all containers managed by a pod die, somehow, or currently a pod has no running containers, for what ever reason</code>.
The statement that a pod is not a container <code>still holds</code>, it
is just that the kubelet uses auxiliary structures to retain the
resources and overall state associated with a pod configuration object.
This is mostly an implementation detail, and end users can not interact
with the so called <code>pause containers</code></p>
<p>Here is an example: imagine a pod with two containers - web server
and logging sidecar container</p>
<ol type="1">
<li>The kubelet starts the pause container first</li>
<li>The pause container sets up the shared network and IPC
namespaces.</li>
<li>The kubelet starts the web server container and joins it to the
pause container namespace</li>
<li>The kubelet starts the logging container and joins it to the same
namespace</li>
</ol>
<p>Now both containers share the same networking namespaces, they
communicate over localhost, and the same IPC namespace (they can use
shared memory)</p>
<p><code>Pause containers simply reserve the linux kernel namespaces, which are then used to be shared between the actual running containers configured for the Pod, all the resources that are shared between containers inside a pod, are actually bound to the namespace of the pause container, for that pod, which the kubelet has started</code></p>
<h4 id="pods-as-unit-of-scaling">Pods as unit of scaling</h4>
<p>Pods are also the minimum unit of scheduling in Kubernetes If you
need to scale an app, you add or remove Pods. You do not scale by adding
more containers to existing Pods. Multi-container Pods are only for
situations where to different but complimentary containers need to share
resources.</p>
<p><code>You never scale an app by adding more of the same app containers to a Pod, multi container pods are not a way to scale an app, they are only for co scheduling and co locating containers that need tight coupling, like a web service and a logging service, or a in memory data store etc. If you need to scale the app you add more pods, or remove pods, this is called horizontal scaling</code></p>
<h4 id="pods-atomic-operations">Pods atomic operations</h4>
<p>The deployment of a Pod is an atomic operation. This mean a Pod is
only ready for service when all its containers are up and running. The
entire Pod either comes up and is put into service or it does not and it
fails. A single Pod can only be schedules to a single node - you can not
schedule a single Pod across multiple nodes. This is also true of multi
container Pods - all containers in the same Pod run on the same
node.</p>
<h4 id="pod-lifecycle">Pod lifecycle</h4>
<p>Pods are mortal. The are created they live and they die. If they die
unexpectedly you do not have to bring them back to life. Instead
Kubernetes starts a new one in its place. However even though the new
Pod looks, smells and feels like the old one, it is not. It is a shiny
new Pod with a shiny new ID and IP address.</p>
<p>This has implications on how you design you app. Do not design them
to be tightly coupled to particular instance of a Pod. Instead design
them so that when Pods fail a totally new one can pop up somewhere else
in the cluster and seamlessly take its place</p>
<h4 id="pod-immutability">Pod immutability</h4>
<p>Pods are also immutable this means you do not change them once they
are running. Once a Pod is running you never change its configuration.
If you need to change or update it, you replace it with a new Pod
instance running the new configuration. When we have talked about
updating Pods, we have really meant delete the old one and replace it
with a new one having the new configuration The immutable nature of Pods
is a key aspect of cloud native microservices, design and patterns and
forces the following:</p>
<ul>
<li>When updates are needed replace all old pods with new ones that have
the updates</li>
<li>When failures occur replace failed Pods with new identical ones</li>
</ul>
<p>To be clear you never update the running pod, you always replace it
with a new pod containing the updates, you also never log onto failed
pods and attempt fixes you build fixes into an updated pod and replace
failed ones with the update ones.</p>
<h4 id="pods-vs-nodes">Pods vs Nodes</h4>
<p>It is vital to understand the difference between those two, while
both are part of the k8s infrastructure their purpose is vastly
different, and while usually a pod corresponds to a single container
instance, that is not always the case, as mentioned above, a Pod can in
theory run multiple containers of the same image, or even of different
images (more common) those can share a single state and make integration
and integration between these services more robust and easier in some
situations that is desirable</p>
<p><code>Pods are not Nodes, Nodes are the computing environments that run the containers,the container runtimes, the kubelet and any other component of the k8s infrastructure, those could be many things, (virtual machines, physical machines, embedded devices and so on) as long as they support running the k8s runtime, the pods, are a logical collection of containers that run on a k8s Node</code></p>
<h4 id="pod-strategies">Pod strategies</h4>
<p>The atomic unit of a scheduling on K8s is the pod, this is just a
fancy way of saying that apps deployed to the k8s are always managed by
Pods.</p>
<p>Why need Pods ? Why not just run the container on the k8s node
directly, the short answer is that you can not, k8s does not allow
containers to run directly on a cluster or a node, they always have to
be wrapped in a Pod object, there are three main reasons why Pods
exist</p>
<ol type="1">
<li>Pods augment containers</li>
<li>Pods assist in scheduling</li>
<li>Pods enable resource sharing</li>
</ol>
<p>On the augmentation front, Pods augment container in all of the
following ways.</p>
<ol type="1">
<li>Labels and annotations</li>
<li>Restart policies</li>
<li>Probes, startup, readiness, liveness, and more</li>
<li>Affinity and anti affinity rules</li>
<li>Termination control</li>
<li>Security policies</li>
<li>Resource requests and limits</li>
</ol>
<p><code>Note that containers still technically run directly on the node, the container runtime to be more precise, which is running on the node, but the container runtime and the containers themselves are managed and run by the Pods, Pods are not physically running the containers, Pods are not like containers, Pods are k8s objects/manifests and through their pod manifest spec they instruct the kubelet, and the kubelet instructs the container runtime (containerd) what to do and how to run the set of containers the Pods 'manage', what resources to allocate for them, and all the nitty gritty fine grained control a container might require. This is needed because there is NO equivalent 'container' spec which can do this, so an intermediate control object like the Pod is needed to do just THAT</code></p>
<p>Labels let you group pods and associate them with other objects in
powerful ways, annotations let you add experimental features and
integrations with 3rd party tools and services, Probes let you test the
health and status of Pods, enabling advanced scheduling, updates and
more. Affinity and anti affinity rules give you control over where Pods
run, Termination control lets you to gracefully terminate Pods and the
app s they run, Security policies let you enforce security features,
Resource requests and limits let you specify minimum and maximum values
for things like CPU and memory and disk IO. Despite bringing so many
features to the party, pods are super lightweight and add very little
overhead. Pods also enable resource sharing, they provide execution
environment for one or more containers, this shared environment includes
things such as shared filesystem network stack, memory and
fs-volumes.</p>
<p>Pods deployed directly fro the Pod manifest are called static Pods
and have no super powers such as self healing scaling and rolling
updates, This is because they are only monitored and managed by the
local kubelet process which is limited to attempting container and Pod
restarts, on the local node, if the node they are running on fails there
is no control plan process watching and capable of starting a new one on
a different node,</p>
<p>Pods deployed via controllers have all the benefits of being
monitored and managed by a highly available controller running on the
control plane, the local kubelet on the node they are running on can
still attempt local restarts but if restart attempts fail or the node
itself fails the observing controller can start a replacement pod on a
different node.</p>
<p>Just to be clear it is vital to understand that Pods as mortal, When
they die, they are gone, there is no fixing them and bringing them back
from the dead, this firmly places them in the cattle category in the
pets vs cattle paradigm, pods are cattle and when they die they get
replaced by another. This is why apps should always store state and data
outside the pod, it is also why you should not rely on individual pods,
they are ephemeral - lasting for a very short time</p>
<p>Pods are objects in the Nodes/Workers, and one can think of it as the
Pod being room in a house, the House itself, is the Node, the Room is
the pod, and the people living in that room are the containers, the Room
or Pod, does not run anything, it does only manage the containers
providing them with the shared state, it is like a bridge or adapter
between multiple containers, as already mentioned one pod object can
manage multiple containers, and can provide them with shared state which
is isolated from other pods and containers</p>
<ol type="1">
<li><p>Each pod has its own IP address, allowing it to communicate with
other Pods, between several Nodes. The container interface plugin is
responsible for assigning IP addresses and setting up networking, pods
on the same node communicate through a bridge, pods on a different nodes
communicate through routing rules set in the CNI plugin</p></li>
<li><p>Storage between containers in a given Pod is shared using
volumes, meaning that a given Pod, mounts volumes from the Host/Node to
all containers that it is responsible for, these volumes are then used
and shared only by the containers that this Pod governs.</p></li>
<li><p>Resources, the governing pod makes sure that each configured
container does not exceed the resources allocated for it, and if it does
it will restart or kill the container</p></li>
</ol>
<p><code>REMEMBER! Pods are not physical services running on the host, they are merely objects defined in the k8s deployment config, these pod objects are picked up by the kubelet, which is the service running on the Node, it is actually the active service that manages pods and by proxy containers defined for these pods, the pods are merely configuration objects, which tell the kubelet how to manage a common set of containers and what to do with them in the event of abnormal occurences or if the desired state diverges from the actual state</code></p>
<h4 id="pods-deployment">Pods deployment</h4>
<p>The process is simple, the pods are defined in files, as already
mentioned pods are mere objects, part of the k8s environment,</p>
<ol type="1">
<li>Define it in an YAML manifest file</li>
<li>Post the YAML to the REST API server</li>
<li>The server authenticates the request</li>
<li>The configuration file is validated</li>
<li>The scheduler deploys the pod to a healthy node</li>
<li>The local kubelet monitors it</li>
</ol>
<p>The pod is deployed via a controller the configuration will be added
to the cluster store as part of overall desired state that has to be
maintained and a controller will monitor it. The pod deployment process
is an atomic one, this means it is all or nothing deployment either
succeeds or it does not, you will never have a scenario where a
partially deployed pod is servicing requests, only after all the pod’s
resources are running and ready will it start servicing requests</p>
<h4 id="pod-lifecycle-1">Pod lifecycle</h4>
<p>The pods’ lifecycle starts with the YAML object, and is served down
to the API server, then it enters the <code>pending phase</code>, it is
schedule to a healthy node, with enough resources, and the local kubelet
instance running on tat node instructs the container runtime to pull all
required images and start all containers, once all containers are pulled
and running, the pod enters the <code>running phase</code>, if it is a
short lived pod, as soon as all containers terminate successfully the
Pod itself terminated and enters the succeeded state. If it is a long
running pod, it remains indefinitely in the
<code>running phase</code></p>
<p>Short lived pods can run all different types of apps, some such as
web servers are intended to be long lived and should remain in the
running phase indefinitely, if any containers in a long lived Pod fail
the local kubelet may attempt to restart them. We say the kubelet may
attempt to restart them, this is based on the container’s restart
policy, which is defined in the Pod object itself, Options include -
Always, OnFailure and Never, always is the default restart policy
appropriate for most long lived pods, Other workloads such as batch
jobs, are designed to be short lived and only run until a task is
complete, Once all containers in a short lived pod terminate, the pods
terminate and its status is set to successful, these container restart
policies - Never, OnFailure, are appropriate for short lived pods</p>
<h4 id="pod-multi-container-control">Pod multi-container control</h4>
<p>Multi container pods are powerful pattern and heavily used in the
real world, at a very high level every container should have a single
clearly defined responsibility, for example an app that pulls content
from a repository and serves it as a web page, has two clear functions -
pull the content, serve the content</p>
<p>In this example one should design two containers one responsible for
pulling the content and the other to serve the web page, we call this
separation of concerns. This design approach keeps each container small
and simple, and it encourages re-use, and makes troubleshooting simpler.
However these are scenarios where it is a good idea to tightly couple
two or more functions, consider the same example app that pulls content
and serves it via web page, a simple design would have the sync
container the one pulling content, put content updates in a volume
shared with the web container, for this to work both containers need to
run in the same Pod, so they share the same volume / storage from the
Pods execution environment. Co-locating multiple containers in the same
pod allows containers to be designed with a single responsibility but
work closely with others, Kubernetes offers several well defined multi
container Pod patterns.</p>
<ol type="1">
<li>Sidecar pattern</li>
<li>Adapter pattern</li>
<li>Ambassador pattern</li>
<li>Init pattern</li>
</ol>
<h5 id="sidecar">Sidecar</h5>
<p>The sidecar pattern is probably the most popular and most generic
multi container pattern it has a main app container and a sidecar
container, it is the job of the sidecar to augment or perform a
secondary task for the main app container, the previous example of a
main app web container plus a helper pulling up to date content is a
classic example of the sidecar pattern - the sync container pulling the
content from the external repository is the sidecar. An increasingly
important user of the sidecar model is the service mesh, at a high level
service meshes inject sidecar containers into app pods and the sidecar
do things like encrypt traffic and expose telemetry and metrics</p>
<h5 id="adapter">Adapter</h5>
<p>The adapter pattern is a specific variation of the generic sidecar
pattern where the helper container takes non standardized output from
the main container and rejigs it into a format required by an external
system, a simple example is NGINX logs being sent to Prometheus, Out of
the box Prometheus does not understand NGINX logs, so a common approach
is to put an adapter sidecar into the NGINX pod, that converts the NGINX
logs into a format accepted by Prometheus</p>
<h5 id="ambassador">Ambassador</h5>
<p>The ambassador pattern is another variation of the sidecar pattern.
This time, the helper container brokers connectivity to an external
system, for example the main app container can just dump its output to a
port the ambassador container is listening on and sit back while the
ambassador container does the hard work of getting it to the external
system.</p>
<h5 id="init">Init</h5>
<p>That pattern is not a form of the sidecar it runs a special init
container that is guaranteed to start and complete before your main app
container. As the name suggests its jobs is to run tasks and initialize
the environment for the main app container. For example a main app
container may need permissions setting an external API to be up and
accepting connections or a remote repository, cloning to a local volume,
in cases like these an init container can do that prep work and will
only exit when the environment is ready for the main app container, The
main app container will not start until the init container completes</p>
<h5 id="takeaways">Takeaways</h5>
<p>Note that while the ambassador and adapter patterns might seem
similar, they are meant for different tasks, while the adapter is meant
as mostly translator or normalization level, for data, from one form to
another. The ambassador pattern is meant for strictly handling
communication between to containers or services, it abstracts away the
communication details, for example Envoy, is a sidecar mesh, which
serves to abstract away the database connection and communication
details between a service and a database, of any type, it provides a
common communications protocol that the container can use to
communicate, without caring what is on the other side, as long as the
other side also understands that protocol, but in all actuality the
other side might be using the same pattern to receive the
communication.</p>
<h3 id="deployments">Deployments</h3>
<p>Most of the time you will deploy Pods, indirectly via a higher level
controllers. Examples of higher level controllers include
<code>Deployments</code>, <code>DeamonSets</code> and
<code>StatefulSets</code>. As an example a Deployment is a higher level
Kubernetes object that wraps around a Pod and adds features such as
self-healing, scaling, zero-downtime rollouts, and versioned
rollbacks.</p>
<p>Behind the scenes,
<code>Deployments, DeamonSets and StatefulSets</code> are implemented as
controllers that run as watch loops constantly observing the cluster and
the k8s API Server making sure observed state matches desired state.</p>
<h3 id="services">Services</h3>
<p>Since we have already mentioned that Pods can die, they are also
managed via a higher level controllers and get replaced when they die or
fail. But replacements come with a totally different IP addresses. This
also happens with rollouts and scaling operations. Rollouts replace old
Pods with new ones with new IPs. Scaling up adds new Pods with new IP
addresses, whereas calling down takes existing Pods away. Events like
these cause a lot of IP churn. The point we are making is that Pods are
unreliable and this poses a challenge. Assume you have got a
microservice app with a bunch of Pods performing video rendering. How
will this work if other parts of the app that use the rendering service
can not rely on rendering Pods being there when needed. This is where
Services come in to play. They provide reliable networking for a set of
Pods.</p>
<p>Services are fully fledged objects in the Kubernetes API - just like
Pods and Deployments. They have a front end consisting of a DNS name, IP
address and port. On the back end they load balance traffic across a
dynamic set of Pods. As pods come and go, the Service observes this,
automatically updates itself, and continues to provide that stable
networking endpoint. The same applies if you scale the number of Pods up
or down. New Pods are seamlessly added to the Service and receive
traffic. Terminated Pods are seamlessly removed from the Service and
will not receive traffic. That is the job of a Service - it is a stable
network abstraction point that provides TCP and UDP load balancing
across a dynamic set or number (replicas) of Pods/containers</p>
<p>As they operate at the TCP and UDP layer, they do not posses
application intelligence, this means they can not provide app layer host
and path routing. For that you need an Ingress which understand HTTP and
provides host and path based routing.</p>
<p><code>Services bring stable IP addresses and DNS names to the unstable world of Pods, they are the abstraction layer, that allows other Services, Pods or Containers to communicate without having to worry about the fact that a target Pod can die</code></p>
<h2 id="clusters">Clusters</h2>
<p>Namespaces are the native way to divide a single k8s cluster into
multiple virtual clusters, these are not the standard Linux kernel
namespaces, that we have already looked at, the ones responsible for
namespacing processes on the kernel level. K8s namespaces divide the
Kubernetes clusters into virtual clusters called - Namespaces</p>
<p>Namespaces partition a Kubernetes cluster and are designed as an easy
way to apply quotas and policies to groups of objects, they are not
designed for strong workload isolation. Most k8s objects are deployed
into a Namespace, these objects are said to be namespaced, and include
common objects like Pods, Services and Deployments. If you do not
explicitly define a target namespace when deployment a namespaced
object, it will be deployed to the default namespace, you can run the
following command to</p>
<p>Namespaces are a good way of sharing a single cluster among different
departments and environments for example a single cluster might have the
following Namespace, Dev, Test, QA. Each one can have its own set of
users and permissions as well as unique resource quotas, What they are
not good for isolating hostile workloads, this is because a compromised
container or pod in one namespace can wreak havoc in other namespaces,
putting into context, you should not competitive workloads together.
Every k8s cluster has set of pre created namespaces, virtual
clusters</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show the list of all namespaces in the current cluster</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get namespaces</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>            STATUS AGE</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-system</span>     Active 3d</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">default</span>         Active 3d</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-public</span>     Active 3d</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-node-lease</span> Active 3d</span></code></pre></div>
<p>The default namespace is where newly created objects go unless you
explicitly specify otherwise, Kube-system is where DNS the metrics
server and other control plane components run, Kube-public is for
objects that need to be readable by anyone, and last but not least
kube-node-lease is used for node heartbeat, and managing node
leases.</p>
<p>Namespaces are first class resources in the core v1 API group, This
means that they are stable well understood and have been around for a
long time, it also means you can create and manage them imperatively
with <code>kubectl</code> and decoratively with YAML manifests.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample namespace that is not the default one</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Namespace</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> shield</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">env</span><span class="kw">:</span><span class="at"> marvel</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the config to the cluster</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> shield-ns.yml</span></code></pre></div>
<p>When you start using Namespaces you will quickly realize it is
painful remembering to dd the -n or –namespace flag on all
<code>kubectl</code> commands. A better way might be to set your
<code>kubeconfig</code> to automatically work with a particular
namespace, the following command configures <code>kubectl</code> to run
all future commands against the shield Namespace</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this will make sure that all following commands against kubectl run in the context of the namespace `shield`</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl config set-context <span class="at">--current</span> <span class="at">--namespace</span> shield</span></code></pre></div>
<p>To deploy to a given Namespace, as already mentioned most all objects
are always tied to a namespace, and if you do not specify otherwise the
default namespace will be used when deploying objects, there are two
different ways to deploy objects to a specific namespace - imperatively
and declaratively.</p>
<p>The imperative method requires you to add the -n flag to the command,
the declarative method specifies the namespace in the YAML manifest
file. We will declaratively deploy a simple app to the shield namespace,
and test it.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ServiceAccount</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> shield</span><span class="co"> # Namespace</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> default &lt;&lt; ServiceAccount name</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> shield</span><span class="co"> # Namespace</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> the-bus</span><span class="co"> # Service name</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">nodePort</span><span class="kw">:</span><span class="at"> </span><span class="dv">31112</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">targetPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">env</span><span class="kw">:</span><span class="at"> marvel</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> shield</span><span class="co"> # Namespace</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> triskelion</span><span class="co"> # Pod name</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="at">&lt;snip&gt;</span></span></code></pre></div>
<p><code>Note the use of metadata, this is a common pattern in k8s config manifests, the metadata field is not just for humans to read, it is often used to also provide control flow to the k8s cluster itself, based on the metadata, the k8s environment knows what to do with the object, the metadata provides context for the object it is defined for in this case we define that this particular object is created for this namespace, but other metadata keys also exist and are used to e.g the name of the object is defined in the metadata field, and that same name is what can be used to reference that object in other objects. The namespace itself, we have create above, the shield name was defined in the metadata section</code></p>
<p>To deploy these resources, save the YAML manifest as file, and then
simply run <code>kubectl apply -f shield-app.yml</code>, to clean up the
same resources one can use the
<code>kubectl delete -f shield-app.yml</code>. The nice part here is
that having all of this deployed in a declarative manner, allows us to
clean up the resources using the same declaration and file, no need of
manual steps to delete each object, or having to know in what order they
need to be deleted, worrying about stopping some of the resources which
have been allocated by these k8s objects them and so on.</p>
<h2 id="deployments-1">Deployments</h2>
<p>Kubernetes offers several controllers that augment Pods with
important capabilities, the deployment controller is specifically
designed for stateless app, we will cover some other controllers later
on as well.</p>
<h3 id="theory-1">Theory</h3>
<p>There are two major pieces to deployments, the spec and the
controller, the deployment spec is a declarative. The deployment spec is
a declarative YAML object where you describe the desired state of a
stateless app, you give that to kubernetes where the deployment
controller implement and manages it, the controller aspect is highly
available and operates as a background loop reconciling observed state
with desired state. Deployment objects, and all of their features and
attributes, are defined in the apps/v1 workloads API.</p>
<p><code>Note that the kubernetes api is architecturally divided into smaller sub groups to make it easier to manage and navigate, the apps sub group is where Deployment, DeamonSets and StatefulSet and other workload related objects are defined, we sometimes call it the workloads API</code></p>
<p>You start with a stateless app package it as a container then define
it in a Pod template, at this point you could run it on the kubernetes,
however static pods like this do not self heal they do not scale and
they do not allow for easy updates and rollbacks. For these reasons you
will almost always wrap them in a deployment object.</p>
<h3 id="replicasets"><code>ReplicaSets</code></h3>
<p>Behind the scenes deployments rely heavily on another object called
replica set. While it is usually recommended not to manage replica sets
directly, deployment controller manage them, it is important to
understand the role they play, at a high level <code>Containers</code>
provide way to package apps and dependencies Pods allow containers to
run kubernetes and enable co scheduling and a bunch of other good stuff,
<code>ReplicaSets</code> manage pods and bring self healing and scaling,
<code>Deployments</code> manage replica sets and add rollouts and
rollbacks.</p>
<p><code>ReplicaSets</code> are implemented as a controller running as a
background reconciliation loop checking the right number of Pod replicas
are present on the cluster, if there are not enough it adds more, if
there too many it terminates some, assume a scenario where the desired
state is 10 replicas but only 8 are present, it makes no difference if
this is due to a failure or if it is because an <code>autoscaler</code>
has increased desired state from 8 to 10, Either way, this is a red
alert condition for Kubernetes, so it orders the control plane to bring
two more replicas.</p>
<p><code>Note that ReplicaSet are owned by the Deployment object, meaning that they are subordinated to them, and their lifecycle is tied to the Deployment object's lifecycle, when deployment configuration is updated create new ReplicaSets for the deployment to which the update was made, to begin the deployments of the new Pods, while the old ReplicaSet pods are being wound down.</code></p>
<h3 id="pods-1">Pods</h3>
<p>A deployment object only manages a single pod template, for example,
an app with a front end web service and a back end catalog will have a
different pod for each (two Pod templates). As a result it will need two
deployment objects one managing front end pods, the other managing back
end pods, however a deployment can manage multiple replicas of the same
pod, for example the front end deployment might be managing 5 identical
front end pod replicas.</p>
<h3 id="rollouts">Rollouts</h3>
<p>Rolling updates with deployments zero downtime rolling updates of
stateless apps are what Deployments are all about and they are amazing,
however they require a couple of things from your microservice apps in
order to work properly, - loose coupling via API and backward and
forward compatibility. Both of these are hallmarks of modern cloud
native microservice apps and work as follows. All microservices in an
app should be decoupled and only communicate via a well defined API.
This allows any microservice to be updated without having to think about
clients and other microservices that interact with them everything talks
to a formalized API that expose documented interface and hide specifics.
Ensuring releases are backwards and forwards compatible means you ca
perform independent, updated without having to factor in which versions
of the clients are consuming the service. With those points in mind,
zero downtime rollouts work like this:</p>
<p>Assume you are running 5 replicas of a stateless web front end. As
long as all clients communicate via API and are backwards and forwards
compatible it does not matter which of the 5 replicas a client connects
to. To perform a rollout, Kubernetes creates a new replica running the
new version and terminates an existing one running the old version. At
this point you have got 4 replicas on the old version and 1 on the new.
This process repeats until all 5 replicas are on the new version. As the
app is stateless and there are always multiple replicas up and running
clients experience no downtime or interruption of service, there is
actually a lot that goes on behind the scenes so let us look at this</p>
<p>You design apps which each discrete microservice as its own Pod. For
convenience self healing and scaling rolling update and more - you wrap
the pod in their own higher level controller such a Deployment. Each
Deployment describes all the following</p>
<ul>
<li>How many Pods replicas</li>
<li>What image to use for the Pods container</li>
<li>What network ports to expose</li>
<li>Details about how to perform rolling updates</li>
</ul>
<p>In the case of Deployments when you post the YAML file to the API
server, the Pods get scheduled to healthy nodes and a deployment and
<code>ReplicaSets</code> work together to make the magic happen. The
<code>ReplicaSet</code> controller sits in a watch loop making sure our
observed state and desired state are in agreement. A Deployment object
sits above the <code>ReplicaSet</code> governing its configuration as
well as how rollouts will be performed. Now assume you are exposed to a
known vulnerability and need to rollout a newer image, with the fix, to
do this you update the same Deployment YAML file with the new image
version and re-post that to the API server. This updates the existing
Deployment object with a new desired state requesting the same number of
Pods but all running the newer image.</p>
<p>To make this happen, kubernetes creates a second
<code>ReplicaSet</code> to create and manage the Pods with the new
image, you now have two <code>ReplicaSets</code> - the original one for
the Pods with the old image, and the new one, for the Pods with the new
image. As Kubernetes increases the number of Pods in the new
<code>ReplicaSet</code> it decreases the number of Pods in the old
<code>ReplicaSet</code>. Net result you get a smooth incremental rollout
with zero downtime.</p>
<p>You can rinse and repeat the process for future updates - just keep
updating the same Deployment manifest file which should be stored in a
version control system</p>
<p>The way Kubernetes knows how to correctly rollout a given pod is by
using the list of labels, the Deployment controller looks for when
finding Pods to update during rollouts operations in this example it is
looking for Pods with the given label,
<code>the label selector is immutable you can not change it once it is deployed</code>.</p>
<p>So imagine the following situation we have a deployment which has a
container image with version 1.0, and we would like to deploy a set of
new pods with a new version 2.0, this would imply we have to only change
one thing, that is the version of the image defined in the YAML manifest
file of the deployment so something like changing the image tag from 1
to 2 here - <code>image: nigelpoulton/k8sbook:2.0</code>, this would
trigger the internal process of creating new <code>ReplicaSet</code> for
the new pods, which will start creating new pods, with the new version,
scaling to the target number of replicas which we have defined in our
deployment manifest file. We simply have to apply with -
<code>kubectl apply -f deploy.yml</code>. To monitor the status one can
use <code>kubectl rollout status deployment hello-deploy</code>.</p>
<p>The rollouts can also be paused, this can be done with the rollout
pause command, for example
<code>kubectl rollout pause deploy hello-deploy</code>, if one tries to
run the <code>kubectl describe</code> provides some information on the
state of the deployment and also the state of the
<code>ReplicaSet</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pause the deployment, after we have run the apply -f deploy.yml, immediately just pause</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl rollout pause deploy hello-deploy</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">deployment.apps/hello-deploy</span> paused</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print out the current state of the deployment object, note it is marked as DeploymentPaused</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe deploy hello-deploy</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                  hello-deploy</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>           deployment.kubernetes.io/revision: 2</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Selector:</span>              app=hello-world</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Replicas:</span>              10 desired <span class="kw">|</span> <span class="ex">4</span> updated <span class="kw">|</span> <span class="ex">11</span> total <span class="kw">|</span> <span class="ex">11</span> available <span class="kw">|</span> <span class="ex">0</span> unavailable</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="ex">StrategyType:</span>          RollingUpdate</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="ex">MinReadySeconds:</span>       10</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="ex">RollingUpdateStrategy:</span> 1 max unavailable, 1 max surge</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Conditions:</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Type</span>            Status                  Reason</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="ex">----</span>            <span class="at">------</span>                  <span class="at">------</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Available</span>       True                    MinimumReplicasAvailable</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="ex">Progressing</span>     Unknown                 DeploymentPaused</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="ex">OldReplicaSets:</span> hello-deploy-85fd664fff <span class="er">(</span><span class="ex">7/7</span> replicas created<span class="kw">)</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="ex">NewReplicaSet:</span>  hello-deploy-5445f6dcbb <span class="er">(</span><span class="ex">4/4</span> replicas created<span class="kw">)</span></span></code></pre></div>
<p>The <code>deployment.kubernetes.io</code> annotation shows the object
is on revision 2, the revision 1, was the initial rollout and this
update we have done is revision 2, Replicas shows the rollout is
incomplete, the third line from the bottom shows the Deployment
condition as progressing but paused, finally you can see that the
<code>ReplicaSet</code> for the initial release is would up to 7
replicas and the one for the new release is up to 4, paused right before
all new pods were actually finished deploying</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To resume the deployment process</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl rollout resume deploy hello-deploy</span></code></pre></div>
<p>After we resume the process will continue from where it was paused,
meaning that the remaining set of replicas and pods will be scaled up to
the desired state, and the state of the deployment object will no longer
show that it is <code>DeploymentPaused</code>, but the state will be
completed, after all pods are up and running, in the meantime the old
<code>ReplicaSet</code> will be gradually decommissioned and all of its
pods with it, we will see in the next section how to actually rollback
to this very <code>ReplicaSet</code> that will be getting
decommissioned, which has the version 1.0 of the image.</p>
<h3 id="rollbacks">Rollbacks</h3>
<p>As you saw older <code>ReplicaSet</code> are wound down and no longer
manage Pods. However their configuration still exists on the cluster,
making them a great option for reverting to previous versions, The
process of a rollback is the opposite of of a rollout you wind the one
of the old <code>ReplicaSet</code> up while you wind the current one
down.</p>
<p>Imagine the situation where the current image for the deployment
object is updated to a new version, from 1 to 2, and that the deployment
object was updated and everything went smoothly, the old version 1 Pods
were decommissioned by the <code>ReplicaSet</code> and the new ones were
now in place, however as we know the old <code>ReplicaSet</code> is
still active, assuming our deployment configuration is configured to
hold at least 2 versions of the deployment history with history -
<code>revisionHistoryLimit</code>.</p>
<p>A rollout history can be obtained using the command
<code>kubectl rollout history deployment hello-deploy</code>, Revision 1
was the initial deploy, that used the 1.0 image, tag, Revision 2.0 is
the rolling update we just performed, The old <code>ReplicaSet</code>
are still active, for the old image version, meaning that we can easily
revert to those, which will in turn commission a new set of Pods with
the original version of the image 1.0. So if we call
<code>kubectl get rs</code>, we should at the very least see two
<code>ReplicaSet</code> for bound to the hello-deploy object.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>NAME                    DESIRED CURRENT READY AGE</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>hello-deploy-65cbc9474c 0       0       0     42m</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>hello-deploy-6f8677b5b  10      10      10    5m</span></code></pre></div>
<p>From this output we can see some useful info, like that the old
<code>ReplicaSet</code> has no pods that are currently active, that one
is the one which was deployed with the original image, version 1.0, and
the new one has, this one is the new one with the new image version 2.0.
Now what we need to do is simply commission the old one again, and
decommission the new one. This will take care of creating the new pods
and removing the old ones as necessary.</p>
<p><code>Note that rollback and update are in a way a similar thing, meaning that the process being followed when a rollback is done is the same as rollout, the difference is only meaningful for the person doing the process, the underlying Kubernetes infrastructure does not distingush between rollout and rollback, it just applies one set of ReplicaSet and decommissions the other.</code></p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl rollout undo deployment hello-deploy <span class="at">--to-revision</span><span class="op">=</span>1</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">deployment.apps</span> <span class="st">&quot;hello-deploy&quot;</span> rolled back</span></code></pre></div>
<p>This operation is not instant, remember that the rollback has to
provision the new (technically old) set of pods with the original image
1.0, and remove the new ones, however as we have already seen this is
not happening in an instant, as it is a gradual process of bringing up
the pods with the old version 1.0 of the image and removing the ones
with the new 2.0 version of the image.</p>
<h3 id="labels">Labels</h3>
<p>As we have already seen that <code>Deployments</code> and
<code>ReplicaSet</code> use labels and selectors to find Pods they own,
it was possible in earlier versions of kubernetes for deployments to
take over management of existing static pods if they had the same label,
however recent versions use the system generated pod template hash label
so only pods create by the <code>deployment/ReplicaSet</code> will be
managed. Assume a quick example you already have 5 pods on a cluster
with the label app=front-end. At a later date, you crate a deployment
that requests 10 pods with the same app=front-end label. Older versions
of Kubernetes would notice there were already 5 Pods with that label and
only create 5 new ones, and the <code>Deployment/ReplicaSet</code> will
manage all 10. However newer versions of Kubernetes tag all pods created
by a <code>deployment/ReplicaSet</code> with the pod-template-hash
label. This stops higher level controllers seizing ownership of existing
static pods.</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe deploy hello-deploy</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span> hello-deploy</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NewReplicaSet:</span> hello-deploy-5445f6dcbb</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe rs hello-deploy-5445f6dcbb</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span> hello-deploy-5445f6dcbb</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Selector:</span> app=hello-world,pod-template-hash=5445f6dcbb</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pods <span class="at">--show-labels</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                      READY STATUS  LABELS</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="ex">hello-deploy-5445f6dcbb..</span> 1/1   Running app=hello-world,pod-template-hash=5445f6dcbb</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="ex">hello-deploy-5445f6dcbb..</span> 1/1   Running app=hello-world,pod-template-hash=5445f6dcbb</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="ex">hello-deploy-5445f6dcbb..</span> 1/1   Running app=hello-world,pod-template-hash=5445f6dcbb</span></code></pre></div>
<p>So you can see how the different levels of objects actually are
linked together through the pod template hash, along with the label
selector</p>
<h3 id="skeleton">Skeleton</h3>
<p>The basic structure of the Deployment object is presented below, it
is crucial to understand that the Deployment object technically controls
many aspects of the underlying process of managing Pods, that includes
creating and destroying <code>ReplicaSet</code> and other object. To be
clear, the Deployment object is just that, an object, the actual
management happens at the kubelet level, which reads these
configurations and controls and manages the actual state of the Node, in
the cluster.</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-deploy</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">replicas</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">revisionHistoryLimit</span><span class="kw">:</span><span class="at"> </span><span class="dv">5</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">progressDeadlineSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">300</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">minReadySeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">strategy</span><span class="kw">:</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">type</span><span class="kw">:</span><span class="at"> RollingUpdate</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">rollingUpdate</span><span class="kw">:</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">maxUnavailable</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">maxSurge</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-pod</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nigelpoulton/k8sbook:1.0</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="kw">-</span><span class="at"> </span><span class="fu">containerPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="fu">limits</span><span class="kw">:</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="at">                          </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> 128Mi</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="at">                          </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.1</span></span></code></pre></div>
<ul>
<li><p><code>apiVersion</code>: At the top the API version is specified
that is to be used.</p></li>
<li><p><code>kind</code>: that is the type of the object that is being
defined, in this case the Deployment</p></li>
<li><p><code>metadata</code>: gives the Deployment a name, this should
be a valid DNS name, so, that means alphanumeric the dot and the dash
are valid, avoid exotic characters.</p></li>
<li><p><code>spec</code>: this section is where most of the action is,
anything directly below spec relates to the Deployment, anything nested
below refers to the actual behavior of the deployment object</p></li>
<li><p><code>spec.template</code> is the Pod template the Deployment
uses to stamp out the Pod replicas, in this example the Pod template
defines a single container Pod.</p></li>
<li><p><code>spec.replicas</code> is how many pod replicas the
deployment should create and manage.</p></li>
<li><p><code>spec.selector</code> is a list of labels that pods must
have in order for the deployment to manage them, notice how the
Deployment selector matches the labels assigned to the pod.</p></li>
<li><p><code>spec.revisionHistoryLimit</code> tells Kubernetes how may
older version of <code>ReplicaSet</code> to keep, keeping more gives you
more rollback options but keeping too many can bloat the object, this
can be a problem on large cluster with lots of software
releases.</p></li>
<li><p><code>spec.progressDeadlineSeconds</code> tells kubernetes how
long to wait during a rollout for each new replica to come online, the
example sets a 5 minute deadline, meaning that each new replica has 5
minutes to complete up before kubernetes considers the rollout stalled,
to be clear the clock is reset after each new replica comes up meaning
each step in the rollout gets its own 5 minute window.</p></li>
<li><p><code>spec.strategy</code> tells the deployment controller how to
update the pods when a rollout occurs. There are some more details to
take a look at here, first the <code>maxUnavailable</code> - which tells
that no more than one Pod below the desired state should considered
valid state, meaning that somehow two pods failed, getting us at 8, the
kubelet will try to scale up to 10. The <code>maxSurge</code> - which
means that we should not have more than one pod above the desired state,
i.e if somehow the deployments overshoot 10, i.e become 12, the
additional pods will be scaled down to match the desired state.</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to activate the deploy configuration</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> deploy.yml</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># to get a brief description of it</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get deploy hello-deploy</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># to get full details of the object</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe deploy hell-deploy</span></code></pre></div>
<h2 id="services-1">Services</h2>
<p>As we have already seen how pods are related containers, and then to
Deployments, we have seen the core levels of abstraction, starting off
from
<code>Containers -&gt; Pods -&gt; -&gt; ReplicaSet -&gt; Deployments</code>,
each of these provide different capabilities, the containers are what
provide a meaningful way to run images, the Pods are used to manage the
resources and namespace the containers, the <code>ReplicaSet</code> are
governing how to scale pods and containers, and the deployments are all
about self healing and overall control over everything else below.</p>
<p>There is a higher level of abstraction in the kubernetes world, and
these are called services. Services provide a reliable networking for a
set of unreliable Pods managed by Deployments, since pods and containers
effectively are immutable and ephemeral can be created and destroyed
without any notice, we need a way to abstract away the gazillion number
of Pods that might come into life or get destroyed, without having to
think about that process at all.</p>
<p>When a Pods fail they get replaced by a new one with new IP. Scaling
up introduces a new Pod with new IP addresses, scaling down removes
Pods. Rolling updates also replace existing Pods with completely new
ones with new IPs. This create a massive IP churn, and demonstrates why
you should never connect directly to any particular pod. You also need
to know 3 fundamental things about Kubernetes Services</p>
<ul>
<li><p>First when talking about Services, we are talking about Service
object in the Kubernetes world that provides a stable networking for
Pods. Just like a <code>Pod</code>, <code>ReplicaSet</code> and
<code>Deployment</code>, <code>Services</code> are defined through a
manifest YAML file, posted to the API server.</p></li>
<li><p>Second every Service gets its own stable IP address, its own
stable DNS name and its own stable port.</p></li>
<li><p>Third, Services use labels and selectors to dynamically select
the Pods to send traffic to.</p></li>
</ul>
<h3 id="theory-2">Theory</h3>
<p>With a service in place the Pods can scale up and down they can fail
and they can be updated and rolled back, and clients will continue to
access them without interruption. This is because the Service is
observing the changes and updating its list of healthy Pods. But it
never changes its stable IP, DNS and port</p>
<p>Think of services as having a static front end and a dynamic back end
the front end consisting of the IP, DNS name and port never change, The
back end comprising the list of healthy Pods can be constantly
changing.</p>
<h3 id="labels-1">Labels</h3>
<p>Services are loosely coupled with Pods via labels and selectors. This
is the same technology that loosely couples Deployments to Pods and is
key to the flexibility of Kubernetes. For the service to send traffic to
a give Pod, the Pod needs every label the Service is selecting on. It
can also have additional Labels the Service is not looking for. However
the
<code>Service might have multiple labels and the logic between those is AND</code>.
Therefore extra care is needed when configuring selection labels for the
pods</p>
<p><code>Services are Orthogonal to Deployments, they are not responsible for managing deployments, they like the deployment object work with pods, therefore the Services and Deployment objects are on the same "level" in the kubernetes hierarchy, just above the Pod, services do not control deployments, they work along side deployment objects to manage pods, deployments are responsible for managing the deployment process, while services are meant to manage traffic and abstract away the communication between the pod and the outside world</code></p>
<p>Take a look at these two definitions, one is for Service the other
for Deployment</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-svc</span><span class="co"> # &lt;&lt;- name of the service</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> NodePort</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">nodePort</span><span class="kw">:</span><span class="at"> </span><span class="dv">30001</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span><span class="co"> # &lt;&lt;- match and manage all pods with this label</span></span></code></pre></div>
<div class="sourceCode" id="cb17"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-deploy</span><span class="co"> # &lt;&lt;- name of the service</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">replicas</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span><span class="co"> # &lt;&lt;- ReplicaSet properties and options start here</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span><span class="co"> # &lt;&lt;- match and manage all pods with this label</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">revisionHistoryLimit</span><span class="kw">:</span><span class="at"> </span><span class="dv">5</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">progressDeadlineSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">300</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">minReadySeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">strategy</span><span class="kw">:</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">type</span><span class="kw">:</span><span class="at"> RollingUpdate</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">rollingUpdate</span><span class="kw">:</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">maxUnavailable</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">maxSurge</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">template</span><span class="kw">:</span><span class="co"> # &lt;&lt;- the pod definition starts here</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span><span class="co"> # &lt;&lt;- this is the label of the pod that the service and deployment will match</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">containers</span><span class="kw">:</span><span class="co"> # &lt;&lt;- tell the pod what image it has to use</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-pod</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nigelpoulton/k8sbook:1.0</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="kw">-</span><span class="at"> </span><span class="fu">containerPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="fu">limits</span><span class="kw">:</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="at">                          </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> 128Mi</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="at">                          </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.1</span></span></code></pre></div>
<p>Note that Deployments and Services both have to have correct
selectors configured to match against the labels of the Pods. While
deployments create the Pods through the use of the
<code>ReplicaSets</code>, deployments are not managed by Services,
services manage pods just like deployments do through the
<code>ReplicaSets</code> objects. It might seem a bit odd, since the pod
is defined only in the Deployment, and does not exist as a standalone
“object” in the Kubernetes world</p>
<p>One might have noticed that the Service uses a different definition
for the selector than the Deployments object, this is because the
Services selector is simpler, the selector of services only provides a
way for simple matching, there is no way to do advanced expression
matching like the one of the Deployment object, which is the
<code>selector.matchLabels</code> or
<code>selector.matchExpression</code></p>
<p>Here are two examples of how Deployment can use the advanced selector
section, which is not available for Services, it can match either on
simple labels, just like Services or on more advanced expression
rules</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Deployment spec</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">app</span><span class="kw">:</span><span class="at"> my-app</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">matchExpressions</span><span class="kw">:</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">key</span><span class="kw">:</span><span class="at"> app</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">operator</span><span class="kw">:</span><span class="at"> In</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">values</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;my-app&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;test-app&quot;</span><span class="kw">]</span></span></code></pre></div>
<p>The service on the other hand is rather simple, it just matches on
one or more labels with AND condition</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Service spec</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> my-app</span></span></code></pre></div>
<h3 id="endpoints">Endpoints</h3>
<p>As pods come and go, the Service dynamically updates its list of
healthy matching Pods. It does this through a combination of label
selection and a construct called and Endpoint object. Every time you
create a Service, Kubernetes automatically creates and associated
Endpoint object. The endpoints object is used to store a dynamic list of
healthy pods matching the service’s label selector</p>
<p>Kubernetes constantly is evaluating the Service label selector
against the healthy Pods on the cluster, as new pods that match the
selector get added to the endpoints object whereas any pods that
disappear get removed this means that the endpoints object is always up
to date.</p>
<p>When sending traffic to pods via a service the cluster’s internal DNS
resolves to the service name to an IP address. It then sends the traffic
to this stable IP address and the traffic gets routed to one of the Pods
in the endpoints list, However a Kubernetes native application, can
query the endpoint API directly bypassing the DNS lookup and use the
service’s IP address</p>
<p>Accessing Services from inside the cluster, there are several service
types, the default one is called <code>ClusterIP</code>. A
<code>ClusterIP</code> service has a stable virtual IP address that s
only accessible from inside the cluster, we call this a
<code>ClusterIP</code> it is programmed into the network fabric and
guaranteed to be stable for the life of the service, programmed into the
network fabric is a fancy way of saying that the network just knows
about it and you do not need to bother with the details.</p>
<p>The <code>ClusterIP</code> is registered against the name of the
service in the cluster internal DNS service, all pods in the cluster a
are pre-configured to use the cluster DNS service meaning all pods can
convert service names to <code>ClusterIP</code></p>
<p>That means that if we create a new Service, called magic-sandbox will
dynamically assign a stable <code>ClusterIP</code>. This name and the
<code>ClusterIP</code> are automatically registered with the cluster’s
DNS service, These are all guaranteed to be long lived and stable. As
all pods in the cluster send service discovery requests to the internal
DNS they can all resolve magic-sandbox to the actual IP, based on the
<code>ClusterIP</code>. <code>Iptables</code> or <code>IPVS</code> rules
are distributed across the cluster to ensure traffic sent to the
<code>ClusterIP</code> gets routed to matching Pods. Net result if a
Pods knows the name of a Service it can resolve that to a
<code>ClusterIP</code> address and connect to the Pods behind it. This
only works for Pods and other objects on the cluster as it requires
access to the cluster’s DNS service, It does not work outside the
cluster</p>
<p>Lets have a simple example. Imagine a service named <code>one</code>
and another one named <code>two</code>, active pods are running for
both, we are located in a pod in service <code>one</code>, how would
like to make a call to a Pod in service <code>two</code>. Here’s how a
<code>curl</code> request would look like from a Pod in Service
<code>one</code> to a Pod in Service <code>two</code>:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that the HOSTNAME of the service two, includes the name of the service, as already established above, that is normal,</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># then the namespace under which this service is deployed, by default that is the `default` namespace, and then the cluster</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># name suffix, that is configured in the `CoreDNS` server that kubernetes is using as implementation of the DNS service</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> http://two.default.svc.cluster.local</span></code></pre></div>
<p>What are the exact elements of this FQDN specified in the curl
request:</p>
<ul>
<li><p><code>two</code>: The name of the Service to call.</p></li>
<li><p><code>default</code>: The namespace where the Service
<code>two</code> is deployed. If the Service is in a different
namespace, replace default with that namespace name.</p></li>
<li><p><code>svc.cluster.local</code>: The default domain for Services
in Kubernetes, The <code>svc.cluster.local</code> domain is the default
DNS suffix for Services in Kubernetes. It is defined in the
<code>CoreDNS</code> or <code>kube-dns</code> configuration. The
configuration is typically stored in a
<strong><code>ConfigMap</code></strong> named <code>coredns</code> (or
<code>kube-dns</code> in older clusters) in the <code>kube-system</code>
namespace.</p></li>
</ul>
<p><code>Services uses a special auxiliary EndpointSlices object internally, to manage the endpoints for the pods the service is responsible for and matches based on the selector labels</code></p>
<h3 id="types">Types</h3>
<p>Accessing Services from outside the cluster, Kubernetes has two types
of Services for requests originating from outside the cluster -
<code>NodePort</code> and <code>LoadBalancer</code></p>
<ul>
<li><code>NodePort</code> Services build on top of the
<code>ClusterIP</code> type and enable external access via a dedicated
port on every cluster node, we call this port the <code>NodePort</code>.
Since the default service type is <code>ClusterIP</code> and it
registers a DNS name virtual IP and port with the cluster’s DNS.
<code>NodePort</code> Services build on this by adding a
<code>NodePort</code> that can be used to reach the service from outside
the cluster. Below is a type of <code>NodePort</code> service</li>
</ul>
<p><code>NodePort service types are not so special, since while they would allow you to access a service from the outside world, there is no other way but to know the exact IP address of a node, you call directly the node, meaning that you always hit the same node from the cluster.</code></p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> hello-svc</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> NodePort</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">nodePort</span><span class="kw">:</span><span class="at"> </span><span class="dv">30001</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> hello-world</span></span></code></pre></div>
<p>Pods on the cluster can access this service by the name
magic-sandbox, on port 8080. Clients connecting from outside the cluster
can send traffic to any cluster node on port 30081.</p>
<ul>
<li><code>LoadBalancer</code> service types make external access even
easier by integrating with an internet facing load balancer, on your
underlying cloud platform, You get a high performance highly available
public IP or DNS name that you can access the service from, you can even
register friendly DNS names to make access even simpler, you do not need
to know the cluster node names or IP. <code>LoadBalancer</code> services
are tightly coupled with cloud providers. They may not work in
on-premises environments without additional configuration (e.g., using
<code>MetalLB</code>).</li>
</ul>
<p><code>LoadBalancer has the benefit that there is a load balancer service/server infront of the cluster nodes, and unlike the NodePort type, we do not hit a cluster node IP directly, we hit the IP or domain name of the load balancer, which would then route the traffic to one of the underlying nodes on the cluster, this gives us the benefit of first not caring about node IP addresses and balancing traffic</code></p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> my-loadbalancer-service</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span><span class="co"> # Port exposed by the load balancer</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">targetPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span><span class="co"> # Port on the Pods</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> my-app</span></span></code></pre></div>
<p>What happens if we have a node inside of which we have deployed
multiple pods that match the given service, would k8s load balance
between the pods within the node itself ? Yes, most often the used
algorithm is just simple round-robin, meaning that all pods on the given
node for a given matching service/selector will be hit sequentially, one
after the other, in a round robin style.</p>
<p><code>Note these Types of services are strictly for accessing the service from outside the cluster, service based pod - pod communication, is handled differently, and is explored in depth below</code></p>
<h3 id="registration">Registration</h3>
<p>Service registration is the process of an app posting its connection
details to a service registry so other apps can find it and consume it,
a few important things to note bout service discovery in k8s -
Kubernetes uses its internal DNS as a service registry, All k8s service
automatically register their details with the DNS.</p>
<p>For this to work, k8s provides a well known internal DNS service that
we usually call the cluster DNS. It is well known because every pod in
the cluster knows where to find it, it is implemented in the kube-system
namespace as a set of Pods managed by a Deployment called
<code>coredns</code>. These pods are fronted by a Service called
kube-dns. Behind the scenes it is based on a DNS technology called
<code>CoreDNS</code>, and runs as a k8s native app.</p>
<p>The actual registration is divided in two parts - we can call them
front and back end, briefly this is what is going on:</p>
<ul>
<li><p><code>The front end</code> - that is the actual API server
receiving the request to deploy the service on the cluster, there are
certain steps (see below) that happen here, like registering the service
IP in the cluster DNS creating the Service object, and other auxiliary
objects</p></li>
<li><p><code>The back end</code> - this is all work that needs to be
done on the actual Node that runs the Pods through the selector
metadata. This is for example configuring
<code>iptables or IPVS rules</code> on the actual nodes</p></li>
</ul>
<div class="sourceCode" id="cb23"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to list the actual pods which are running the coredns deployment</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pods <span class="at">-n</span> kube-syste- <span class="at">-l</span> k8s-app=kube-dns</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                     READY STATUS  RESTARTS AGE</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="ex">coredns-5644d7b6d9-fk4c9</span> 1/1   Running 0        28d</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="ex">coredns-5644d7b6d9-s5zlr</span> 1/1   Running 0        28d</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to list the actual deployment object that is managing these pods</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get deploy <span class="at">-n</span> kube-system <span class="at">-l</span> k8s-app=kube-dns</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>    READY UP-TO-DATE AVAILABLE AGE</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="ex">coredns</span> 2/2   2          2         28d</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get svc <span class="at">-n</span> kube-system <span class="at">-l</span> k8s-app=kube-dns</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>     TYPE      CLUSTER-IP     EXTERNAL-IP PORT<span class="er">(</span><span class="ex">S</span><span class="kw">)</span>                <span class="ex">AGE</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-dns</span> ClusterIP 192.168.200.10 <span class="op">&lt;</span>none<span class="op">&gt;</span>      53/UDP,53/TCP,9153/TCP 28d</span></code></pre></div>
<p>So how does the process of service registration work:</p>
<ol type="1">
<li>You post a new service manifest to the API server.</li>
<li>The request is authenticated authorized and subject to admission
policies</li>
<li>The service is allocated a stable virtual IP address called
<code>ClusterIP</code></li>
<li>An endpoints object i.e <code>EndpointSlices</code> is created to
hold a list of healthy pods matching the service label selector</li>
<li>The pod network is configured to handle traffic sent to the
<code>ClusterIP</code></li>
<li>The service name and IP are registered with the cluster DNS
service</li>
</ol>
<p>The step 6 is the secret sauce, we mentioned earlier that the cluster
DNS is a kubernetes native app. This means it knows it is running on k8s
and implements a controller that watches the API server for new Service
objects, any time it observes one it automatically creates the DNS
records mapping and links the service name to its
<code>ClusterIP</code>. This means apps and even services do not need to
perform their own service registration the cluster DNS does it for them
it is important to understand that the name registered in the DNS for
the service is the value stored in its <code>metdata.name</code>
property, this is why it is important that service names are a valid DNS
names and do not include exotic characters, The <code>ClusterIP</code>
is dynamically assigned by K8s</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> valid-dns-name-goes-here</span><span class="co"> # this is the secret sauce and should be a valid dns name</span></span></code></pre></div>
<p>Now that the service front end is registered and can be discovered by
other apps, the back end needs building so there is something to send
traffic to, this involves maintaining a list of healthy pod IPs the
service will load balance traffic to. As explained earlier every service
has a label selector that determines which pods it will load balance and
manage traffic to. To help with backend operations such as knowing which
pods to send traffic to and how traffic is routed k8s builds and
endpoint object - <code>EndpointSlices</code> for every Service</p>
<p>The kubelet agent on every node is watching the API server for new
<code>EndpointSlices</code> objects. When it sees one it creates local
networking rules to redirect <code>ClusterIP</code> traffic to pod IP in
modern k8s clusters the technology used to create these rules is the
Linux IP virtual server (<code>IPVS</code>). Older version used
<code>iptables</code>.</p>
<p>At this point the service is fully registered, and ready to be used.
Next is the active phase which is the service discovery this happens
when an actual application wants to actually connect to another Service
running in the cluster.</p>
<h3 id="discovery">Discovery</h3>
<p>Service discovery is quite an important topic, but to be brief
kubernetes uses DNS service, Kubernetes clusters run an internal DNS
service that is the center of service discovery, service names are
automatically registered with the DNS service on the cluster, every Pod
and container is pre-configured to use the cluster DNS
(e.g. /etc/resolv.conf). This means every Pod or container can resolve
every Service name to a <code>ClusterIP</code> and connect to the Pods
behind it.</p>
<p>The alternative form of service discovery is through environment
variables every pod gets a set of environment variables that resolve
Services currently on the cluster. This is extremely limited, they can
not learn about new services added after the Pod they are in was
created. This is a major reason DNS is the preferred method.</p>
<p>Let us assume there are two microservice apps on the same K8s cluster
- called <code>enterprise</code> and <code>cerritos</code>. The Pods for
enterprise sit behind a Service called <code>ent</code>, and the Pods
for <code>cerritos</code> sit behind another Service called
<code>cer</code>. They are being assigned <code>ClusterIP</code>, which
are registered with the cluster DNS service, and things are as
follows</p>
<table>
<thead>
<tr class="header">
<th>App</th>
<th>Service name</th>
<th>ClusterIP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Enterprise</td>
<td>ent</td>
<td>192.168.201.240</td>
</tr>
<tr class="even">
<td>Cerritos</td>
<td>cer</td>
<td>192.168.200.217</td>
</tr>
</tbody>
</table>
<p>For service discovery to work apps need to know both of the
following:</p>
<ul>
<li>The name of the other app they want to connect to - that is the name
of service fronting the pods</li>
<li>How to convert the name of the Service to an IP address that
corresponds to a Pod managed by the Service</li>
</ul>
<p>Apps developers are responsible for point 1, which is normal, They
need to code apps with the names of other apps they want to consume,
Actually they need to code the names of Services fronting the remote
apps, or in other words the pods running the apps. K8s takes care of the
second part.</p>
<p>Converting the names to IP addresses using the cluster DNS, happens
by k8s, automatically configuring that in every container so it can find
and use the cluster DNS to convert service names to IPs. It does this by
populating every container’s <code>/etc/resolv.conf</code> file with the
IP address of the CLUSTER DNS service, as well any search domains that
should be appended to unqualified names</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> cat /etc/resolv.conf</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="ex">search</span> svc.cluster.local cluster.local default.svc.cluster.local</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="ex">nameserver</span> 192.168.200.10</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="ex">options</span> ndots:5</span></code></pre></div>
<p>Let us explore a little side tangent. What is the structure of the
<code>resolv.conf</code> file, and here is some basic theory:</p>
<p>What is an unqualified name ? That is a short name such as
<code>ent</code>. Appending a search domain converts it to a fully
qualified domain name (FQDN) such as
<code>ent.default.svc.cluster.local</code>. The following snippet shows
a container that is configured to send DNS queries to the cluster DNS at
192.168.200.10. It also lists three search domains to append to
unqualified names.</p>
<p><strong>Fully Qualified Domain Name (FQDN):</strong> A hostname that
ends with a dot (e.g., <code>host.</code>) is considered an FQDN. This
means the DNS resolver will treat it as a complete domain name and
<strong>will not append any search domains</strong>.</p>
<p><strong>Unqualified Hostname:</strong> A hostname without a trailing
dot (e.g., <code>host</code>) is considered unqualified. In this case,
the DNS resolver will append search domains (if configured) to attempt
resolution.</p>
<ul>
<li><p><code>nameserver</code> - well that is pretty self explanatory,
this is pointing at the IP address of the cluster DNS service, this is a
must have in order to resolve the Service names, otherwise there is no
way for us to map the Service name to an actual <code>ClusterIP</code>,
and eventually to an actual <code>EndpointSlices</code> and to a
physical Pod IP address</p></li>
<li><p><code>search</code> - this one is a bit more complex, first we
have to understand what an FQDN is, those are domain names that end with
a dot <code>.</code>, usually the dot is omitted in most cases but
according to the spec a fully qualified domain name is only the one that
ends with a dot, if it does not it is not FQDN by omission, meaning that
if we use the following hostname in our app configuration
<code>ent</code> to refer to the enterprise service, this would be seen
as non FQDN, therefore according to the <code>resolv.conf</code> it will
try to resolve the host as follows
<code>ent.svc.cluster.local, ent.cluster.local and ent.default.svc.cluster.local</code>,
in that order it will try each and every one of those host names against
the DNS cluster <code>nameserver</code>, if the DNS cluster returns a
valid IP for the host we are good to go. Now what if our app
configuration was actually providing the host as <code>ent.</code> note
the <code>.</code> at the end, well in that case the search config will
be ignored, as it will consider this a FQDN and directly try to resolve
<code>ent.</code> ip address from the cluster DNS service, which will
fail.</p></li>
<li><p><code>options</code> - directive allows you to configure
additional resolver behavior. In our example,
<code>options ndots:5</code> specifies a threshold for the number of
dots (<code>.</code>) in a hostname before the resolver treats it as a
fully qualified domain name (FQDN).</p></li>
</ul>
<p>So what is the process, how and by whom is the
<code>resolv.conf</code> file actually get interpreted, in order to
obtain the actual IP address to establish TCP connection to</p>
<ol type="1">
<li><strong>Application:</strong> Your app calls
<code>getaddrinfo("example.com")</code> internally, take
<code>curl</code> as simple example</li>
<li><strong>Resolver Library:</strong> The resolver library (glibc)
reads <code>/etc/resolv.conf</code> to determine the nameserver and
search domains.</li>
<li><strong>DNS Query:</strong> The resolver library sends a DNS query
to the specified nameserver.</li>
<li><strong>Kernel:</strong> The kernel handles the network
communication (e.g., sending TCP packets to the DNS server).</li>
<li><strong>Response:</strong> The resolver library processes the DNS
response and returns the <code>ClusterIP</code> address of the service
to your app.</li>
</ol>
<p>This is however not the end, having the <code>ClusterIP</code>, does
not help much since it is on a different network, if we want our Pod for
service <code>ent</code> to talk to another Pod for service
<code>cer</code>, we need to know the IP of at least one Pod from the
<code>cer</code> service, how does that happen. Enter
<code>Subnet masks, Gateways, IPVS and iptables</code>…</p>
<h3 id="network-magic">Network magic</h3>
<p><code>ClusterIP</code> are on a special network called the
<code>service network</code> and there are no routes to it, this means
containers send all <code>ClusterIP</code> traffic to their default
gateway. In this case a default gateway is where devices send traffic
when there is no known route, normally the default gateway forwards
traffic to another device with a larger routing table in hope it will
have a route to the destination network.</p>
<p>The way this works, is that as we know each device on a network has a
local routing table, this routing table tells it how to route outgoing
traffic, i.e to which gateway to send the traffic, when the destination
network is remote, when it is local the traffic sent directly. So here
is a brief overview of the communication process</p>
<ul>
<li><strong>Your device</strong>: <code>192.168.1.10</code></li>
<li><strong>Local network</strong>: <code>192.168.0.0</code></li>
<li><strong>Default gateway</strong>: <code>192.168.1.1</code></li>
<li><strong>Destination</strong>: <code>www.google.com</code> (let’s say
its IP is <code>142.250.190.78</code>)</li>
</ul>
<table style="width:100%;">
<colgroup>
<col style="width: 26%" />
<col style="width: 18%" />
<col style="width: 18%" />
<col style="width: 12%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th>Destination Network</th>
<th>Subnet Mask</th>
<th>Gateway</th>
<th>Interface</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>192.168.1.0</td>
<td>255.255.255.0</td>
<td>0.0.0.0</td>
<td>eth0</td>
<td>(Local Network)</td>
</tr>
<tr class="even">
<td>172.16.0.0</td>
<td>255.240.0.0</td>
<td>192.168.1.253</td>
<td>eth0</td>
<td>(Remote Network)</td>
</tr>
<tr class="odd">
<td>10.0.0.0</td>
<td>255.0.0.0</td>
<td>192.168.1.254</td>
<td>eth0</td>
<td>(Remote Network)</td>
</tr>
<tr class="even">
<td>0.0.0.0</td>
<td>0.0.0.0</td>
<td>192.168.1.1</td>
<td>eth0</td>
<td>(Default Gateway)</td>
</tr>
</tbody>
</table>
<p>Now providing a bit more detail, so when your device wants to
transmit traffic it needs to know where to send it to, this is done by
using the subnet mask, the subnet mask is used to determine which
network a given IP belongs to, using the routing table, your device gets
the IP - <code>142.250.190.78</code> and applies the subnet masks from
the routing table, top to bottom, first it checks against
<code>255.255.255.0</code>, the resulting network from the masking the
destination IP is <code>142.250.190.0</code>, it checks to see
<code>if the destination network produced for this subnet mask in the table matches</code>,
in this case it does not, the destination network for this mask
<code>255.255.255.0</code> says <code>192.168.1.0</code>, no match, move
to the next entry in the table. Now if we take a close look we will see
that none of the destination networks, match, after the mask is applied
the only one which does match, is the last one with a mask of
<code>0.0.0.0</code>, it always matches, that is the default gateway,
which is usually the last entry in the table, when no other subnet mask
matches, that is our last hope, send the traffic to the default gateway,
in this case <code>192.168.1.1</code>. So here is how the process
usually goes:</p>
<ol type="1">
<li><p>Your device wants to send data to
<code>142.250.190.78</code>.</p></li>
<li><p>It checks its local routing table and sees that
<code>142.250.190.78</code> is not in the local network
(<code>192.168.1.x</code>).</p></li>
<li><p>The subnet masking and routing table mapping says, “Send this to
the default gateway, we did not find any other match in the routing
table (<code>192.168.1.1</code>).”</p></li>
<li><p>The default gateway device) receives the data, checks its own
routing table, and forwards it to the internet, or another gateway
device</p></li>
<li><p>The response comes back to the router, which sends it to your
device</p></li>
</ol>
<p><code>The key takeaway here is to realize that the subnet masking is a process that involves matching two pairs of values, from the table, when an IP is masked against a subnet mask, that produces a destination network address, that address has to match the destination network in the routing table, in order to consider the subnet masking a match, otherwise move to the next entry in the IP routing table</code></p>
<p>The container’s default gateway sends the traffic to the node it is
running on, the node does not have a route to the service network
either, so it sends it to its own default gateway, doing this causes the
traffic to be processed by the node’s kernel which is where the magic
happens, Every kubernetes node runs a system service called
<code>kube-proxy</code>, At a high level the proxy is responsible for
capturing traffic destined for <code>ClusterIP</code> and redirecting it
to the IP addresses of Pods matching the Service’s label selector.
<code>kube-proxy</code> is running on the node and is Pod based
kubernetes native app, that implements a controller watching the API
server for new Service and Endpoint objects - in other words this boils
down to Pods being created and destroyed, on other Nodes. When it sees
them it creates local <code>IPVS</code> rules telling the Node how to
intercept traffic destined for these Service’s <code>ClusterIP</code>
and by “proxy” to the Pods being managed by these Service/Endpoint
objects and forward it to actual Pod IPs.</p>
<p>This means that every time a Node’s kernel processes traffic headed
for an address on the Service Network a trap occurs and the traffic is
redirected to the IP of a healthy pod matching the service’s label
selector. Meaning the Node itself becomes its own load balancer for
traffic between other nodes, and pods on these nodes.
<code>There is NO separate intermediate physical mediator service network which is responsible for the traffic,</code>
instead the network model between the pods is FLAT, meaning nodes on a
cluster communicate directly with each other, and the IP mapping happens
thanks to kube-proxy running on the Pod which dynamically configures the
<code>iptables</code> or in newer versions of k8s <code>IPVS</code>
rules</p>
<p>Kubernetes originally used <code>iptables</code> to do this trapping
and load-balancing. However it was replaced by <code>IPVS</code> in
kubernetes 1.11. This is because <code>IPVS</code> is a high performance
kernel based <code>L4 load balancer</code> that scales better than
<code>iptables</code> and implements better load balancing. The previous
implementation which was using <code>iptables</code>, was not well
suited, since <code>iptables</code> are meant to be used first and fore
most for firewall configurations not for load balancing traffic</p>
<p><code>Key takeaway, from everything said above, you will notice that the actual LOAD BALANCING between Pod to Pod communication, that is internally between pods on different nodes, happens by the Nodes themselves or rather by the IPVS which is the kernel level load balancer implementation in Linux</code></p>
<h3 id="network-traffic">Network Traffic</h3>
<p>So let us summarize the process, of how the entire network
communication between Pods on different Nodes works</p>
<ol type="1">
<li>Query the DNS with the service name for the stable
<code>ClusterIP</code> IP of the service</li>
<li>Receive the <code>ClusterIP</code> address</li>
<li>Send traffic to the <code>ClusterIP</code> address</li>
<li>No route, send to container’s default gateway</li>
<li>Forward to node</li>
<li>No route, send to the node’s default gateway</li>
<li>Processed by the Node’s kernel,</li>
<li>Trap by <code>ipvs</code> rule</li>
<li>Rewrite IP destination to field to IP</li>
</ol>
<p>Assume that using the example with the <code>enterprise</code> and
<code>cerritos</code> from above, the enterprise app is sending traffic
to <code>cerritos</code> app, first up it needs the host name of the
service running the <code>cerritos</code> apps, that would be
<code>cer</code> that is defined in the manifest of the Service as well
as the actual <code>enterprise</code> app configuration, done by the
developers themselves. An instance of the <code>enterprise</code> app,
read a container running on a node, tries to send traffic to the
<code>cer</code> service. But networks work with numbers not names or
strings. So the container hosting the <code>enterprise</code> app sends
the name <code>cer</code> to the pre configured DNS service on the
container (the nameserver in the <code>/etc/resolv.conf</code> file). It
is asking the DNS service to resolve this name <code>cer</code> to an
actual stable IP address. The DNS service returns a stable
<code>ClusterIP</code> address and the <code>enterprise</code> app sends
the traffic to that address. However <code>ClusterIP</code> are on a
special <code>service network</code>, they are not accessible from the
Nodes, and the container does not have a route to them. So it sends it
to the default gateway, which forwards it to the host Node. The Node
does not have a route either, so it sends it to its own default gateway.
However en-route, the request is processed and intercepted by the Node’s
Kernel. Now beforehand the <code>kube-proxy</code> service running on
the Node, has already been watching for new Endpoints i.e Pods, on the
cluster and already having configured the <code>IPVS</code> rules, it
knows all the Pods and their IPs matching the target Service label, the
kernel simply uses these rules to trigger a trap, and the request is
redirected to an IP address of a Pod that matches the Service label
selector, the <code>IPVS</code> implementation in the kernel as
mentioned is a load balancer, meaning that subsequent requests to the
same Service will hit other Pods, meaning the mapping will resolve to
other IPs of Pods matching the Service label, maybe on a round robin
principle, based on how the <code>IPVS</code> is configured, but that is
not that important</p>
<h3 id="namespaces">Namespaces</h3>
<p>Every cluster has an address space and Kubernetes Namespaces
partition it. Cluster address spaces are based on a DNS domain that we
call the cluster domain. The domain name is usually cluster.local and
objects have unique names within it. For example a service called
<code>ent</code> will have fully qualified name (FQDN) of
<code>ent.default.svc.cluster.local</code>. The format is
<code>&lt;object-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>.</p>
<p>Namespaces let you partition the address space below the cluster
domain level. For example creating a couple of Namespaces called
<code>dev</code> and <code>acc</code>, will give you two new address
spaces.</p>
<ul>
<li><code>dev</code>:
<code>&lt;object-name&gt;.dev.svc.cluster.local</code></li>
<li><code>acc</code>:<code>&lt;object-name&gt;.acc.svc.cluster.local</code></li>
</ul>
<p>Object names have to be unique within a Namespace, but not across
Namespaces, For example you can not have two Services named the same in
the same Namespace, but you can if they were to be in different
namespaces. This is useful for parallel development and production
configuration. Objects can connect to services, in the local Namespace
using short names such as simply specifying the
<code>&lt;object-name&gt;</code> (that is no magic, we already mentioned
above this is thanks to service discovery, the pre-configured
<code>/etc/resolv.conf</code> and all that network magic already
discussed above)</p>
<p>Imagine we had two services both in different namespaces both with
different names,
<code>svc1 in namespace dev and svc2 and in namespace acc</code>, we
want to hit them from a Pod called <code>svc3</code> in a third
namespace, <code>default</code> one</p>
<p>Here is a cool trick that will prove that the namespaces are nothing
really complex simply domain name context separators, imagine we have
the following <code>resolv.conf</code> file in our container</p>
<pre class="conf"><code>search svc.cluster.local cluster.local default.svc.cluster.local dev.svc.cluster.local acc.svc.cluster.local
nameserver 192.168.200.10</code></pre>
<p>We can still use the short names of these services, <code>svc1</code>
and <code>svc2</code>, without providing the FQDN, Why ? Well the
<code>resolv.conf</code> file will do the heavy lifting, it will take
the search config, and start attempting to send the host names to the
DNS cluster server, in order starting from i</p>
<ul>
<li><code>svc1.svc.cluster.local</code> - no hit, there is no such
service in the system/unnamed namespace</li>
<li><code>svc1.svc.default.svc.cluster.local</code> - no hit, there is
no such service in the default namespace</li>
<li><code>svc1.dev.svc.cluster.local</code> - we have a hit, since it is
deployed on the dev namespace that is its actual FQDN</li>
</ul>
<p>The DNS will resolve for the -
<code>svc1.dev.svc.cluster.local</code> - will return the IP address and
the rest will be handled by the <code>IPVS</code> rules and the kernel,
as we have already seen above.</p>
<p>Now, this is not something that one should or even can do, since the
<code>/etc/resolv.conf</code> file is not mutable, it is maintained and
managed by the k8s runtime and environment, even if we were to change it
manually, that would be on a per container instance, and will not
persist across Pod and container replication</p>
<p>The example above, was aiming to show, and link the different
networking concepts we have examined, and how they interlink, from the
moment the request is made by the app, using the unqualified domain
name, down to the actual IP address and traffic routing to the target
Pod.</p>
<h3 id="skeleton-1">Skeleton</h3>
<p>Below is the general skeleton of a service object, presented as a
YAML manifest file. There are a few interesting properties to take a
note of which are important to understand how the service integrates
with the running pods managed by a deployment object</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> svc-test</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">chapter</span><span class="kw">:</span><span class="at"> services</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> NodePort</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8080</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">nodePort</span><span class="kw">:</span><span class="at"> </span><span class="dv">30001</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">targetPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">9090</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">chapter</span><span class="kw">:</span><span class="at"> services</span></span></code></pre></div>
<ul>
<li><p><code>apiVersion</code>: At the top the API version is specified
that is to be used.</p></li>
<li><p><code>kind</code>: that is the type of the object that is being
defined, in this case the Service</p></li>
<li><p><code>metadata</code>: gives the Service a name, this should be a
valid DNS name, so, that means alphanumeric the dot and the dash are
valid, avoid exotic characters.</p></li>
<li><p><code>spec</code>: this section is where most of the action is,
anything directly below spec relates to the Service, anything nested
below refers to the actual behavior of the service object</p></li>
<li><p><code>spec.type</code>: In this case it is configured as
<code>NodePort</code> not a default <code>ClusterIP</code>, for the sake
of this example</p></li>
<li><p><code>spec.port</code>: this is the port on which the service
listens to</p></li>
<li><p><code>spec.targetPort</code>: this is the port on which the app
inside the container listens to</p></li>
<li><p><code>spec.nodePort</code>: this is the cluster wide port on
which the service can be accessed from the outside</p></li>
<li><p><code>spec.protocol</code>: by default, using TCP, but UDP for
example is also a probable option, based on the type of app</p></li>
</ul>
<div class="sourceCode" id="cb30"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to just deploy the service manifest file</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> svc.yml</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># to inspect the list of service</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get svc</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># to check on the details of specific service</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get svc svc-test</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span> TYPE CLUSTER-IP EXTERNAL-IP PORT<span class="er">(</span><span class="ex">S</span><span class="kw">)</span> <span class="ex">AGE</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="ex">hello-svc</span> NodePort 100.70.40.2 <span class="op">&lt;</span>none<span class="op">&gt;</span> 8080:30001/TCP 8s</span></code></pre></div>
<p>To inspect some of the created resources, alongside the service,
remember that endpoints are created per service, and they hold the
information about the active / alive pods and their actual IP addresses,
these will be used to route the traffic to the pods, when a service
hostname is hit</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to get the endpoint objects, which are tied to a service</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get endpointslices</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span> ADDRESSTYPE PORTS ENDPOINTS AGE</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="ex">svc-test-sbhbj</span> IPv4 8080 10.42.1.119,10.42.0.117,10.42.1.120... 6m38s</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># to get the details for the given endpoint object for the svc-test</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe endpointslices svc-test-sbhbj</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>        svc-test-sbhbj</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>   default</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>      endpointslice.kubernetes.io/managed-by=endpointslice-controller.k8s.io kubernetes.io/service-name=svc-test</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span> endpoints.kubernetes.io/last-change-trigger-time: 2021-02-05T20:01:31Z</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="ex">AddressType:</span> IPv4</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Ports:</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Name</span>         Port Protocol</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="ex">Endpoints:</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Addresses: 10.42.1.119</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Conditions:</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Ready:</span>     true</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Hostname:</span>  <span class="op">&lt;</span>unset<span class="op">&gt;</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="ex">TargetRef:</span> Pod/svc-test-84db6ff656-wd5w7</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="ex">Topology:</span>  kubernetes.io/hostname=k3d-gsk-book-server-0</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Addresses: 10.42.0.117</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="op">&lt;</span>Snip<span class="op">&gt;</span></span></code></pre></div>
<p>Take a note at the <code>Endpoints:</code> section, which describes
in detail all (output abbreviated) Pods and their IP addresses, they
also provide status information about the Pod. Similarly to how
<code>ReplicaSet</code> are helper objects to the
<code>Deployments</code> the <code>Endpoints</code> are helper objects
for the <code>Services</code>.</p>
<p>If we were to change the type of this service which in the manifest
above is of type <code>NodePort</code>, to a <code>LoadBalancer</code>,
one simply needs to change the configuration slightly to
<code>type: LoadBalancer</code> and remove the config for
<code>nodePort: 30001</code>, the rest will be
<code>automagically</code> done by the cloud provider, internally
Kubernetes will interface with the cloud provider’s internal load
balancer, and setup the required configurations to deploy and make the
service accessible over the load balancers’ host name (which is managed
and owned by the cloud provider directly). This can also be done in an
on premise location, but the load balancer setup is something that we
would have to do manually, for example using something like
<code>MetalLB</code>, which has a native integration with
Kubernetes.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to inspect the state of the service in a continuous loop, and see when the external-ip is assigned, use the --watch arg</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># usually the external ip will take some time to get populated since there is some non trivial setup to be done when the</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># new load balancer instance is setup by the cloud provider</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get svc <span class="at">--watch</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>     TYPE         CLUSTER-IP    EXTERNAL-IP PORT<span class="er">(</span><span class="ex">S</span><span class="kw">)</span>        <span class="ex">AGE</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="ex">svc-test</span> LoadBalancer 10.43.128.113 172.21.0.4  9000:32688/TCP 47s</span></code></pre></div>
<p>The external IP columns shows the public address of the service
assigned to by your cloud provider. On some cloud platforms this might
be a DNS name, instead of an IP, and it may take a minute to let the
setup complete.</p>
<p>Note that it is very much the case that the cloud provider would
create a separate load balancer service instance per service, that is
configured with <code>LoadBalancer</code>, this is to provide maximum
isolation from the other services, which provides better encapsulation.
Each load balancer server instance will be configured to connect to the
underlying Service. This however has some (actually a lot of issues and
drawbacks) in the next section, these drawbacks will be addressed using
another core Kubernetes object called <code>Ingress</code></p>
<h2 id="ingress">Ingress</h2>
<p>Ingress is all about accessing multiple web applications through a
single <code>LoadBalancer</code> service. A working knowledge of
Kubernetes Services is recommended before reading forward. We have
already seen how Service objects provide a stable networking for Pods.
You also saw how to expose apps to external consumers via a
<code>NodePort</code> services and <code>LoadBalancer</code> Services.
However both of these have limitations. <code>NodePort</code> only work
on high port numbers - 30000 - 32767 and require knowledge of node names
or IPs <code>LoadBalancer</code> Services fix this but require a 1-to-1
mapping between an internal Service and a cloud load balancer. This
means a cluster with 25 internet facing, apps will need 25 cloud load
balancers, and cloud load balancers are not cheap. They may also be a
finite resource you may be limited to how many cloud load balancer
instances you can provision, regardless of how much money you are ready
to pay them !</p>
<p>Ingress fixes this by exposing multiple Service through a single
cloud load balancer, it creates a <code>LoadBalancer</code> Service, on
port
<code>80 or 443 and uses host based and path based routing to send traffic to the correct backend Service</code>.</p>
<h3 id="theory-3">Theory</h3>
<p>Ingress is a stable resource in the Kubernetes API. It went
<code>general availability in Kubernetes 1.19</code> after being in beta
for over 15 releases. During the 3+ years it was in alpha and beta,
service meshes increased in popularity and there is some overlap in
functionality, as a result if you plan to run a service mesh you may not
need ingress. Ingress is defined in the <code>networking.io</code> API
sub group as a <code>v1</code> object and is based on the usual two
constructs:</p>
<ul>
<li>A controller - running in a reconciliation loop, to handle the
state</li>
<li>An object spec - a well defined and versioned manifest
specification</li>
</ul>
<p>The object spec defined rules that govern traffic routing and the
controller implements the rules. However a lot of Kubernetes clusters do
not ship with a built in ingress controller you have to install your
own. This is the opposite of other API resources, such as
<code>Deployments</code> and <code>ReplicaSets</code>, which have a
built in pre-configured controller. However some hosted Kubernetes
clusters such as <code>GKE</code> (Google Kubernetes Engine) have
installed one. Once you have an Ingress controller you deploy Ingress
objects with rules that govern how traffic hitting the Ingress is
routed</p>
<p>On the topic of routing, Ingress operates at a layer 7 of the OSI
model, also known as the app layer. This means it has awareness of HTTPS
headers, and can inspect them and forward traffic based on the hostnames
and paths, The following table shows how hostnames and paths can route
to backend <code>ClusterIP</code> Services.</p>
<table>
<thead>
<tr class="header">
<th>Host-based</th>
<th>Path-based</th>
<th>Backend K8s Service</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>shield.mcu.com</td>
<td>mcu.com/shield</td>
<td>svc-shield</td>
</tr>
<tr class="even">
<td>hydra.mcu.com</td>
<td>mcu.com/hydra</td>
<td>svc-hydra</td>
</tr>
</tbody>
</table>
<p>This shows how two different hostnames, configured to hit the same
load balancer, an ingress object is watching and uses the hostnames in
the HTTPS headers to route traffic to the appropriate backend service.
This is an example of the HTTP host based routing pattern, and it is
almost identical for path based routing</p>
<p><code>For this to wrok name resolution needs to point to the appropriate DNS names to the public endpoint of the Ingress load balancer</code></p>
<p>A quick side note, The OSI model is the reference model for modern
networking, it comprises seven layers, numbered from 1-7, with the
lowest layer concerned with things like signaling and electronics -
hardware, the middle layers dealing with reliability through things like
<code>acks</code> and retries and the higher layers adding awareness of
user apps such as HTTPS services, Ingress operates at a layer 7, also
known as the app layer and implements HTTP intelligence</p>
<p><code>Ingress exposes multiple ClusterIP Services through a single cloud load balancer, you create and deploy ingress objects, which are rules governing how traffic reaching the load balancer is routed to the backend services, the ingress controller which you usually have to install yourself uses hostnames and paths to make intelligent routing decisions.</code></p>
<p>So from 40k feet, what is going on on a high level is that the
ingress controller is actually exposed as an actual Service object in
the k8s environment, unlike other controllers, which are not, the
Ingress controller is exposed through a load balancer type Service, to
the public internet, then that service is actually backed by pods with
the NGINX image under the hood, which actually does the active on demand
routing which is defined in the Ingress object.</p>
<p>Common Ingress Controllers include different implementations, but
amongst the most popular options are:</p>
<ul>
<li><p><strong>NGINX</strong></p></li>
<li><p><strong>HAProxy</strong></p></li>
<li><p><strong>AWS ALB</strong></p></li>
<li><p><strong>Traefik</strong></p></li>
<li><p>The Ingress Controller is typically deployed as a
<strong>Kubernetes Deployment</strong> or <strong>DaemonSet</strong> and
runs as a pod in the cluster.</p>
<ol type="1">
<li><p><strong>Ingress Controller Deployment:</strong></p>
<ul>
<li>The Ingress Controller is deployed as a pod (or multiple pods) in
the cluster.</li>
<li>It listens for incoming traffic and routes it based on the Ingress
rules.</li>
</ul></li>
<li><p><strong>Service for the Ingress Controller:</strong></p>
<ul>
<li>A <strong>Service</strong> is created to expose the Ingress
Controller to external traffic.</li>
<li>The type of this Service can be:
<ul>
<li><strong>LoadBalancer</strong> (for cloud providers that support
external load balancers).</li>
<li><strong>NodePort</strong> (for exposing the Ingress Controller on
specific ports of the cluster nodes).</li>
<li><strong>ClusterIP</strong> (for internal-only access, though this is
less common for Ingress Controllers).</li>
</ul></li>
</ul></li>
<li><p><strong>Ingress Rules:</strong></p>
<ul>
<li>The Ingress resource defines rules for routing traffic to backend
services.</li>
<li>The Ingress Controller reads these rules and configures itself, its
running Pods (e.g., NGINX, Traefik) to route traffic accordingly.</li>
</ul></li>
</ol></li>
<li><p>What Makes the Ingress Controller Unique, from other controllers
?</p></li>
<li><p><strong>Backed by an Image:</strong> The Ingress Controller is
implemented as a custom application (e.g., NGINX, Traefik) running in a
container. This is different from most other Kubernetes controllers,
which are part of the Kubernetes control plane and are not exposed to
external traffic.</p></li>
<li><p><strong>Exposed to the Internet:</strong> The Ingress Controller
is typically exposed via a <strong>Service</strong> (e.g.,
<code>LoadBalancer</code> or <code>NodePort</code>) to handle external
HTTP/HTTPS traffic. This means it is directly accessible from outside
the cluster.</p></li>
<li><p><strong>Interfaces with External Traffic:</strong> Unlike other
k8s controllers, the Ingress Controller interacts directly with external
clients (e.g., web browsers, APIs) to route traffic to backend
services.</p></li>
</ul>
<div class="sourceCode" id="cb33"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>The Ingress controller is essentially a glorified service + pods that run a reverse proxy (like NGINX, Traefik, HAProxy,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>etc.). Its primary responsibility is to:</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>1.  Watch for Ingress resources: The Ingress controller monitors the Kubernetes API for changes to Ingress objects.</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>2.  Configure the reverse proxy: Based on the rules defined in the Ingress objects, the Ingress controller dynamically</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    configures the reverse proxy (e.g., NGINX) to route traffic to the appropriate backend services.</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>3.  Handle traffic routing: The reverse proxy (e.g., NGINX) then routes incoming traffic to the correct backend services</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    based on the configured rules.</span></code></pre></div>
<h3 id="network-traffic-1">Network traffic</h3>
<p>The network traffic from external parties or clients like browsers or
any other client consumer into the cluster targeted at the ingress
controller is first hitting the load balancer (assume that NGINX is the
used implementation) container. As we have already established, the way
the ingress controller is implemented is through k8s Services and Pods,
which use the preferred load balancer implementation/image.</p>
<p>Now here is the interesting part, as we have already seen above, the
usual Pod to Pod communication within a cluster happens with the help of
<code>IPVS</code> or <code>iptables</code>, depending on the
implementation/configuration. The <code>crucial part</code> here is to
understand that the ingress controller pods are also working in the
exact same way, one might think that for them the load balancing is
performed by the proxy alone, but that is not the case, even in this
case, the actual traffic is load balanced by the <code>IPVS</code> or
<code>iptables</code> configuration, the load balancer (NGINX) in our
case is only used to create the configurations and do the actual
routing.</p>
<p>In the ingress controller k8s object, defines the routing and mapping
that basically says which route should be mapped to which service, this
configuration is directly transferred to the underlying to the
underlying load balancer,in our example NGINX</p>
<p>So here is how it works, on a high level, the steps that a client
traffic request will go through to hit an underlying cluster pod</p>
<ol type="1">
<li><p>Client sends and HTTP request to the k8s cluster, given the host
name of the cluster</p></li>
<li><p>Load balancing ingress controller is deployed as a
<code>LoadBalancer</code> type Service</p></li>
<li><p>Ingress controller (NGINX) receive the request, and it matched
against its configured routing rules, from the Ingress object</p></li>
<li><p>Ingress controller in this case NGINX forwards, to the correct
K8s service</p></li>
<li><p><code>CoreDNS</code> resolves the name of that service and
returns the virtual <code>ClusterIP</code> address of the
service</p></li>
<li><p>Kube-proxy has already configured the <code>IPVS</code> or the
<code>iptable</code> rules for that service, since it runs on a
<code>watchloop</code></p></li>
<li><p>Kernel seeing the request made from the ingress controller pods,
matches the virtual <code>ClusterIP</code> of the service against the
actual physical Pod IP addresses</p></li>
<li><p>Traffic is sent over directly to one of these real physical IP
addresses, and a pod for the target service is hit</p></li>
</ol>
<p>Now looking at the flow above, you will immediately notice that
starting from step 5, the steps are exactly one to one the same steps
that a Pod to Pod would take to actually communicate traffic to another
Pod, this is because as we have already established the ingress
controller, and the ingress object, are special types of objects, which
in essence procure a normal native k8s service and k8s pod objects at
the end of the day, they are just like any other service and pod.</p>
<p>Key point to take into account here is namespaces, if the ingress
object manifest specifies a namespace, for the ingress controller that
implies that the routing mapping will be created against that namespace,
which means that only services from that namespace will be hit, in all
actuality what is going on is that simply the
<code>/etc/resolv.conf</code> file is configured with
<code>search</code> rule to only look in that target namespace, so it
will be capable of resolving the <code>/ClusterIP</code> addresses only
of hosts which are part of that namespace</p>
<h3 id="cluster-traffic">Cluster Traffic</h3>
<p>So how does that tie into the actual cluster, and the public
internet, how do we access the actual service from the public internet
on a given cluster. As we have already seen the ingress controller is
deployed as either a <code>NodePort</code> or <code>LoadBalancer</code>
service, usually a <code>LoadBalancer</code>.</p>
<p>A k8s cluster itself is not directly exposed to the public internet,
and certainly not through a single IP address. Instead individual
services are exposed and each service can have its own external IP or
hostname, Here is how the process works</p>
<p>When the NGINX controller is created and as we have already
established, the NGINX controller is a glorified method of creating a
<code>LoadBalancer</code> Service is created, the cloud provider
provisions a load balancer (e.g. for AWS - ELB, GCP Load Balancer). The
load balancer gets an external IP, or hostname, which you can use to
access the service. If the load balancer’s external IP is 203.0.133.10,
you can hit <a href="http://203.0.113.0"
class="uri">http://203.0.113.0</a> which will directly hit the exposed
load balancer service or in other words the ingress controller service
and the actual ingress controller pods.</p>
<p>Lets use an example, say that Multiple Users are on the Cloud
Platform, usually not each user will receive his own cluster, unless we
are speaking about enterprise customers, or generally high value users,
the cluster is shared between users, users however will have their
resources namespaced.</p>
<p>In a shared Kubernetes cluster, namespaces are the primary mechanism
for isolating resources between users or teams. Namespaces provide a
logical boundary for resources, ensuring that objects created by one
user or team do not interfere with those created by another. Let’s dive
into how namespaces work and how they are used to separate objects
between users.</p>
<ul>
<li><p>User A:</p></li>
<li><p>Creates a Kubernetes cluster and deploys an Ingress
controller.</p></li>
<li><p>The Ingress controller is exposed with an external IP
203.0.113.10.</p></li>
<li><p>Configures DNS to point <code>myapp.com</code> to
203.0.113.10.</p></li>
<li><p>User B:</p></li>
<li><p>Creates a separate Kubernetes cluster and deploys an Ingress
controller.</p></li>
<li><p>The Ingress controller is exposed with an external IP
203.0.113.20.</p></li>
<li><p>Configures DNS to point <code>myapi.com</code> to
203.0.113.20.</p></li>
<li><p>Traffic Flow:</p></li>
<li><p>When a client accesses <code>myapp.com</code>, DNS resolves it to
203.0.113.10, and the request is routed to User A’s cluster.</p></li>
<li><p>When a client accesses <code>myapi.com</code>, DNS resolves it to
203.0.113.20, and the request is routed to User B’s cluster.</p></li>
</ul>
<h3 id="skeleton-2">Skeleton</h3>
<p>The skeleton of the ingress manifest file is relatively simple,
however there are a few things to take a note of, first as all other k8s
objects, these can be namespaces as well, the service mapping will be
applied as already mentioned only to services in that namespace,
therefore you have to make sure that the service rules and the namespace
match and that namespace has a service with that name already
created</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> networking.k8s.io/v1</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Ingress</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> example-ingress</span><span class="co"> # the name of the ingress object</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> app-namespace</span><span class="co"> # the name of the specific namespace</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">host</span><span class="kw">:</span><span class="at"> example.com</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">http</span><span class="kw">:</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">paths</span><span class="kw">:</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">pathType</span><span class="kw">:</span><span class="at"> Prefix</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">backend</span><span class="kw">:</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">name</span><span class="kw">:</span><span class="at"> app-service</span><span class="co"> # only services which are in the app-namespace will be hit, so this service better be there</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">port</span><span class="kw">:</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="at">                                </span><span class="fu">number</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span></code></pre></div>
<p>Here is an example with the ingress controller configured to use TLS
and on top of that in the regular <code>pass through</code> mode, which
is basically making the ingress controller act as a dumb forwarder,
meaning that there is no way for the ingress controller to read the
request, therefore there is no way to configure sub path routing for a
service, meaning we can only rely on <code>SNI</code>, to resolve the
hostname</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> networking.k8s.io/v1</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Ingress</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> passthrough-ingress</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">nginx.ingress.kubernetes.io/ssl-passthrough</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tls</span><span class="kw">:</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">hosts</span><span class="kw">:</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> example.com</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> api.example.com</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">host</span><span class="kw">:</span><span class="at"> example.com</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">http</span><span class="kw">:</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">paths</span><span class="kw">:</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">backend</span><span class="kw">:</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">name</span><span class="kw">:</span><span class="at"> example-service</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">port</span><span class="kw">:</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="at">                                </span><span class="fu">number</span><span class="kw">:</span><span class="at"> </span><span class="dv">443</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">host</span><span class="kw">:</span><span class="at"> api.example.com</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">http</span><span class="kw">:</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">paths</span><span class="kw">:</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">backend</span><span class="kw">:</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">name</span><span class="kw">:</span><span class="at"> api-service</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">port</span><span class="kw">:</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a><span class="at">                                </span><span class="fu">number</span><span class="kw">:</span><span class="at"> </span><span class="dv">443</span></span></code></pre></div>
<p>A small side tangent, what is really <code>SNI</code> - server name
indication is an extension of the TLS protocol that allows a client to
specify the hostname it is trying to connect to during the initial
handshake. This is crucial for servers hosting multiple websites or
services on a single IP address, like our beloved k8s clusters and cloud
platform providers. So that is how that works, is it rather simple, when
you have encrypted traffic and no way to decrypt it, the
<code>SNI</code> springs into action, and the ingress/NGINX service is
able to determine at the very least the hostname, that is why route/path
based routing is not possible, because it is part of the headers, and
those are encrypted (so is the original host name information by the
way, were it not for <code>SNI</code> extension, we would not be able to
read the host name either), <code>SNI</code> does not help here</p>
<ol type="1">
<li><p>Client Hello: When a client initiates a TLS connection, it sends
a “Client Hello” message that includes the <code>SNI</code> extension,
specifying the hostname (e.g., example.com) it wants to access.</p></li>
<li><p>Server Selection: The server uses the <code>SNI</code>
information to select the correct SSL/TLS certificate for the requested
hostname.</p></li>
<li><p>Handshake Completion: The server responds with the appropriate
certificate, and the TLS handshake proceeds as usual, securing the
connection.</p></li>
</ol>
<p>Here is another example which in this case uses a re-encrypt
approach, meaning that it terminated the connection exactly at the
ingress controller pods, and the traffic down to the target services and
their pods is re-encrypted by a special certificate provided in the
configuration. This process is often called TLS edge termination, the
certificate to re-encrypt the traffic is called edge certificate</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> networking.k8s.io/v1</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Ingress</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> example-ingress</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">nginx.ingress.kubernetes.io/ssl-redirect</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tls</span><span class="kw">:</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">hosts</span><span class="kw">:</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> example.com</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">secretName</span><span class="kw">:</span><span class="at"> example-tls</span><span class="co"> # this is the certificate to be used for re-encrypting the traffic to the pod</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">host</span><span class="kw">:</span><span class="at"> example.com</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">http</span><span class="kw">:</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">paths</span><span class="kw">:</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /app</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">pathType</span><span class="kw">:</span><span class="at"> Prefix</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">backend</span><span class="kw">:</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">name</span><span class="kw">:</span><span class="at"> app-service</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">port</span><span class="kw">:</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="at">                                </span><span class="fu">number</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /api</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">pathType</span><span class="kw">:</span><span class="at"> Prefix</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">backend</span><span class="kw">:</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">name</span><span class="kw">:</span><span class="at"> api-service</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="at">                            </span><span class="fu">port</span><span class="kw">:</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="at">                                </span><span class="fu">number</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span></code></pre></div>
<h2 id="storage">Storage</h2>
<p>Storage is critical to most real world production apps, fortunately,
k8s has a mature and feature rich storage subsystem, called the
<code>persistent volume subsystem</code></p>
<p>K8s supports lots of storage back ends and each requires slightly
different configuration, The examples here are made to work with Google
k8s engine, clusters and will not work on other cluster types. The
principle and theory that you will learn is applicable to all types of
Kubernetes though</p>
<h3 id="the-big-picture">The big picture</h3>
<p>As mentioned k8s supports many storage providers, block, file and
object storage from a variety of external systems that can be in the
cloud or your on premise datacenters. However no matter what type of
storage or where it comes from when its exposed on k8s it is called a
volume. Azure File resources surfaced in k8s are called volumes as are
block devices from HPE. Modern volumes are based on the Container
Storage Interface (CSI) which is an open standard aimed at providing a
clean storage interface for container orchestrators such as k8s. Prior
to the CSI all storage plugins were implemented as part of the main k8s
code tree. This meant that they had to be open source and all updates
and bug fixes were tied to the main k8s release cycle. This was a
nightmare for developers &amp; maintainers. However now with the
existence of CSI storage vendors no longer need to open source their
code.</p>
<h3 id="the-storage-providers">The Storage Providers</h3>
<p>K8s uses storage from a wide range of external systems. These can
native cloud services such as AW Elastic Block Store, Azure File, but
can also be traditional on premise storage arrays providing NFS volumes
and such. Other options exist but the take home point is that k8s gets
its storage from a wide range of external systems including battle
hardened enterprise grade systems, from all the major data management
companies. Some obvious restrictions apply. For example you can not use
AWS storage services if your k8s service is running on Microsoft
Azure.</p>
<p>Each provider or provisioner needs CSI plugin to expose their storage
assets to k8s, the plugin usually runs as a set of Pods in the
kube-system Namespace.</p>
<h3 id="the-csi---container-storage-interface">The CSI - Container
Storage Interface</h3>
<p>The Container storage interface is a vital piece of the k8s storage
jigsaw and has been instrumental in bringing enterprise grade storage
from traditional vendors to k8s. However unless you are a developer
writing storage plugins you are unlikely to interact with it very often.
It is an open source project that defines a standards based interface so
that storage can be leveraged in an uniform way across multiple
container orchestrators. For example a storage vendor should be able to
write a single CSI plugin that works across multiple orchestrators such
as k8s and docker swarm. In practice k8s is the focus but docker is
implementing support for the CSI as well.</p>
<p>In the k8s world the CSI is the preferred way to write plugins,
drivers and means that plugin code no longer needs to exist in the main
k8s code tree. It also exposes a clean interface and hides all the ugly
volume machinery inside of the k8s code.</p>
<p>From a day to day perspective your main interaction with the CSI will
be referencing the appropriate CSI plugin in your YAML manifest files
and reading its documentation to find supported features and attributes.
Sometimes we call these plugins - “provisioners” especially when we talk
about storage classes later.</p>
<h3 id="the-persistent-volume-subsystem">The Persistent Volume
Subsystem</h3>
<p>This is where we will spend most of our time configuring and
interacting with storage. At a high level persistent volumes (PV) are
how external storage assets are represented in k8s, Persistent volume
claims (PVC) are like tickets that grant a Pod access to the persistent
volume, the Storage Classes (SC) make it all possible and dynamic.</p>
<p>Here is an example - assume that you have an external storage system
with two tiers of storage, - flash/ssd fast storage, and mechanical slow
archive storage (hard drives). You would expect apps on your k8s cluster
to use both, so you can crate two Storage Classes and map them as
follows:</p>
<table>
<thead>
<tr class="header">
<th>External tier</th>
<th>K8s Storage class name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SSD</td>
<td>sc-fast</td>
</tr>
<tr class="even">
<td>MECHANICAL</td>
<td>sc-slow</td>
</tr>
</tbody>
</table>
<p>With that Storage class in place apps can create volumes on the fly
by creating persistent volume claims (PVC) referencing either of the
storage classes. Each time this happens the CSI plugin referenced in the
SC instructs the external storage system to create an appropriate
storage asset. This is automatically mapped to a Persistent Volume (PV)
on K8s and the app uses the persistent volume claims or PVC to claim it
and mount it for use.</p>
<p>The section below tries to link all the components of the persistent
volume subsystem, as best as it can, giving an example with 3 different
persistent volume providers - <code>NFS, Ceph and AWS</code>, all of
those are simply storage providers, which allow us to store data of any
type, the underlying implementation is not important, what is important
to understand is that we need the components of the persistent volume
subsystem to be able to interact with them within our pods</p>
<h4 id="persistentvolume">PersistentVolume</h4>
<p>A PV is a piece of storage in the cluster that has been provisioned
by an administrator or dynamically provisioned using a
<code>StorageClass</code>. PVs represent the actual storage resources
available in the cluster, such as a disk on AWS EBS, a GCP Persistent
Disk, or an NFS share. PVs are cluster-wide resources and are not tied
to a specific namespace. They exist independently of Pods and PVCs.
Think of a PV as a physical hard drive or a network-attached storage
device.</p>
<p><code>The PV of persistent volume actually represents the physical storage devices that the cloud platform is exposing to the user, that would be things like hard drives, solid state drives, NAS (Network attached storage) and more. These are stting at the lowest level in the the persistent volume system hierarchy in the k8s environment</code></p>
<h4 id="storageclass">StorageClass</h4>
<p>A <code>StorageClass</code>, this is an abstraction on top of the
persistent volume, the <code>StorageClass</code> basically is
responsible for creating the link between the physical device /
persistent volume and the PVC and the Pods which would later on use
them. This happens through the use of plugins or provisioners, these
plugins are the custom vendor drivers which know how to interact with
the underlying persistent volume - for example Network File System (NFS)
or Ceph (a storage system that provides object, block and file storage).
These have to be configured externally of the cluster, the
<code>StorageClass</code> provides the means of providing an interface
to interact with them, through the plugin (provisioner), the actual
plugin has to be installed on a per use case basis, meaning based on
which type of persistent volume provider one would like to use. Think of
the <code>StorageClass</code> as the way to define the systems drivers
for persistent volumes, to enable interaction with them, these system
drivers are implemented by the plugin/provisioner which is defined in
the <code>StorageClass</code> object</p>
<p>So here is the definition that a systems administrator, or a user
would have to perform to setup the link between a persistent volume
system, like NFS, Ceph and so on, to allow the cluster and the pods
within to be configured to interact with those custom storage providers.
The example below is usually what a systems administrator on the cluster
would do, since most of the regular users would really rely on the
default <code>StorageClass</code> provided by the cloud platform, there
are cases where the users - large enterprise organizations might provide
their own <code>StorageClass</code> for more exotic persistent volume
vendors</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-storage</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> cluster.local/nfs-client</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">archiveOnDelete</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;false&quot;</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-rbd</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> rook-ceph.rbd.csi.ceph.com</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">clusterID</span><span class="kw">:</span><span class="at"> rook-ceph</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">pool</span><span class="kw">:</span><span class="at"> replicapool</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">imageFormat</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;2&quot;</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">imageFeatures</span><span class="kw">:</span><span class="at"> layering</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> ebs.csi.aws.com</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> gp2</span><span class="co"> # General Purpose SSD</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">encrypted</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span><span class="co"> # Optional: Enable encryption</span></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a><span class="fu">reclaimPolicy</span><span class="kw">:</span><span class="at"> Delete</span></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="fu">volumeBindingMode</span><span class="kw">:</span><span class="at"> WaitForFirstConsumer</span></span></code></pre></div>
<h4 id="persistentvolumeclaim">PersistentVolumeClaim</h4>
<p>A PVC is a request for storage by a user. It is a way for users to
ask for a specific amount of storage with certain characteristics (e.g.,
size, access mode). PVCs act as a “middleman” between Pods and PVs. They
allow users to request storage without needing to know the details of
the underlying storage infrastructure. PVCs are namespaced resources,
meaning they belong to a specific namespace. When a PVC is created,
Kubernetes binds it to a PV that matches the requested size and access
mode. Think of a PVC as a “ticket” that a user creates to request
storage. The ticket is then matched to an available physical storage
device or in other words a persistent volume (PV) - like a “hard
drive”.</p>
<p>So here are the definitions of the PVCs for the storage classes
mentioned above, as we can see the PVC reference the name of the storage
class, in this case we have 3 different ones - NFS, Ceph and AWS. These
provide the rules on how these storage classes and in particular the
actual persistent volumes will be accessed by the pods which require
access to them.</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-pvc</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteMany</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> nfs-storage</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-pvc</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> ceph-rbd</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs-pvc</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> aws-ebs</span></span></code></pre></div>
<h4 id="pod">Pod</h4>
<p>A Pod is the smallest deployable unit in Kubernetes. They can request
storage by referencing a PVC. Pods are ephemeral, meaning they can be
created, deleted, and rescheduled frequently. However, the data stored
in a PV (via a PVC) persists even if the Pod is deleted. It needs
storage to read/write data, which it gets by mounting a PVC.</p>
<p>Here is the final part of the puzzle, linking the PVC / claims with
actual pods, which will use these storage class and persistent volume
providers to mount inside their own environments, take a good look at
the <code>volumeMounts</code> and the <code>volumes</code> properties,
in the manifest below. The <code>volumes</code> links the pod with the
<code>pvc</code> the <code>volumeMounts</code> tells the pod where to
mount / what path to mount that <code>pvc</code> into the container.
That way we have just made the ephemeral pod, which has no context of
persistent storage, into something that can store data, now when if the
pods die and are re-created they will be able to retain a persistent
state</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-pod</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-container</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-storage</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /usr/share/nginx/html</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nfs-storage</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> nfs-pvc</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-pod</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-container</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-storage</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /usr/share/nginx/html</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ceph-storage</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> ceph-pvc</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs-pod</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs-container</span></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs-storage</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /usr/share/nginx/html</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> aws-ebs-storage</span></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> aws-ebs-pvc</span></span></code></pre></div>
<h3 id="storageclass-1"><code>StorageClass</code></h3>
<p>As far as K8s goes storage classes are resources in the
<code>storage.k8s.io/v1</code> API group. The resource type in
<code>StorageClass</code> and you define them in regular YAML format
manifest files, and they are posted to the API server for deployment,
you can use the <code>sc</code> short name to refer to them when using
<code>kubectl</code></p>
<p>As with all k8s yaml, kind tells the api server what type of object
you are defining and the api version tells it which version of the
schema to use, when creating it, metadata.name is an arbitrary string
that lets you give the object a friendly name. This example is using
fast-local.provisioner and tells Kubernetes which plugin to use and the
parameters block let you fine tune the storage attributes, finally the
<code>allowedTopologies</code> property lets you list where replicas
should go. Also a few notes worth noting</p>
<h4 id="multiple-storageclass-objects">Multiple
<code>StorageClass</code> objects</h4>
<p>You can configure as many <code>StorageClass</code> as you would
need. However each class can only relate to a single type of storage on
a single back end. For example if you have a Kubernetes cluster with
<code>StorageOS</code> and <code>Portworx</code> storage back end you
will at needs least two <code>StorageClasses</code>, this is because you
would need at the very least two provisioners/plugins which are for the
two back ends, since they do not share the same internal working,
therefore different plugin implementations are in order and
required.</p>
<p>One the flip side each back end storage system can offer multiple
classes tiers of storage each of which, needs its own
<code>StorageClass</code> on Kubernetes A simple example which will be
explored later on, is that a slower standard persistent disk and the
faster SSD persistent disk tiers offered by the Google Cloud back end.
These are typically implemented with the following SC on the
<code>GKE</code></p>
<ol type="1">
<li><code>standard-rwo</code> for the slower standard disk</li>
<li><code>premium-rwo</code> for the faster SSD</li>
</ol>
<p>The following <code>StorageClass</code> defines a block storage
volume on a <code>Commvault Hedvig</code> array that is replicated
between data centers in <code>Sunderland and New York</code>, it will
only work if you have <code>Commvault Hedvig</code> storage systems and
appropriate replication configured on the storage system.</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> sc-hedvig-rep</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> io.hedvig.csi</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">backendType</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;hedvig-block&quot;</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">rp</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;DataCenterAware&quot;</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">dcName</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;sunderlang,new-york&quot;</span></span></code></pre></div>
<h4 id="persistent-volumes">Persistent Volumes</h4>
<p>There are few other important settings you can configure in a
<code>StorageClass</code>, such as access mode, reclaim policy.
Kubernetes supports three access modes for volumes -
<code>READWRITEONCE (RWO), READWRITEMANY (RWM), READONLYMANY (ROM)</code>.</p>
<h5 id="access-mode">Access Mode</h5>
<p>The access mode type specifies that and by what a given storage class
can be accessed,</p>
<ul>
<li><p><code>ReadWriteOnce(RWO)</code> - defines a persistent volume,
that can only be bound as R/W by a single Node, and by proxy a Pod,
attempts to write or read data from another Pod, will fail, this is idea
for stateful aps like databases where only one app can access the data
at a time, to retain atomicity.</p></li>
<li><p><code>ReadWriteMany(RWX)</code> - defines a persistent volume,
that can be bound as a R/W by multiple Nodes, and by proxy Pods, this
behavior is strictly dependent on the underlying apps, since such
concurrent reads and writes are unpredictable, and the underlying apps
have to be able to handle this gracefully. A good use case is apps that
need shared data, and that can publish that shared data in real time
like content management systems, but the data they publish does not
necessarily interfere with each other, and can be published without
concurrent conflicts.</p></li>
<li><p><code>ReadOnlyMany(ROX)</code> - defines a persistent volume that
can be bound as a R/O by multiple Nodes, and by proxy Pods, these are
meant for read only, access and are mostly useful for app
configurations, stateful read only configurations, which are used to
bootstrap the apps and Pods and are also used during the runtime of
these apps and services.</p></li>
</ul>
<p>What are the general guidelines on which type of access mode is used
where:</p>
<ul>
<li><p>Use <code>ReadWriteOnce (RWO)</code> for stateful applications
like databases.</p></li>
<li><p>Use <code>ReadOnlyMany (ROX)</code> for sharing static immutable
data across multiple apps.</p></li>
<li><p>Use <code>ReadWriteMany (RWX)</code> for applications that need
shared read-write access, like file/ftp servers.</p></li>
</ul>
<h5 id="reclaim-policy">Reclaim policy</h5>
<p>A volume’s <code>ReclaimPolicy</code> tell kubernetes how to deal
with a persistent volume when its PVC is released. Two policies
currently exist - <code>Delete and Retain</code>.</p>
<p><code>Delete</code> is the most dangerous and its the default for
Persistent volumes, It deletes the Persistent Volume and associated
storage resource on the external storage system when the PVC is
released. This means all data will be lost! You should obviously use
this policy with caution.</p>
<p><code>Retain</code> will keep the associated persistent volume object
on the cluster as well as any data stored on the associated external
asset. However other PVCs are prevented from using it in future. The
obvious disadvantage is it requires manual clean up.</p>
<h4 id="skeleton-3">Skeleton</h4>
<p>To finalize the section about <code>StorageClass</code>, here is a
brief overview of the skeleton spec structure of the
<code>StorageClass</code> object, this is just a brief overview of what
it supports as a k8s native object</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> slow</span><span class="co"> # the name of the storage class object</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageclass.kubernetes.io/is-default-class</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> kubernetes.io/gce-pd</span><span class="co"> # this is the plugin or provisioner spec/id</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> pd-standard</span><span class="co"> # any parameters to enhance or configure the provisioner</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="fu">reclaimPolicy</span><span class="kw">:</span><span class="at"> Retain</span></span></code></pre></div>
<ol type="1">
<li><p>The <code>StorageClass</code> objects are immutable this means
that you can not modify them after they are deployed</p></li>
<li><p>The metadata.name should be a meaningful name as it is how you
and other objects refer to the class</p></li>
<li><p>The terms provisioner and plugin are used
interchangeably</p></li>
<li><p>The parameters block is for plugin specific values and each
plugin is free to support its own set of values configuring this section
requires knowledge of the storage plugin and associated storage back
end. Each provisioner usually provides documentation.</p></li>
</ol>
<p>The <code>StorageClass</code> lets you dynamically create physical
back end storage resources that get automatically mapped to a Persistent
Volumes on Kubernetes. You define <code>StorageClasses</code> in YAML
files that reference a plugin or provisioner and tie them to a
particular tier of storage back end. For example high performance SSD
storage in the AWS Mumbai Region. The <code>StorageClass</code> needs a
name and you deploy it using the <code>kubectl</code> apply. Once
deployed the <code>StorageClass</code> watches the API server for new
PVC objects referencing its name. When matching <code>PVCs</code> appear
the <code>StorageClass</code> dynamically creates the required asset on
the back end storage system and maps it to a persistent volume on the
K8s environment. Apps can then claim it with a PVC.</p>
<h3 id="application">Application</h3>
<p>As we have already established the <code>StorageClass</code> are
usually created by system administrators, on most cloud platform, those
<code>StorageClass</code> are mostly generic ones providing the needed
basic capabilities for users and their pods to interact with a
persistent storage medium.</p>
<h4 id="using-existing-storageclass">Using existing
<code>StorageClass</code></h4>
<p>The following command will list all SC defined on a typical (in our
example <code>GKE</code> cluster), based on the type of cloud platform
provider and cluster those will likely be different</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get sc</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>         PROVISIONER           RECLAIMPOLICY        VOLUMEBINDINGMODE</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="ex">premium-rwo</span>  pd.csi.storage.gke.io Delete               WaitForFirstConsumer</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="ex">standard</span>     <span class="er">(</span><span class="ex">default</span><span class="kw">)</span>             <span class="ex">kubernetes.io/gce-pd</span> Delete Immediate</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="ex">standard-rwo</span> pd.csi.storage.gke.io Delete               WaitForFirstConsumer</span></code></pre></div>
<p>There is quite a lot to learn from the output. First up all three SC
were automatically created when the cluster was built by the actual
cloud provider (Google in this case). This is common on hosted
Kubernetes platforms, but your cluster may not have any The one on the
second line is listed as the default, This means it will be used by any
PVC that do not explicitly specify an SC, Default SC are only useful in
development environments and times when you do not have specific storage
requirements, in production environments you should explicitly use a
named SC in your PVC that meets the requirements of the app. The
<code>PROVISIONER</code> column shows two of the SC using the CSI plugin
the other is using the legacy in-tree plugin, built into the source tree
of k8s, The <code>RECLAIMPOLICY</code> is set to Delete as for all
three. This means any PVC that use these <code>SC</code> will create
<code>PV</code> and volumes that will e deleted when the PVC is deleted,
This will result in data being lost, The alternative is Retain. Setting
<code>VOLUMEBINDINGMODE</code> to immediate will create the volume on
the external storage system as soon as the PVC is created, if you have
multiple data centers or cloud regions, the volume might be created in a
different data center or region than the Pod that eventually consume it.
Setting the <code>WaitForFirstConsumer</code> will delay creation until
the Pod using the PVC is created. This ensures that the volume will be
created in the same data center or region as the Pod and actually the
Node where the pod is running on.</p>
<p>You can use <code>kubectl</code> describe to get more detailed
information, just as with any other k8s object, and
<code>kubectl get sc &lt;name&gt; -o yaml</code> will show the full
configuration in YAML format.</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe sc premium-rwo</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                 premium-rwo</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="ex">IsDefaultClass:</span>       No</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>          components.gke.io/component-name=pdcsi-addon...</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Provisioner:</span>          pd.csi.storage.gke.io</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Parameters:</span>           type=pd-ssd</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="ex">AllowVolumeExpansion:</span> True</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="ex">MountOptions:</span>         <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="ex">ReclaimPolicy:</span>        Delete</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="ex">VolumeBindingMode:</span>    WaitForFirstConsumer</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="ex">Events:</span>               <span class="op">&lt;</span>none<span class="op">&gt;</span></span></code></pre></div>
<p>Let us create a new volume using the <code>premium-rwo</code>
<code>StorageClass</code>, the premium one is a basic fast SSD storage,
First let us list any existing persistent volumes and persistent volume
claims, we will quickly see that there are none, created by us, which is
normal, those are user managed objects, unlike most of the
<code>StorageClass</code> objects which are rarely provisioned by
regular users, outside of enterprise organizations, which have more
exotic needs.</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pv</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="ex">No</span> resources found</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pvc</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="ex">No</span> resources found</span></code></pre></div>
<p>The following PVC definition uses the <code>premium-rwo</code>
storage class / driver and reserves or requests 10 gigs of storage, with
the proper access type - <code>ReadWriteOnce</code>, this means that the
persistent volume created by this PVC eventually, will only be
accessible by only one single PVC, that is, and the command to create it
following the manifest below</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc-prem</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> premium-rwo</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span></code></pre></div>
<div class="sourceCode" id="cb46"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> pvc.yaml</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="ex">persistentVolumeClaim/pvc-prem</span> created</span></code></pre></div>
<p>The following commands will show a PVC has been created, however it
is in the pending state and no <code>PV - persistent volume</code> has
been created on the actual underlying storage device, This is because
the <code>premium-rwo</code> <code>StorageClass</code> volume binding
mode is set to <code>WaitForFirstConsumer</code>, as we have seen above,
when we described that <code>StorageClass</code>, meaning that it will
not provision a volume until a Pod claims it.</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pv</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="ex">No</span> resources found</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pvc</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>     STATUS  VOLUME CAPACITY ACCESS</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pvc-prem</span> Pending premium-rwo     1m</span></code></pre></div>
<p>Create the Pod from the following manifest document, and that will
then mount a volume via the <code>pvc-prem</code>
<code>PersistentVolumeClaim</code> which will in turn actually provision
the persistent volume on the storage device through the use of the
<code>StorageClass</code> and drivers/plugin/provisioner</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> volpod</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> pvc-prem</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ubuntu-ctr</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> ubuntu:latest</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">command</span><span class="kw">:</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> /bin/bash</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;-c&quot;</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;sleep 60m&quot;</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /data</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span></code></pre></div>
<p>This will actually force the persistent volume to be created since we
now have a pod that is using it actually, the <code>volumeMounts</code>
above show that the <code>pvc-prem</code> will be bound to /data in the
container. The volumes section defines what volumes are defined for the
pod, in this case it is just one, and that one uses the already created
<code>claimName</code> <code>pvc-prem</code>. Give the pod some time to
start and after that log out the output of
<code>kubectl get pv</code></p>
<div class="sourceCode" id="cb49"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># provision the pod, with the spec provided above, for this example we mostly care about the pv that will be created</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> pod.yml</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pod/prempod</span> created</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co"># wait for some time before doing this, to allow for the pod and all resources to actually get provisioned</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pv</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>         CAPACITY MODES RECLAIM POLICY STATUS  CLAIM            STORAGECLASS</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="ex">pvc-796af...</span> 10Gi     RWO   Delete         Bound   default/pvc-prem premium-rwo</span></code></pre></div>
<p>To drop the resources we have just created, one would simply do the
following, however remember what will happen with the persistent volume,
since the <code>StorageClass</code> defines is at Delete as the
<code>ReclaimPolicy</code>, then the persistent volume will also be
deleted once nothing is bound to it anymore.</p>
<div class="sourceCode" id="cb50"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># deletes the pod, however the persistent volume will also go away, since the StorageClass defines the ReclaimPolicy as delete</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl delete pod prempod</span></code></pre></div>
<h4 id="creating-new-storageclass">Creating new
<code>StorageClass</code></h4>
<p>What about creating a custom <code>StorageClass</code> object, that
is then used to create a new volume just as the examples above. The
storage class that we will create is defined in the following manifest
file, and is of the following properties</p>
<ul>
<li>Fast SSD device, <code>pd-ssd</code></li>
<li>Replicated - <code>regional-pd</code></li>
<li>Create on demand -
<code>volumeBindingMode: WaitForFirstConsumer</code></li>
</ul>
<div class="sourceCode" id="cb51"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> sc-fast-repl</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> pd.csi.storage.gke.io</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> pd-ssd</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">replication-type</span><span class="kw">:</span><span class="at"> regional-pd</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="fu">volumeBindingMode</span><span class="kw">:</span><span class="at"> WaitForFirstConsumer</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="fu">reclaimPolicy</span><span class="kw">:</span><span class="at"> Retain</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="co"># allowedTopologies:</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="co"># - matchLabelExpressions:</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   - key: topology.kubernetes.io/zone</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co">#     values:</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     - europe-west2-b</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="co">#     - europe-west2-c</span></span></code></pre></div>
<p><code>Remember that the parameters above, are strictly for the provisioner to use, in this case it is a proprietary google based storage provisioner/plugin - pd.csi.storage.gke.io these parameters are meaningless for any other provisioner generally</code></p>
<p>The parameters field in a <code>StorageClass</code> is a key value
pair map that allows you to pass configuration options to the
provisioner, these parameters share specific to the provisioner and the
underlying storage system, for example the AWS EBS - uses parameter
names such as type, Ceph might use parameters like <code>pool</code>,
<code>clusterID</code>, <code>imageFormat</code>, and NFS might use
parameters like server and path. These parameters are not standardized
across all CSI drivers or provisioners, each CSI driver or provisioner
is defining its own set of supported parameters based on the
capabilities of the storage backend. The CSI spec standardizes the API
for communication between the k8s and the storage backend but it does
not enforce specific parameters.</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># provision the storage class, remember that it is immutable once created there is no changing it</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> my-sc.yml</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="ex">storageclass.storage.k8s.io/sc-fast-repl</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># list the details of all storage classes, where we can see the new one being created</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get sc</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>         PROVISIONER           RECLAIMPOLICY VOLUMEBINDINGMODE</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="ex">premium-rwo</span>  pd.csi.storage.gke.io Delete        WaitForFirstConsumer</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="ex">sc-fast-repl</span> pd.csi.storage.gke.io Retain        WaitForFirstConsumer</span></code></pre></div>
<p>Now to deploy a new manifest of a persistent volume claim that is
going to use the new <code>StorageClass</code>, we can simply use the
following manifest as follows. The manifest below is a simple manifest
for a PVC that is using the custom storage class created above, just as
with any other PVC, the volume size is specified and the
<code>accessModes</code>,</p>
<div class="sourceCode" id="cb53"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc2</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> sc-fast-repl</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 20Gi</span></span></code></pre></div>
<p>Finally the final steps is to create a new Pod, that is going to be
using this PVC, similarly to before as we have created the custom
storage class with <code>WaitForFirstConsumer</code>, that means that
the volume would be only created on demand when at least one pod with a
valid PVC that is bound to that <code>StorageClass</code> is configured
to be used in a Pod.</p>
<div class="sourceCode" id="cb54"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> volpod</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> pvc2</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ubuntu-ctr</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> ubuntu:latest</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">command</span><span class="kw">:</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> /bin/bash</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;-c&quot;</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;sleep 60m&quot;</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /data</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span></code></pre></div>
<p>Since the Pod and PVC were both deployed with a manifest, we can
simply either use the manifest files, to delete them, or manually delete
the objects, usually the preferred way is to use the same manifest files
used to create the objects, to remove them, to remove them, execute the
following</p>
<div class="sourceCode" id="cb55"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># delete the pod</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl delete <span class="at">-f</span> volpod.yml</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># delete the persistent claim</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl delete <span class="at">-f</span> pvc2.yml</span></code></pre></div>
<p>Now even those are now deleted, the <code>kubectl get pv</code> will
show that the persistent volumes still exist. This is because the
storage class it was created from is using the Retain policy. This keep
the persistent volume associated with the back end volumes and the data
even when the PVC are deleted.</p>
<h2 id="configmaps-secrets"><code>ConfigMaps</code> &amp; Secrets</h2>
<p>Most business app comprise of two main parts - the app and the
configuration. A simple example is a web server such as NGINX or httpd.
Neither are very useful without a configuration. However when you
combine them with a configuration they become extremely useful.</p>
<p>In the past we coupled the app and the configuration into a single
easy to deploy unit. As we moved into the early days of cloud native
microservices and apps we brought this model with us. However it is an
anti-pattern in the cloud native world. You should de couple the app and
the configuration.</p>
<h3 id="the-big-picture-1">The big picture</h3>
<p>As already mentioned most apps need configuration to function
correctly, This does not change in the k8s world, Let us consider a
simple example - we would like to deploy modern apps to K8s and have 3
distinct environments, - Dev, Test, and Prod. The developers write and
update the app, initial testing is performed in the dev environment,
further testing is done in the test environment here more stringent
rules are taken. Finally stable components graduate to the prod
environment.</p>
<p>However each env, has subtle differences, this includes things such
as number of nodes, configuration of nodes, network and security
policies different sets of credentials and certificates and more. You
currently package each app microservice with its configuration backed
into the container. With this in mind you have to perform all of the
following on every business app</p>
<ul>
<li>Build three distinct images, for each env, each of which will have
backed in the different configurations for each env</li>
<li>Store the images in three distinct repositories, (dev, test and
prod)</li>
<li>Run each version of the image in a specific environment</li>
</ul>
<p>Every time you change the config of an app, even the smallest change
like fixing a typo, you will need to build and package and entirely new
image and perform some type of update to the entire app.</p>
<p>There are several drawbacks to the approach of storing the app and
its configuration as a single artifact - container image. As your dev,
test and prod environments have different characteristics each env needs
its own image, A dev or test image will not work in the prod env,
because of things like different security credentials and restrictions.
This requires extra work to create an maintain 3x copies of each app.
This complicates matters and increases the chances of mistakes,
including things that work in dev and test but not prod.</p>
<p>How should it work then ? Well you should be able to build a single
version of your app, that is shared across all three environments, you
store a single image in a single repo, you run a single version of that
image in all environments. To make this work you build your app and
images as generically as possible with no embedded configuration, you
then create and store configurations in a separate objects, and apply a
configuration to the app at the time it is run. For example you have a
single copy of a web server that you can deploy to all three
environments. When you do deploy it to prod you apply the prod
configuration. When you run it in dev you apply the dev configuration
and so on.</p>
<p>In this model you create and test a single version of each app image,
and you can even re use images across different apps. For example a
hardened stripped down NGINX image can be used by lots of different
apps, just load different configurations at run time</p>
<h3 id="configmaps"><code>ConfigMaps</code></h3>
<p>K8s provides an object called a <code>ConfigMap</code>, that lets you
store configuration data outside of a Pod, it also lets you dynamically
inject the config into a Pod at run-time. When we use the term Pod we
really mean container, after all it is ultimately the container that
receives the configuration data and runs the app, but the pod is the
logical object that the <code>ConfigMap</code> is bound to, since the
Pod is the overarching manager of the containers and images that are
being run</p>
<p><code>ConfigMaps</code> are first class objects in the k8s API, under
the core API group and they are in v1 of the API. They are stable, and
have been around for a while, you can operate on them with the usual
<code>kubectl</code> commands and they can be defined and deployed via
the usual manifest YAML file format</p>
<h4 id="premise">Premise</h4>
<p><code>ConfigMap</code> are typically used to store non sensitive
configuration data such as environment variables and entire
configuration files, hostnames, server ports, account names etc. You
should not use the <code>ConfigMap</code> to store sensitive data as
certificates and password. K8s provides a different object called a
Secret, for storing sensitive data. Secrets and <code>ConfigMaps</code>
are very similar in design and implementation the major difference is
that k8s takes steps to obscure the data stored in secrets. It makes no
such efforts to obscure data stored in <code>ConfigMaps</code></p>
<p>The way <code>ConfigMaps</code> are defined is as a map of key value
pairs and we call each key value pair and entry.</p>
<ul>
<li>Keys are an arbitrary name that can be created from alphanumeric,
dashes, dots and underscores</li>
<li>Values can contain anything really, including multiple lines with
carriage returns</li>
<li>Keys and values are separated by a colon -
<code>key: value</code></li>
</ul>
<p>More complex examples can store entire configuration files like that,
below is the actual value that the <code>ConfigMap</code> stores:</p>
<div class="sourceCode" id="cb56"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>key: conf</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>value:</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    directive in;</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    main block;</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    http {</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        server {</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        listen 80 default_server;</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        server_name *.nigelpoulton.com;</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        root /var/www/nigelpoulton.com;</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>        index index.html</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>            location / {</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>                root /usr/share/nginx/html;</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>                index index.html;</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Once the data is stored in a <code>ConfigMap</code> it can be
injected into containers at run time via any of the following
methods</p>
<ul>
<li>Environment variables</li>
<li>Arguments to the container’s startup command</li>
<li>Files and volumes</li>
</ul>
<p>All of the methods work seamlessly with existing app, in fact all an
app sees is its configuration data in either - env variable, an argument
or a file on the filesystem. The app is unaware the data originally came
from a <code>ConfigMap</code> or a Secret type of object. The most
flexible of the three methods is the volume option, whereas the most
limited are the startup command.</p>
<p>A K8s native app is one that knows its running on k8s and can talk to
the k8s API. As a result they can access <code>ConfigMap</code> data
directly via the API without needing things like environment variables,
startup arguments or variables and volumes. This can simplify app
configuration, but the app will only run in k8s environment.</p>
<h4 id="definition">Definition</h4>
<div class="sourceCode" id="cb57"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl create configmap testmap1 <span class="dt">\</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">--from-literal</span> shortname=AOS <span class="dt">\</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--from-literal</span> longname=<span class="st">&quot;Agents of Shield&quot;</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe cm testmap1</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>        testmap1</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>   default</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>      <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span> <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Data</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="ex">====</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="ex">longname:</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="ex">----</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="ex">Agents</span>       of Shield</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="ex">shortname:</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="ex">----</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="ex">AOS</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="ex">Events:</span>      <span class="op">&lt;</span>none<span class="op">&gt;</span></span></code></pre></div>
<h4 id="creating">Creating</h4>
<p>The most basic example, creating a new config map object directly
from the command line, without using any manifest files, this is solely
to demonstrate how they can be created, but in the real world, the
preferred creation method is always the YAML manifest way.</p>
<div class="sourceCode" id="cb58"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> `ConfigMap`</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span><span class="kw">:</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">given</span><span class="kw">:</span><span class="at"> Nigel</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">family</span><span class="kw">:</span><span class="at"> Poulton</span></span></code></pre></div>
<p>Here is a proper example, which uses the regular manifest file to
create a new <code>ConfigMap</code>, notice that the config map is given
a name, and also that the data section, specifies the number of key
value pairs which are representing the data this config map
provides.</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> apply <span class="at">-f</span> multimap.yml</span></code></pre></div>
<p>The manifest below uses the special pipe characters, that tells the
k8s api to treat everything after the pipe as a literal value, meaning
that the actual value of the key <code>config</code> is the multi line
text, which we can see after the pipe character, that is cool since this
is a good way to represent values which can be later on mounted as files
form the config map. This is often used to inject complex
configurations, such as json files, or even shell scripts</p>
<div class="sourceCode" id="cb60"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> `ConfigMap`</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> test-conf</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span><span class="kw">:</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="fu">    config</span><span class="kw">: </span><span class="ch">|</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        env = plex-test</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        endpoint = 0.0.0.0:31001</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        char = utf8</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        vault = PLEX/test</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>        log-size = 512M</span></code></pre></div>
<h4 id="injecting">Injecting</h4>
<p>Binding the <code>ConfigMap</code> to a pod and injecting it is the
next step. Looking at each of the 3 methods to inject
<code>ConfigMaps</code>, and their pros and cons</p>
<h5 id="environment-variables">Environment variables</h5>
<p>A common way to get <code>ConfigMap</code> data into a container is
via environment variables. You create the <code>ConfigMaps</code> then
you map its entries int environment variables in the container section
of a Pod template. When the container is started the environment
variables appear in the container as standard Linux or Windows
environment variables, we use a combination of
<code>configMapKeyRef</code>, where the name of the key and the name of
the source <code>ConfigMap</code> must be provided</p>
<div class="sourceCode" id="cb61"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> envpod</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ctr1</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> busybox</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">]</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">args</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;infinity&quot;</span><span class="kw">]</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">env</span><span class="kw">:</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> FIRSTNAME</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">valueFrom</span><span class="kw">:</span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">configMapKeyRef</span><span class="kw">:</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">key</span><span class="kw">:</span><span class="at"> given</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> LASTNAME</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">valueFrom</span><span class="kw">:</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">configMapKeyRef</span><span class="kw">:</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">key</span><span class="kw">:</span><span class="at"> family</span></span></code></pre></div>
<p>When the Pod is schedules and the container started, the
<code>FIRSTNAME</code> and <code>LASTNAME</code> will be created as
standard Linux environment variables inside the container. Apps can use
these like regular variables because they are. Run the following to
deploy the pod from the manifest above,</p>
<div class="sourceCode" id="cb62"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl appy <span class="at">-f</span> envpod.yml</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl exec envpod <span class="at">--</span> env <span class="kw">|</span> <span class="fu">grep</span> NAME</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="ex">HOSTNAME</span>  = envpod</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="ex">FIRSTNAME</span> = Nigel</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="ex">LASTNAME</span>  = Poulton</span></code></pre></div>
<p>A drawback to using <code>ConfigMaps</code> with environment
variables is that environment variables are static, This means that
updates made to the map are not reflected in running containers. For
example if you update the values of the given and family entries in the
<code>ConfigMap</code> environment variables in existing containers will
not see those updates. This is a major reason environment variables are
not very good</p>
<h5 id="startup-arguments">Startup arguments</h5>
<p>This concept is simple, you specify a startup command for a
container, and then customize it with variables, the following pod
template is showing that, how a single container is called with
<code>args1</code>. Also if you play a close attention you will notice
that this is also using the environment variables approach.</p>
<div class="sourceCode" id="cb63"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> startup-pod</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">chapter</span><span class="kw">:</span><span class="at"> configmaps</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">restartPolicy</span><span class="kw">:</span><span class="at"> OnFailure</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> container</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> busybox</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="co">          # that is the startup command for the container, in other words the ENTRYPOINT from Dockerworld</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;/bin/sh&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;-c&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;echo First name $(FIRSTNAME) last name $(LASTNAME)&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;wait&quot;</span><span class="kw">]</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">env</span><span class="kw">:</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> FIRSTNAME</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">valueFrom</span><span class="kw">:</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">configMapKeyRef</span><span class="kw">:</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">key</span><span class="kw">:</span><span class="at"> given</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> LASTNAME</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">valueFrom</span><span class="kw">:</span></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">configMapKeyRef</span><span class="kw">:</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">key</span><span class="kw">:</span><span class="at"> family</span></span></code></pre></div>
<h5 id="volumes-files">Volumes &amp; Files</h5>
<p>Using the <code>ConfigMaps</code> with volumes is the most flexible
option. You can reference entire configuration files as well as make
updates to the <code>ConfigMap</code> and have them reflected in running
containers. This means you can make changes to entries in the
<code>ConfigMap</code> after you have deployed a container and those
changes be seen in the container and available for running apps. The
updates may take a minute or so to appear in the container. But how is
the <code>ConfigMap</code> exposed as a volume you may ask ?</p>
<p>Well first what is the process of creating such a config map, that
would be simply creating a <code>ConfigMap</code> object, afterwards
this <code>ConfigMap</code> object is bound to or as a volume to a Pod,
and mounted into the container. Entries in the <code>ConfigMap</code>
would appear in the container is individual files</p>
<div class="sourceCode" id="cb64"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> cmvol</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> volmap</span><span class="co"> # this is the name of the volume and we have bound a value to it below</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">configMap</span><span class="kw">:</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multimap</span><span class="co"> # the bound value and here we are re-using the multimap we have created</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> container</span><span class="co"> # name the container, for the pod</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span><span class="co"> # tell the container what image to use</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span><span class="co"> # specifies a list of volumes to the pod</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> volmap</span><span class="co"> # tell the pod which volume to use</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /etc/name</span><span class="co"> # tell the pod where to exactly mount this volume</span></span></code></pre></div>
<p>Now what would the effect of this be, the values inside the
<code>multimap</code> <code>ConfigMap</code> are two key value pairs.
The config map has two keys given and family, these will be used to
create the names of the files, while the values for these keys will be
used to populate the files, therefore the contents of the files will be
Nigel and Poulton respectively. This is quite powerful since we can
define the configuration for any application and any changes to the
config map will correctly reflect the changes in the file.</p>
<h3 id="secrets">Secrets</h3>
<p>Secrets are almost identical to <code>ConfigMaps</code>, they hold
app configuration data that is injected into containers at run-time.
However Secrets are designed for sensitive data such as passwords,
certificates and OAuth tokens.</p>
<h4 id="security-secrets">Security &amp; Secrets</h4>
<p>Are secrets really secure, The quick answer to this question is no,
but there is a slightly longer answer. Despite being designed for
sensitive data, kubernetes does not encrypt Secrets. It merely obscures
them as base 64 encoded values that can easily be decoded, fortunately
it is possible to configure encryption at rest with
<code>EncryptionConfiguration</code> objects, and most service meshes
encrypt network traffic. A typical workflow for Secret is as follow.</p>
<ol type="1">
<li>The secret is created and persisted to the cluster store as an
un-encrypted object</li>
<li>A pod that uses it gets scheduled to a cluster node</li>
<li>The secret is transferred over the network un-encrypted to the
node</li>
<li>The kubelet on the node starts the Pod and its containers</li>
<li>The Secret is mounted into the container via an in memory tmpfs file
system and decoded from base64 to plain text</li>
<li>The app consumes it</li>
<li>When the pod is deleted the secret is deleted from the node.</li>
</ol>
<p>While it is possible to encrypt the secret in the cluster store and
leverage a service mesh to encrypt it in flight on the network, it is
always mounted as plain text in the Pod, container. This is so the app
can consume it without having to perform decryption or base64 decoding
operations. Also the use of in memory tmpfs file systems mean they are
never persisted to disk on a node. So to cut a long story short, no
secrets are not very secure, but you can take extra steps to make them
secure. They are also limited to 1MB of size. An obvious use case for
Secrets is a generic TLS termination proxy for use across your dev, test
and prod, environments. You create a standard image, and load the
appropriate TLS keys at run time for each environment.</p>
<p>By default every pod gets a secret mounted into it as a volume which
it uses to authenticate itself if it talks to the API server, if we take
a look at any running pod in our cluster we will be able to use the
describe command from kubectl and we can actually see the
<code>defulat-token-s9nmx</code> which is a secret value that is mounted
into the container automatically by the kubectl so it can communicate
with the API server, that is simple API token, to call the REST
endpoints with.</p>
<div class="sourceCode" id="cb65"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe pod <span class="op">&lt;</span>podname<span class="op">&gt;</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                                          cmvol</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>                                     default</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Snip<span class="op">&gt;</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Containers:</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="ex">ctr:</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Container</span>                                      ID: containerd://0de32d677251cbbda3ebe53e8...</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Image:</span>                                         nginx</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Mounts:</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a> <span class="ex">/etc/name</span>                                     from volmap <span class="er">(</span><span class="ex">rw</span><span class="kw">)</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a> <span class="ex">/var/run/secrets/kubernetes.io/serviceaccount</span> from default-token-s9nmx <span class="er">(</span><span class="ex">ro</span><span class="kw">)</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a> <span class="op">&lt;</span>Snip<span class="op">&gt;</span></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a> <span class="ex">Volumes:</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a> <span class="ex">default-token-s9nmx:</span></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a> <span class="ex">Type:</span>                                         Secret <span class="er">(</span><span class="ex">a</span> volume populated by a Secret<span class="kw">)</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a> <span class="ex">SecretName:</span>                                   default-token-s9nmx</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a> <span class="ex">Optional:</span>                                     false</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a> <span class="ex">QoS</span>                                           Class: BestEffort</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>Snip<span class="op">&gt;</span></span></code></pre></div>
<p><code>Also note where this is mounted, /var/run/secrets/kubernetes.io/, this is a common way for k8s to prefix its own resources with kubernetes.io, in this case the secret is mounted under /var/run/secrets and the kubernetes specific folder</code></p>
<div class="sourceCode" id="cb66"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get secrets</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                TYPE                                DATA AGE</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="ex">default-token-s9nmx</span> kubernetes.io/service-account-token 3    21d</span></code></pre></div>
<p>You might get an output similar to this one, which will show the list
of currently active secrets, one of which will be the actual certificate
we, attached to the Pod itself, and used by the Pod to communicate with
the REST server of the control plane.</p>
<div class="sourceCode" id="cb67"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe secret default-token-s9nmx</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>                              default-token-s9nmx</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Namespace:</span>                         default</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>                            <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>                       kubernetes.io/service-account.name: default</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="ex">kubernetes.io/service-account.uid:</span> c5b5a4b3-3c5c...</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Type:</span>                              kubernetes.io/service-account-token</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Data</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="ex">====</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">token:</span>                             eyJhbGciOiJSUzI1NiIsIm...</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ca.crt:</span>                            570 bytes</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="ex">namespace:</span>                         7 bytes</span></code></pre></div>
<h4 id="making-secrets">Making secrets</h4>
<p>As we have already seen the secrets are not encrypted in the cluster
store (etcd) they are not encrypted in flight, not encrypted on the
network and not encrypted when surfaced in a container. There are ways
to encrypt them, but at the end of the day the container has to have a
way to read these secrets in plain text, either by decrypting them
itself, or having the secret decrypted when delivered or mounted into
the container. Now by default when we create secrets in the manifest
file, the contents of the manifest file or more precisely the
<code>data</code> section of the manifest must contain the base64
version of the secret or data</p>
<div class="sourceCode" id="cb68"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Secret</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tkb-secret</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">chapter</span><span class="kw">:</span><span class="at"> configmaps</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="fu">type</span><span class="kw">:</span><span class="at"> Opaque</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span><span class="kw">:</span><span class="co"> # provide a base64 encoded data, the api server will accept it as is</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">username</span><span class="kw">:</span><span class="at"> bmlnZWxwb3VsdG9u</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">password</span><span class="kw">:</span><span class="at"> UGFzc3dvcmQxMjM=</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="co"># stringData: # provide the raw data, the api server will base64 encode it itself</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="co">#   username: myusername</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   password: mypassword</span></span></code></pre></div>
<div class="sourceCode" id="cb69"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to apply the secret manifest from above</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> secret.yml</span></code></pre></div>
<p>Here in this example the <code>data</code> contains values that are
already encoded in base64, this is crucial to remember because, the
<code>data</code> field does not contain a value that is not encoded in
base64, the API server will reject the secret, meaning posting the
manifest will fail, to provide the raw un-encoded data in raw plain text
one should use the <code>stringData</code> field, instead of the
<code>data</code> one.</p>
<div class="sourceCode" id="cb70"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to decode the value and see the actual value</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> echo <span class="op">&lt;</span>base-64-encoded-vlaue<span class="op">&gt;</span> <span class="kw">|</span> <span class="fu">base64</span> <span class="at">-d</span></span></code></pre></div>
<h4 id="using-secrets">Using secrets</h4>
<p>The most flexible way to inject a secret into a pod (container) is
via a special type of volume called a secret volume. The following YAML
describe a single container Pod with a Secret volume called “secret-vol”
based on the secret manifest from above, created in the previous
step.</p>
<div class="sourceCode" id="cb71"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> secret-pod</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">topic</span><span class="kw">:</span><span class="at"> secrets</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> secret-vol</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">secret</span><span class="kw">:</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">secretName</span><span class="kw">:</span><span class="at"> tkb-secret</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> secret-ctr</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> secret-vol</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /etc/secret</span></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">readOnly</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code></pre></div>
<p>The main difference between the <code>ConfigMap</code> type volumes
and the Secret type volumes is that the Secret volumes are automatically
mounted as read only, and also the data in these volumes is decoded from
base64 to base plain text before hand, so they can be mounted as the
actual value the base64 encoding is representing. To apply this pod we
do the usual</p>
<div class="sourceCode" id="cb72"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a pod where the secret is mounted under /etc/secret</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl apply <span class="at">-f</span> secret-pod.yml</span></code></pre></div>
<h2 id="statefulset"><code>StatefulSet</code></h2>
<p>For the purposes of this discussion a stateful app is one that
creates and saves valuable data, an example might be an app that saves
data about client sessions and uses it for future sessions. Other
examples include databases and other data stores.
<code>StatefulSet</code> are the logical equivalent of Deployments in
the K8s world. While the spec structure is similar, there are some
differences:</p>
<ul>
<li><p><code>StatefulSet</code>:</p></li>
<li><p>Includes fields like <code>serviceName</code> (to associate with
a headless service).</p></li>
<li><p>Supports <code>volumeClaimTemplates</code> for creating
<code>PersistentVolumeClaims</code> (PVCs) for each pod.</p></li>
<li><p>Does not support the strategy field for updates (it always uses a
rolling update strategy).</p></li>
<li><p><code>Deployment</code>:</p></li>
<li><p>Includes a strategy field to define update strategies (e.g.,
RollingUpdate or Recreate).</p></li>
<li><p>Does not have <code>serviceName</code> or
<code>volumeClaimTemplates</code>.</p></li>
</ul>
<h3 id="theory-4">Theory</h3>
<p>It is often useful to compare the <code>StatefulSets</code> with
Deployments both are first class API objects and follow the typical
kubernetes controller architecture, they are both implemented as
controller that operate reconciliation loops watching the state of the
cluster, via the API server and moving the observed state into sync with
the desired state. Deployments and <code>StatefulSets</code> also
support self healing scaling updates and more. However there are some
vital differences between <code>StatefulSets</code> and Deployments.
<code>StatefulSets</code> guarantee:</p>
<ul>
<li>Predictable and persistent Pod names</li>
<li>Predictable and persistent DNS hostnames</li>
<li>Predictable and persistent volume bindings</li>
</ul>
<p>These three properties form the sate of Pod, sometimes referred to as
its sticky ID. <code>StatefulSets</code> ensure this state/id is
persisted across failures scaling and other scheduling operations making
them ideal for apps that require unique pods that are not
interchangeable.</p>
<p>A quick example failed Pods managed by a <code>StatefulSet</code>
will be replaced by new Pods with the exact same Pod name the exact same
DNS hostname, and the exact same volumes. This is true even if the
replacement pod is started on a different cluster node. The same is not
true of Pods managed by a Deployment.</p>
<div class="sourceCode" id="cb73"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StatefulSet</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tkb-sts</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">replicas</span><span class="kw">:</span><span class="at"> </span><span class="dv">2</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">app</span><span class="kw">:</span><span class="at"> mongo</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">serviceName</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;tkb-sts&quot;</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">app</span><span class="kw">:</span><span class="at"> mongo</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ctr-mongo</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> mongo:latest</span></span></code></pre></div>
<p>The name of the <code>StatefulSet</code> is <code>tkb-sts</code> and
it defines three pod replicas running the <code>mongo:latest</code>
image. You post this to the API server it is persistent to the cluster
store the replicas are assigned to cluster nodes and the
<code>StatefulSet</code> controller monitors the state of the cluster
making sure observed state matches the desired state, that is the big
picture, let us take a closer look at some the major characteristics of
<code>StatefulSet</code> before walking through an example</p>
<h3 id="naming">Naming</h3>
<p>All Pods managed by a <code>StatefulSet</code> get predictable and
persistent names. These names are vital and are the core of how Pods are
started self healed, scaled, deleted and attached to volumes and more.
The format of the <code>StatefulSet</code> Pod names is
<code>&lt;StatefulSetName&gt;-&lt;Integer&gt;</code>. The integer
<code>is a zero based index ordinal which is just a fancy way of saying number starting from 0</code>.
The first Pod created by a <code>StatefulSet</code> always get index
ordinal 0, and each subsequent Pod gets the next highest. Assuming the
previous YAML snippet the first Pod created will be called
<code>tks-sts-0</code>, the second will be called
<code>tsb-sts-1</code>. Be ware that <code>StatefulSet</code> names need
to be a valid DNS names so no exotic characters are allowed, this rule
and its reasons will come in play later.</p>
<h3 id="creation">Creation</h3>
<p>Another fundamental characteristic of the <code>StatefulSet</code> is
that controlled and ordered way they start and stop Pods.
<code>StatefulSet</code> create one Pod at a time, and always wait for
previous Pods to be running and ready before creating the next. This is
different than from Deployments that use a <code>ReplicaSet</code>
controller to start all Pods at the same time, causing potential race
conditions.</p>
<p>As per the previous YAML snippet <code>tkb-sts-0</code> will be
started first and must be running and ready before the
<code>StatefulSet</code> controller starts <code>tkb-sts-1</code>. The
same applies to subsequent Pods - <code>tkb-sts-1</code> needs to be
running and ready before <code>tkb-sts-2</code> starts and so on.</p>
<p>Scaling operations are also governed by the same ordered startup
rules. For example scaling from 3 to 5 replicas will start a new Pod
called <code>tkb-sts-3</code> and wait for it to be in the running and
ready state, before creating <code>tkb-sts-4</code> Scaling down follows
the same rules in reverse, the controller terminates the Pod with the
highest index ordinal (number) first waits for it to fully terminate
before terminating the Pod with the next highest ordinal.Knowing the
order in which Pods will be scaled down as well knowing that Pods will
not be terminated in parallel is a game changer for many stateful apps.
For example clustered apps that store data can potentially lose data if
multiple replicas go down at the same time. <code>StatefulSet</code>
guarantee this will never happen You can also inject other delays via
things like <code>terminationGracePeriodSeconds</code>, to further
control the scaling down process. All in all <code>StatefulSets</code>
bring a lot to the table for clustered apps that create and store
data.</p>
<p>Finally it is worth noting that <code>StatefulSets</code> controllers
do their own self healing and scaling, this is architecturally different
to Deployments which use a separate <code>ReplicaSet</code> helper
controller for these operations</p>
<p><code>All in all StatefulSet provide a robust predictable and easy to manage control flow over the deployment lifecycle of pods. This is crucial for services which require more fine grained control over their own lifecycle and other dependent components</code></p>
<h3 id="deleting">Deleting</h3>
<p>There are two major things to consider when deleting a
<code>StatefulSet</code> object,</p>
<p>Firstly deleting a <code>StatefulSet</code> does not terminate Pods
in order, With this in mind you may want to scale a
<code>StatefulSet</code> to 0 replicas before deleting it You can also
use the <code>terminationGracePeriodSeconds</code> to further control
the way Pods are terminated. It is common to set this to at least 10
seconds to give apps running in the Pods a chance to flush local buffers
and safely commit any writes that are still in-flight</p>
<h3 id="volumes">Volumes</h3>
<p>Volumes are an important part of the <code>StatefulSet</code> Pod
stick ID and state. When <code>StatefulSet</code> Pod is created any
volumes it needs are created at the same time and named in a special way
that connects them to the right Pod. That means that each volume created
by the PVC. Volumes are appropriately decoupled from Pods via the normal
Persistent Volume Claims system. This means volumes have separate
lifecycle to Pods, allowing them to survive Pod failures and termination
operations. For example any time a <code>StatefulSet</code> Pod fails or
is terminated associate volumes are unaffected. This allows replacement
Pods to attach to the same storage as the Pods they are replacing. This
is true even if replacement Pods are schedule to different cluster
nodes.</p>
<p>The same is true for scaling operations if a <code>StatefulSet</code>
Pod is deleted as part of a scale down operation subsequent scale up
operations will attach new Pods to the surviving volumes that match
their names, this behavior can be a life saver if you accidentally
delete a <code>StatefulSet</code> Pod especially if it is the last
replica.</p>
<p>So here is something to take a good note of, of how
<code>Deployments</code> and <code>StatefulSets</code> differ in their
usage of the persistent volumes and persistent volume claims:</p>
<p>If you have a Deployment, every replica of the Deployment is
identical, aside from its name. In particular, every replica will share
the same <code>PersistentVolumeClaim</code> and the same underlying
<code>PersistentVolume</code>.</p>
<p>Conversely, each replica of a <code>StatefulSet</code> gets its own
<code>PersistentVolumeClaim</code>, assuming you use the
<code>volumeClaimTemplates</code>: field to declare the PVC. If anything
causes the <code>StatefulSet</code> to scale up, the new Pod will get a
new empty <code>PersistentVolume</code>. If it scales down, the
<code>PersistentVolume</code> is preserved, and if it scales up again,
the previous <code>PersistentVolume</code> is reused.</p>
<p><code>Important to note how the names of the unique PVC objects are generated when using the volumeClaimTemplates, since each of the pvc objects is created for each pod replica, the name is generated from the name defined under volumeClaimTemplates, in the StatefulSet spec, and using the ordinal number of the pod it is attached to.</code></p>
<h3 id="handling-failures">Handling Failures</h3>
<p>The <code>StatefulSet</code> controller observes the state of the
cluster and attempts to keep observed state in sync with the desired
state. The simplest example is a Pod failure, if you have a
<code>StatefulSet</code> called <code>tkb-sts</code> with 5 replicas,
and <code>tkb-sts-3</code> fails, the controller will start a
replacement Pod with the same name and attach it to the same volumes it
was already attached to. However if a failed Pod recovers after
Kubernetes has replaced it you will have two identical Pods trying to
write to the same volume. This can result in data corruption. As a
result the <code>StatefulSet</code> controller is extremely careful how
ti handles failures.</p>
<p>Possible node failures are very difficult to deal with. For example,
if Kubernetes loses contact with a node, how does it know if the node
has failed / is down and will never recover or if its temporary glitch,
such as network partition a crashed kubelet or the node is simply
rebooting. To complicate matters further the controller can not even
force the Pod to terminate as the local kubelet may never receive the
instruction to do that. With all of this in ind, manual intervention is
needed before K8s will replace Pods on failed nodes.</p>
<p><code>Unlike deployments, where once the Pod is considered failed, it will never be re-started or ever re instated back into the k8s environment, meaning that a brand new one will be created and the old one will be completely de commissioned, in the StatefulSet world, that is not the case a pod might actually recover so the stateful state controller has to be more conservative when managing failed Pods, unlike its Deployment and ReplicaSet counterpart</code></p>
<h3 id="services-2">Services</h3>
<p>We have already said that <code>StatefulSet</code> are for apps that
need Pods to be predictable and long lived as a result other parts of
the app as well as other apps may need to connect directly to individual
Pods. To make this possible <code>StatefulSet</code> use a headless
service to create predictable DNS names for every pod replica they
manage. Other apps can then query DNS service for the pull list of Pod
replicas and use these details to connect directly to the Pods. The
following YAML snippet shows a headless Service called
<code>mongo-prod</code> that is listed in the <code>StatefulSet</code>
YAML.</p>
<div class="sourceCode" id="cb74"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> mongo-prod</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">clusterIP</span><span class="kw">:</span><span class="at"> None</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> mongo</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">env</span><span class="kw">:</span><span class="at"> prod</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StatefulSet</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> sts-mongo</span></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">serviceName</span><span class="kw">:</span><span class="at"> mongo-prod</span></span></code></pre></div>
<p>Let us explain the terms headless Service and governing Service. A
headless Service is just a regular K8s service object without an IP
address (see that the <code>ClusterIP</code> is not set, or rather is
set to none). This tells K8s that this service will and must not receive
any virtual <code>ClusterIP</code> address. It become a
<code>StatefulSet</code> governing service when you list it in the
<code>StatefulSet</code> manifest under <code>spec.serviceName</code>.
When the two objects are combined like this the Service will create
<code>DNS SRV</code> records for each Pod replica that matches the label
selector of the headless Service. Other Pods and apps can then find
members of the <code>StatefulSet</code> by performing DNS lookup against
the name of the headless Service.</p>
<h3 id="network-traffic-2">Network traffic</h3>
<p>Unlike regular services, the ones represented by
<code>StatefulSet</code>, will not have a virtual <code>ClusterIP</code>
address, the headless service,</p>
<h3 id="skeleton-4">Skeleton</h3>
<p>The snippets below represent the general skeleton and structure of a
deployment stage for a <code>StatefulSet</code>, which includes the
creation and definition of all components that tie into the
<code>StatefulSet</code> - like PVC, <code>StorageClasses</code>,
headless Services and more.</p>
<p>First lets create the most low level structure -
<code>StorageClass</code>, that will be the entry point for the PVC
later, even though most cloud providers do have default SC objects, it
is good to see how it all ties together, of course in the real world you
will end up using the SC provided by the cloud provider most likely.</p>
<p>Nothing fancy, the <code>StorageClass</code> below, just defines that
the store is of type SSD, which is using the plugin -
<code>pd.csi.storage.gke.io</code>. This is specific to the google cloud
provider services, but it will be pretty similar for other providers as
well, in this case the <code>StorageClass</code> represents a basic fast
storage - backed by a solid state drive</p>
<div class="sourceCode" id="cb75"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> storage.k8s.io/v1</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StorageClass</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> flash</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="fu">provisioner</span><span class="kw">:</span><span class="at"> pd.csi.storage.gke.io</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="fu">volumeBindingMode</span><span class="kw">:</span><span class="at"> WaitForFirstConsumer</span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="fu">allowVolumeExpansion</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="fu">parameters</span><span class="kw">:</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> pd-ssd</span></span></code></pre></div>
<p>Here is the headless service definition, we know it is headless by
the fact that the <code>ClusterIP</code> here is set to
<code>none</code>, this is pretty much the only difference with the
regular k8s service objects</p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> dullahan</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> web</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">name</span><span class="kw">:</span><span class="at"> web</span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">clusterIP</span><span class="kw">:</span><span class="at"> None</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> web</span></span></code></pre></div>
<p>Finally the <code>StatefulSet</code>, this one, has a bit more meat
to it. Here we can see some of the most prominent differences compared
to the Deployment object, like the
<code>terminationGracePeriodSeconds</code> fields, also notice that most
of the spec of the Pod in the <code>template</code> section is pretty
much the same as in the Deployment object, the <code>template</code>
section as we know already defines the properties of the Pod object that
would be created by the k8s environment</p>
<p>Now take a look at the most interesting part that differs
significantly from the way we define Pod templates in the Deployments,
that is <code>volumeClaimTemplates</code>, you may notice that the
format and spec of this section matches perfectly with the spec of a
<code>PersistentVolumeClaim</code> object, that is because it is,
however since we have already mentioned above, we know that the
<code>PersistentVolumeClaim</code> for <code>StatefulSet</code> have to
be bound to Pod instances, unlike with Deployments’ Pods.</p>
<p>That is why the PVC is defined in the spec of the
<code>StatefulSet</code>, that way for each new replica of the Pod, a
new PVC will be created and by proxy a new fresh volume which will be
independent of the volumes created for the other PVCs and Replicas, the
PVC will be bound to the name of the Pod. That is why it is also
important that the name of the pod template -
<code>spec.serviceName</code> to be DNS format compliant</p>
<div class="sourceCode" id="cb77"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> StatefulSet</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tkb-sts</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">replicas</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">app</span><span class="kw">:</span><span class="at"> web</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">serviceName</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;dullahan&quot;</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">app</span><span class="kw">:</span><span class="at"> web</span></span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">terminationGracePeriodSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ctr-web</span></span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx:latest</span></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="kw">-</span><span class="at"> </span><span class="fu">containerPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">name</span><span class="kw">:</span><span class="at"> web</span></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx-data</span></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /usr/share/nginx/data</span></span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> config-data</span></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a><span class="at">                        </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /usr/share/nginx/config</span></span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> config-data</span></span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> config-data</span></span>
<span id="cb77-32"><a href="#cb77-32" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumeClaimTemplates</span><span class="kw">:</span></span>
<span id="cb77-33"><a href="#cb77-33" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb77-34"><a href="#cb77-34" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx-data</span></span>
<span id="cb77-35"><a href="#cb77-35" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb77-36"><a href="#cb77-36" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">accessModes</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;ReadWriteOnce&quot;</span><span class="kw">]</span></span>
<span id="cb77-37"><a href="#cb77-37" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;flash&quot;</span></span>
<span id="cb77-38"><a href="#cb77-38" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb77-39"><a href="#cb77-39" aria-hidden="true" tabindex="-1"></a><span class="at">                  </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb77-40"><a href="#cb77-40" aria-hidden="true" tabindex="-1"></a><span class="at">                      </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 1Gi</span></span></code></pre></div>
<p>Note that we can still use the regular way to defined volumes for
<code>StatefulSet</code>, the regular way being the one supported by the
Deployment object, however these will not be bound to the underlying
Pods as with stateful services, what would be the use case for the basic
volume and PVC definition ? Well we could bind read only configuration
data for example, that does not require stateful and Pod bound volumes.
It is just important to keep in mind that the
<code>volumeClaimTemplates</code> DOES NOT REPLACE the regular method of
defining PVCs for pods through the <code>spec.volumes</code> section</p>
<p>Given the <code>volumeClaimTemplates</code> form above and the name
for that defined in <code>metadata.name</code>, <code>nginx-data</code>,
the names of the PVC objects that will be created under the hood and
tied to the Pod replicas will use the base name
<code>nginx-data-&lt;ordinal&gt;</code>, the ordinal number will be the
same as for the pod the PVC is bound to. Remember that the name of the
Pods are actually generated from the name of the
<code>StatefulSet</code>, and also appended an ordinal value which
represents their replication order or in other words ID. That means that
a pod with an ordinal or 3 was the 4th Pod replica to be started,
remember that these ordinals start from 0.</p>
<p>The key takeaway here is to really understand that unlike
Deployments, the <code>StatefulSet</code> are really tied, tightly
coupled with the underlying service which exposes the Pods created by
the <code>StatefulSet</code>, that can be seen in the fact that the
headless service name is actually present in the spec of the
<code>StatefulSet</code>, unlike with Deployments where the interlinking
part is the Pod, between a Deployment and a Service, here the Service is
directly linked to the <code>StatefulSet</code> expressed in the
property <code>spec.serviceName</code>, that is because other elements
of the <code>StatefulSet</code> are also tied in to the name of the
headless service - like the name resolution of the dynamic PVC objects
that are created for each Pod replica in the
<code>StatefulSet</code></p>
<p>Finally the DNS names that would be resolvable for each Pod will be
generated using the <code>StatefulSet</code> name along with the service
name and the ordinal of the Pod, in this case, having the example above
we will have the following DNS names,
<code>tbk-sts-0.dullahan.default.svc.cluster.local</code>, and so on -
the general structure that the DNS name will follow is
<code>tbk-sts-&lt;n&gt;.dullahan.default.svc.cluster.local</code>, where
<code>&lt;n&gt;</code> is going to be the ordinal index of the Pod
replica starting from 0, the <code>tbk-sts</code> is the name of the
<code>StatefulSet</code> object, and the <code>dullahan</code> is the
name of the headless service</p>
<h2 id="security">Security</h2>
<p>Kubernetes is API centric and the API is served through the API
server, below we will inspect how a typical API request to the API
server is processed on the control plane, and what security measures
does the kubernetes API server take to make sure that no unauthorized
parties access the API. Through the use of Role Based Access Control
(RBAC)</p>
<h3 id="theory-5">Theory</h3>
<p>All of the following make CRUD style requests to the API server -
operators and developers using the <code>kubectl</code>, Pods, Kubelets,
Control plane service &amp; controllers and more. The usual flow that
the request follow is that - of a subject (user,group) -&gt; api server
-&gt; authentication -&gt; authorization -&gt; admission. Consider a
quick example where a user called “grant” is trying to create a
Deployment object called “hive” in the namespace “terran”. User “grant”
issues a kubectl command to create the Deployment. This generates a
request to the API server, with the user’s credential embedded, thanks
to the magic of TLS the connection between the client and the API server
is secure. The authentication module determines whether its grant-ward
or an impostor. After that the authorization module (RBAC) determines
whether grant-ward is allowed to create Deployments in the “terran”
Namespace. If the request passes authentication and authorization
admission control checks and applies policies and the request is finally
accepted and executed.</p>
<p>It is a lot like flying on a plane, You travel to the airport and
authenticate yourself with a Photo Id, usually your passport. You then
present a ticket authorizing you to board the plane and occupy a
particular seat. If you pass authentication and are authorized to board
admission controls may then check and apply airline policies such as not
taking hot food on board restricting your hand luggage and prohibiting
alcohol in the cabin. After all of that you are finally allowed to board
the plane and take your set.</p>
<h3 id="authentication">Authentication</h3>
<p>Authentication is about providing your identity. You might see or her
it shortened to <code>authN</code>. At the heart of authentication are
credentials. All requests to the API server have to include credentials,
and the authentication layer is responsible for verifying them. If
verification fails the API server returns an HTTP 401 error and the
request is denied. If it passes it moves on to authorization.</p>
<p>The authentication layer in Kubernetes is pluggable and popular
modules include client certs, <code>webhooks</code> and integration with
external identity management systems, such as Active Directory (AD) and
cloud based Identify Access Management (IAM). In fact it is impossible
to create user accounts in Kubernetes as it does not have its own built
in identity database, instead Kubernetes forces you to use an external
system, this is great as Kube does not install yet another identity
management silo.</p>
<p>Out of the box most Kubernetes clusters support client certificates
but in the real world you will want to integrate with your chosen cloud
or corporate identity management system. Many of the hosted Kubernetes
services make it easy to integrate with their native identity management
systems.</p>
<h3 id="authentication-1">Authentication</h3>
<p>Cluster Cluster details and credentials are stored in a
<code>kubeconfig</code> file. Tools like <code>kubectl</code> read this
file to know which cluster to send commands to as well as which
credentials use, it is usually stored in the following locations:</p>
<ul>
<li>Windows: <code>C: \Users\&lt;user&gt;\.kube\config</code></li>
<li>Unix: <code>/home/&lt;user&gt;/.kube/config</code></li>
</ul>
<p>Many Kubernetes installations can automatically merge cluster
endpoint details and credentials into your existing
<code>kubeconfig</code>, for example every <code>GKE</code> cluster
provides a <code>gcloud</code> command that will merge the necessary
cluster details and credentials to your local <code>kubeconfig</code>
config file. The following is an example do not try and run it.
<code>gcloud container clusters get-credentials tkb --zone europe-west1-c --project &lt;project&gt;</code>.</p>
<p>Here is what a <code>kubeconfig</code> file looks like. As you can
see it defines a cluster and a user, combines them into a context and
sets the default context for all <code>kubectl</code> commands</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Config</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">clusters</span><span class="kw">:</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">cluster</span><span class="kw">:</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> prod-shield</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">server</span><span class="kw">:</span><span class="at"> https://&lt;url-or-ip-address-of-api-server&gt;:443</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">certificate-authority-data</span><span class="kw">:</span><span class="at"> LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0…LS0tCg==</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="fu">users</span><span class="kw">:</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> &lt;username&gt;</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">user</span><span class="kw">:</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">as-user-extra</span><span class="kw">:</span><span class="at"> </span><span class="kw">{}</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">token</span><span class="kw">:</span><span class="at"> eyJhbGciOiJSUzI1NiIsImtpZCI6IlZwMzl…SZY3uUQ</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a><span class="fu">contexts</span><span class="kw">:</span></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">context</span><span class="kw">:</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> shield-admin</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">cluster</span><span class="kw">:</span><span class="at"> prod-shield</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> default</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">user</span><span class="kw">:</span><span class="at"> &lt;username&gt;</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="fu">current-context</span><span class="kw">:</span><span class="at"> shield-admin</span></span></code></pre></div>
<p>You can see it is divided into 4 top level sections, so what is the
structure of this file:</p>
<p>The <code>clusters</code> section defines one or more Kubernetes
clusters, each one has a friendly name and API server endpoint, and the
public key of its certificate authority. The cluster in the example is
exposing the secure API endpoint on port 443, but it is also common to
see it exposed on 6443</p>
<p>The <code>users</code> section defines one or more users. Each user
requires a name and token. The token is often a X.509 encrypted value
which is user’s ID.</p>
<p>The <code>contexts</code> section combines users and clusters and the
current-context is the cluster and user <code>kubectl</code> will use
for all commands</p>
<p>Assuming the previous <code>kubeconfig</code> all
<code>kubectl</code> commands will go to the prod-shield cluster and
authenticate as the “<username>” user. The authentication module is
responsible for determining if the user really is
<code>&lt;username&gt;</code>, and if using client certificates it will
determine if the certificate is signed by a trusted CA.</p>
<h3 id="authorization">Authorization</h3>
<p>Authorization happens immediately after successful authentication and
you will sometimes see it shortened to <code>authZ</code>, Kubernetes
authorization is pluggable and you can run multiple authorization
modules on a single cluster. As soon as any of the modules authorization
requests is made, it moves on to admission control.</p>
<h4 id="access-control">Access control</h4>
<p>The most common authorization module is RBAC - role based access
control. At the highest level it is all about three things - users,
actions and resources. Which users can perform which actions against
which resources in the cluster. The following table shows a few
examples.</p>
<table>
<thead>
<tr class="header">
<th>User</th>
<th>Action</th>
<th>Resource</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bao</td>
<td>create</td>
<td>Pods</td>
</tr>
<tr class="even">
<td>Kalila</td>
<td>list</td>
<td>Deployments</td>
</tr>
<tr class="odd">
<td>Josh</td>
<td>delete</td>
<td>ServiceAccounts</td>
</tr>
</tbody>
</table>
<p>RBAC is enabled on most Kubernetes clusters and has been stable in
general availability since Kubernetes 1.8. It is a least privilege deny
by default system. This means all actions are denied by default and you
enable specific actions by creating an allow rule. In fact Kubernetes
does not support any deny rules, it only supports allow rules. This
might seem like a small thing, but it makes the Kubernetes world and
RBAC much simpler to implement and troubleshoot, and safer.</p>
<p>Two concepts are vital to understanding Kubernetes <code>RBAC</code>
- <code>Roles</code> and <code>RoleBindings</code>:</p>
<p><code>Roles</code> define a set of permissions and
<code>RoleBindings</code> grant those permissions to users. The
following resource manifest defines a Role object it is called read
deployments and grants permissions to get, watch, and list Deployments
objects in the shield namespace.</p>
<div class="sourceCode" id="cb79"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> rbac.authorization.k8s.io/v1</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Role</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> shield</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> read-deployments</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">apiGroups</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;apps&quot;</span><span class="kw">]</span></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">resources</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;deployments&quot;</span><span class="kw">]</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">verbs</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;get&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;watch&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;list&quot;</span><span class="kw">]</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> rbac.authorization.k8s.io/v1</span></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> RoleBinding</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> read-deployments</span></span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> shield</span></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a><span class="fu">subjects</span><span class="kw">:</span></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">kind</span><span class="kw">:</span><span class="at"> User</span></span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> sky</span><span class="co"> # This is the authenticated user, that was given the role</span></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">apiGroup</span><span class="kw">:</span><span class="at"> rbac.authorization.k8s.io</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a><span class="fu">roleRef</span><span class="kw">:</span></span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">kind</span><span class="kw">:</span><span class="at"> Role</span></span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> read-deployments</span><span class="co"> # This is the Role from above to bind to the user</span></span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">apiGroup</span><span class="kw">:</span><span class="at"> rbac.authorization.k8s.io</span></span></code></pre></div>
<p>If both of these are deployed to a cluster an authenticated user
called sky will be able to run commands such as
<code>kubectl get deployments -n shield</code> It is important to
understand that the username listed in the <code>RoleBindings</code> has
to be a string and has to match the username that was successfully
authenticated.</p>
<h4 id="roles-resources">Roles &amp; Resources</h4>
<p>The previous <code>Role</code> object has three properties, -
<code>apiGroups, resources and verbs</code>. Together these define which
actions are allowed against which objects, <code>apiGroups</code> and
<code>resources</code> define the object, and <code>verbs</code> define
the actions. The example allows read access (get, watch, and list)
against Deployment objects. The following table shows
<code>apiGroups</code> and resources combinations.</p>
<div class="sourceCode" id="cb80"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>apiGroup         resource     Kubernetes API path</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>””               pods         /api/v1/namespaces/{namespace}/pods</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>””               secrets      /api/v1/namespaces/{namespace}/secrets</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>“storage.k8s.io” storageclass /apis/storage.k8s.io/v1/storageclasses</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>“apps”           deployments  /apis/apps/v1/namespaces/{namespace}/deployments</span></code></pre></div>
<p>An empty set of double quotes <code>""</code> in the
<code>apiGroup</code> field indicates the core <code>apiGroup</code>,
all other api sub-groups need specifying as a string enclosed in double
quotes. The <code>apiGroup</code> is important since it namespaces the
different kubernetes resources and have to be specified for a given
resource that belongs to the given <code>apiGroup</code>, unless it is
part of the default one which is represented as already mentioned by the
empty quotes <code>""</code>. Kubernetes uses a standard set of verbs to
describe the actions a subject can perform on a resource. Verb names are
self explanatory and case sensitive, the following table lists them and
demonstrates the REST based nature of the API by showing how they map to
HTTP methods, it also lists some common HTTP response codes.</p>
<table>
<thead>
<tr class="header">
<th>HTTP method</th>
<th>Kubernetes verbs</th>
<th>Common responses</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>POST</td>
<td>create</td>
<td>201 created, 403 Access Denied</td>
</tr>
<tr class="even">
<td>GET</td>
<td>get, list and watch</td>
<td>200 OK, 403 Access Denied</td>
</tr>
<tr class="odd">
<td>PUT</td>
<td>update</td>
<td>200 OK, 403 Access Denied</td>
</tr>
<tr class="even">
<td>PATCH</td>
<td>patch</td>
<td>200 OK, 403 Access Denied</td>
</tr>
<tr class="odd">
<td>DELETE</td>
<td>delete</td>
<td>200 OK, 403 Access Denied</td>
</tr>
</tbody>
</table>
<p>The kubernetes verbs column lists the verbs you use in the rules
section of a Role object. Running the following command shows all API
resources supported on your cluster. It also shows API group and
supported verbs and is a great resource for helping build rule
definitions</p>
<div class="sourceCode" id="cb81"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl api-resources <span class="at">--sort-by</span> name <span class="at">-o</span> wide</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                       SHORTNAMES APIVERSION                   NAMESPACED KIND                      VERBS                                                       CATEGORIES</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="ex">apiservices</span>                           apiregistration.k8s.io/v1    false      APIService                create,delete,deletecollection,get,list,patch,update,watch  api-extensions</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="ex">bindings</span>                              v1                           true       Binding                   create</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="ex">certificatesigningrequests</span> csr        certificates.k8s.io/v1       false      CertificateSigningRequest create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="ex">clusterrolebindings</span>                   rbac.authorization.k8s.io/v1 false      ClusterRoleBinding        create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="ex">clusterroles</span>                          rbac.authorization.k8s.io/v1 false      ClusterRole               create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="ex">deployments</span>                deploy     apps/v1                      true       Deployment                create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="ex">ingresses</span>                  ing        networking.k8s.io/v1         true       Ingress                   create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pods</span>                       po         v1                           true       Pod                       create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="ex">replicasets</span>                rs         apps/v1                      true       ReplicaSet                create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="ex">replicationcontrollers</span>     rc         v1                           true       ReplicationController     create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="ex">resourcequotas</span>             quota      v1                           true       ResourceQuota             create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a><span class="ex">roles</span>                                 rbac.authorization.k8s.io/v1 true       Role                      create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="ex">secrets</span>                               v1                           true       Secret                    create,delete,deletecollection,get,list,patch,update,watch</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a><span class="ex">services</span>                   svc        v1                           true       Service                   create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a><span class="ex">statefulsets</span>               sts        apps/v1                      true       StatefulSet               create,delete,deletecollection,get,list,patch,update,watch  all</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a><span class="ex">storageclasses</span>             sc         storage.k8s.io/v1            false      StorageClass              create,delete,deletecollection,get,list,patch,update,watch</span></code></pre></div>
<p>This table represents the abridged version of the original, which
contains many more objects which the kubernetes environment manages by
default, the table above only lists most of the objects which were
already looked at so far, and also those being one of the most important
ones</p>
<p>Also take a note of the <code>SHORTNAMES</code> column which is an
alias for the actual resource name, all resources which define a
<code>SHORTNAME</code> can be referred by it in all contexts such as
manifest files, shell commands and so on.</p>
<p>And if we take a good look with the role manifest definition above,
we can clearly see that the rules, objects, verbs and resources match
with the table output above such as</p>
<div class="sourceCode" id="cb82"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">apiGroups</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;apps&quot;</span><span class="kw">]</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">resources</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;deployments&quot;</span><span class="kw">]</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">verbs</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;get&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;watch&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;list&quot;</span><span class="kw">]</span></span></code></pre></div>
<p>To refer to all objects one can use the asterisk (*), that would bind
the verbs to all objects which are exposed by the Kubernetes
environment, For example the following rule block grants all actions on
all resources in every API group, (basically cluster admin). It is just
for demonstration purposes and you would probably never want to do this
in the actual world, or in actual production grade Kubernetes
clusters</p>
<div class="sourceCode" id="cb83"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">apiGroups</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;*&quot;</span><span class="kw">]</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">resources</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;*&quot;</span><span class="kw">]</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">verbs</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;*&quot;</span><span class="kw">]</span></span></code></pre></div>
<h4 id="cluster-roles">Cluster roles</h4>
<p>So far we have seen how to create <code>Roles</code> and
<code>RoleBindings</code>. However, Kubernetes actually has 4 RBAC
objects,
<code>Roles, ClusterRoles, RoleBindings and ClusterRoleBindings</code>.</p>
<p><code>Roles</code> and <code>RoleBindings</code> are namespaced
objects. This means they can only be applied to a single Namespace,
<code>ClusterRole</code> and <code>ClusterRoleBindings</code> are
cluster wide objects and apply to all Namespaces. All 4 are defined in
the same API sub group and their YAML structures are almost
identical.</p>
<p>A powerful pattern is to define Roles at the cluster level and bind
them to a specific Namespace via <code>RoleBinding</code>. This lets you
define common roles once and re use them across multiple Namespaces. For
example the following YAML defines the same read-deployments role but
this time at the cluster level, this can be re-used in selected
Namespaces via a regular <code>RoleBinding</code></p>
<div class="sourceCode" id="cb84"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> rbac.authorization.k8s.io/v1</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ClusterRole</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> read-deployments</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rules</span><span class="kw">:</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">apiGroups</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;apps&quot;</span><span class="kw">]</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">resources</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;deployments&quot;</span><span class="kw">]</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">verbs</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;get&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;watch&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;list&quot;</span><span class="kw">]</span></span></code></pre></div>
<p>Look closely at the previous YAML manifest, the only difference with
the earlier one is that the kind here is defined as
<code>ClusterRole</code>, instead of a Role, and it does not have a
metadata.namespace property, since there is no meaning in namespacing
object in this case the cluster role that should be cluster wide.</p>
<h4 id="existing-resources">Existing resources</h4>
<p>As you might have figured out, there is a set of pre-defined Roles
and Bindings, granting permissions to an all powerful user. Many will
also configure <code>kubectl</code> to operated under the context of
that user. The following example walks you through the pre-defined and
pre-created user roles and bindings, on a docker desktop cluster the
names of Pods and RBAC objects will be different on other clusters, but
the principles will be the same and it gives you an idea of how things
are made to work.</p>
<p>Mnikube runs API server in Pod in the kube-system namespace. It has
an –authorization flag that tells Kubernetes which authorization modules
to use. The following command shows the node and RBAC modules are both
enabled.</p>
<div class="sourceCode" id="cb85"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get pods <span class="at">-n</span> kube-system</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                               READY   STATUS    RESTARTS      AGE</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="ex">coredns-668d6bf9bc-mtpmm</span>           1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="ex">etcd-minikube</span>                      1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-apiserver-minikube</span>            1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-controller-manager-minikube</span>   1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-proxy-z9gf9</span>                   1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="ex">kube-scheduler-minikube</span>            1/1     Running   1 <span class="er">(</span><span class="ex">23h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a><span class="ex">storage-provisioner</span>                1/1     Running   3 <span class="er">(</span><span class="ex">22h</span> ago<span class="kw">)</span>   <span class="ex">23h</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe pod/kube-apiserver-minikube <span class="at">-n</span> kube-system <span class="kw">|</span> <span class="fu">grep</span> authorization</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a><span class="ex">--authorization-mode=Node,RBAC</span></span></code></pre></div>
<p>You wont be able to interrogate the API server like this on a hosted
Kubernetes cluster, This is because critical control plane features like
this are hidden from you. <code>Minikube</code> also updates your kube
config files with the cluster called <code>minikube</code>, here is how
one config file might look like for <code>Minkube</code> that would be
located at <code>$HOME/.kube/config</code></p>
<div class="sourceCode" id="cb86"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">clusters</span><span class="kw">:</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">cluster</span><span class="kw">:</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">certificate-authority</span><span class="kw">:</span><span class="at"> /home/asmodeus/.minikube/ca.crt</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">extensions</span><span class="kw">:</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">extension</span><span class="kw">:</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">last-update</span><span class="kw">:</span><span class="at"> Sat, 08 Mar 2025 16:27:39 EET</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">provider</span><span class="kw">:</span><span class="at"> minikube.sigs.k8s.io</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">version</span><span class="kw">:</span><span class="at"> v1.35.0</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">name</span><span class="kw">:</span><span class="at"> cluster_info</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">server</span><span class="kw">:</span><span class="at"> https://127.0.0.1:60703</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> minikube</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="fu">contexts</span><span class="kw">:</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">context</span><span class="kw">:</span></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">cluster</span><span class="kw">:</span><span class="at"> minikube</span></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">extensions</span><span class="kw">:</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">extension</span><span class="kw">:</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">last-update</span><span class="kw">:</span><span class="at"> Sat, 08 Mar 2025 16:27:39 EET</span></span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">provider</span><span class="kw">:</span><span class="at"> minikube.sigs.k8s.io</span></span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a><span class="at">                    </span><span class="fu">version</span><span class="kw">:</span><span class="at"> v1.35.0</span></span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">name</span><span class="kw">:</span><span class="at"> context_info</span></span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> default</span></span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">user</span><span class="kw">:</span><span class="at"> minikube</span></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> minikube</span></span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a><span class="fu">current-context</span><span class="kw">:</span><span class="at"> minikube</span></span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Config</span></span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a><span class="fu">preferences</span><span class="kw">:</span><span class="at"> </span><span class="kw">{}</span></span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a><span class="fu">users</span><span class="kw">:</span></span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> minikube</span><span class="co"> # Here is the name of the user that is created</span></span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">user</span><span class="kw">:</span></span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">client-certificate</span><span class="kw">:</span><span class="at"> /home/asmodeus/.minikube/profiles/minikube/client.crt</span><span class="co"> # client credentials</span></span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">client-key</span><span class="kw">:</span><span class="at"> /home/asmodeus/.minikube/profiles/minikube/client.key</span><span class="co"> # client credentials</span></span></code></pre></div>
<p>Note that we can also see the client’s certificate and the private
key, by client that would imply the client that interfaces with the API
server, in our case that is <code>kubectl</code>, this is mandatory to
be able to contact the API server, especially on non local cluster
deployments, this provides the authorization step, in this case this
allows the API server to establish that the client instance connecting
to it is authorized to talk to it</p>
<p><code>RABC</code> is enabled and a user <code>kubeconfig</code> is
created, the user is called <code>minikube</code>, now let us take a
look at the cluster role and the cluster role bindings, objects which
are pre-configured to grant permissions to that user. Here are some of
the cluster role bindings which are bound to the
<code>ClusterRole</code> named <code>cluster-admin</code>, these are not
all.</p>
<div class="sourceCode" id="cb87"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl get clusterrolebindings <span class="kw">|</span> <span class="fu">grep</span> cluster-admin</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                                                            ROLE                                                                               AGE</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="ex">cluster-admin</span>                                                   ClusterRole/cluster-admin                                                          23h</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="ex">minikube-rbac</span>                                                   ClusterRole/cluster-admin                                                          23h</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe clusterrolebindings minikube-rbac</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         minikube-rbac</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  <span class="op">&lt;</span>none<span class="op">&gt;</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Role:</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Kind:</span>  ClusterRole <span class="co"># the cluster role type</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Name:</span>  cluster-admin <span class="co"># the name of the cluster role</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Subjects:</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Kind</span>            Name     Namespace</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">----</span>            <span class="at">----</span>     <span class="at">---------</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ServiceAccount</span>  default  kube-system</span></code></pre></div>
<p>As a result of these bindings all commands in a default on premise
<code>minikube</code> installation are executed with the cluster-admin
permissions. This might be OK for development environments and on
premise testing environments, but certainly not okay for production
ones. How let us see what exactly is defined for this cluster role in
its manifest file</p>
<div class="sourceCode" id="cb88"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe clusterrole cluster-admin</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Name:</span>         cluster-admin</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Labels:</span>       kubernetes.io/bootstrapping=rbac-defaults</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Annotations:</span>  rbac.authorization.kubernetes.io/autoupdate: true</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="ex">PolicyRule:</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Resources</span>  Non-Resource URLs  Resource Names  Verbs</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">---------</span>  <span class="at">-----------------</span>  <span class="at">--------------</span>  <span class="at">-----</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">*.*</span>        []                 []              <span class="pp">[</span><span class="ss">*</span><span class="pp">]</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>             <span class="ex">[*]</span>                []              <span class="pp">[</span><span class="ss">*</span><span class="pp">]</span></span></code></pre></div>
<p>As you can see it allowed everything, for every resource and every
verb or action that can be performed on the cluster, which is normal for
that type of environment as mentioned already.</p>
<p><code>Authorization ensures that already authenticated users are allowed to carry out the actions they are attempting. RBAC is a popular Kubernetes authorization module and implements least privilege access based on a deny by default model where all actions are assumed to be denied unless a rule exists that allows it, The model is similar to a whitelist firewall where everything is blocked and you open up access by creating allow rules. Kubernetes RBAC uses Roles and ClusterRole to create permissions and it uses RoleBinding and ClusterRoleBinding to grant those permissions to users, in other words to bind them to the users, these bindings objects serve as a middle man to allow us to create different permutations and combinations of role objects to grant to users, this way we can achieve the most granular access granting and permission model</code></p>
<h3 id="admission-control">Admission control</h3>
<p>Admission control runs immediately after successful authentication
and authorization, and it is all about policies there two types of
admission controllers - Mutating and Validating. The names are self
explanatory, Mutating controllers check for compliance and can modify
requests whereas validating controllers check for policy compliance but
cannot modify requests Mutating controllers always run first and both
types only apply to requests that will modify state. Requests to read
state are not subjected to admission control.</p>
<p>Assume a quick example where all new and updated objects to your
cluster must have the env-prod label. A mutating controller can check
for the presence of the label and add it if it does not exist, on the
flip side a validating controller can only reject the request if it does
not exist. The following command on a <code>minikube</code> cluster
shows the API server is configured to use the
<code>NodeRestriction</code> admission controller, amongst others</p>
<div class="sourceCode" id="cb89"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl describe pod/kube-apiserver-minikube <span class="at">-n</span> kube-system <span class="kw">|</span> <span class="fu">grep</span> admission</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="ex">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</span></span></code></pre></div>
<p>Most real world clusters will have a lot more admission controllers
enabled. There are lots of admission controllers but the
<code>AlwaysPUllImage</code> controller is a great example. It is a
mutating controller that sets the
<code>spec.containers.imagePullPolicy</code> of all new Pods to always.
This means that images for all containers in all Pods will always be
pulled form the registry. This accomplishes quite a few things,
including the following</p>
<ul>
<li>Preventing the use of locally cached images that could be
malicious</li>
<li>Preventing other Pods and processes using locally cached images</li>
<li>Forcing the container runtime to present valid credentials to the
registry to get the image</li>
</ul>
<p>If any admission controller rejects a request the request is
immediately rejected without checking other admission controllers.
However if all admission controllers approve the request it gets
persisted to the cluster store, As previously mentioned there are lots
of admission controllers and they are becoming more and more important
in real world production clusters</p>
<h3 id="general-summary">General Summary</h3>
<p>The authentication layer is responsible for validating the identity
of the request. Client certificates are commonly used to integration
with <code>AD</code> and other <code>IAM</code> services is recommended
for production clusters. Kubernetes does not have its own identity
database, meaning it does not store or manage user accounts or
credentials.</p>
<p>The authorization layer checks whether the authenticated user is
authorized to carry out the action in the request. This layer is also
pluggable and the most common module is RBAC. The RBAC module comprises
4 objects that define permissions and assigns them to the users.</p>
<p>Admission control, kicks in after the client/user is authenticated
and authorized and is responsible for enforcing policies. Validating
admission controller reject request if htey do not conform to the
policy, where as mutating admissions controllers can modify the incoming
request to the API server, in other words to mutate the incoming object
manifest to enhance or override the structure of the document according
to some policies,</p>
<p>This section will describe a few fast and quick ways to obtain
Kubernetes. Will also introduce you to <code>kubectl</code>, the
Kubernetes command line tool.</p>
<h2 id="kubernetes-api">Kubernetes API</h2>
<p>Understanding the Kubernetes API, and how it works is vital to
mastery of Kubernetes. However it can be extremely confusing, if you are
new to the API.</p>
<h3 id="theory-6">Theory</h3>
<p>As already seen Kubernetes is API centric, the entire functionality
is based around interacting with the API server, through the user level
client, in this case <code>kubectl</code>. This means that everything in
Kubernetes is about the API. And everything goes through the API server.
We will get into the details soon, but for now let us just look at the
big picture.</p>
<p>Clients <code>send</code> request to kubernetes to create, read,
update and delete objects such as Pods and Services and so on. For the
most part you will use <code>kubectl</code> to send these requests
however, you can craft them in code or use the API testing and
development tools to generate them The point is no matter how you
generate requests they go to the API sever where they are authenticated
and authorized, assuming they pass these checks they are executed on the
cluster. If a create request is posted, the object is deployed to the
cluster and the serialized state of it is persisted to the cluster store
(etcd).</p>
<h3 id="serialization">Serialization</h3>
<p>Kubernetes serializes objects such a Pods and Services, as JSON
strings to be send over HTTP. The process happens in both directions
with clients like <code>kubectl</code> serializing the object when
posting to the api server and the api server serializing the response
back to clients. In the case of Kubernetes the serialized state of
objects are also persisted to the cluster store which is usually based
on the etcd database service</p>
<p>So in Kubernetes serialization is the process of converting an object
to into a JSON string to be sent over an HTTP connection and persisted
to the cluster store. However as well as JSON Kubernetes also supports
Protobuf as a serialization schema. This is faster and moreefficient and
scales better than JSON. But it is not user friendly, when it comes to
introspection and troubleshooting. At the time of writing Protobuf is
mainly sued for internal cluster traffic, whereas son is used when
communicating with external clients.</p>
<p>One final thing on serialization, when clients send requests to the
API server they use the content type head to list the serialization
schema they support. For example a client that only supports JSON will
specify <code>Content-Type: application/json</code> in the HTTP header
of the request. Kubernetes will honour this with a serialized response
in JSON</p>
<h3 id="the-api-server-1">The API server</h3>
<p>The API server exposes the API over a secure RESTful, interface using
HTTPS. It acts as the front end to the API and is a bit like Grand
Central for Kubernetes everything talks to everything else via the REST
API calls to the API server.</p>
<ul>
<li>ALL <code>kubectl</code> commands go to the API server (creating,
retrieving, updating deleting objects)</li>
<li>ALL node <code>kubelets</code> watch the API server for new tasks
and report status to the API server</li>
<li>ALL control lane services communicate via the API server, NOT
directly to each other</li>
</ul>
<p>The API server is a Kubernetes control plane service. This usually
means that it runs as a set of Pods in the kube-system Namespace on the
control plane nodes of your cluster. If you build and manage your own
Kubernetes clusters you need to make sure the control plane is highly
available and has enough performance to keep the API server up and
running and responding quickly to requests. If you are using the hosted
Kubernetes cluster the way the API server is implemented including the
performance and availability will be hidden away from you.</p>
<p>The main job of the API server is to make API available to clients
inside and outside the cluster. It uses TLS to encrypt the client
connection and i leverages of a bunch o authentication and authorization
mechanisms to ensure only valid requests are accepted and actioned upon.
Requests from internal and external sources all have to pass through the
same authentication and authorization.</p>
<p>The API is RESTful. This is a jargon for a modern web API that
accepts CRUD style requests via standard HTTPS methods. Every standard
HTTP method like PUT, DELETE, UPDATE, POST can be mapped to a
corresponding verb in the Kubernetes world as far as role verbs are
concerned.</p>
<p>It is common for the API server to be exposed on port 443 or 6443 but
its possible to configure it to operate on whatever port you require,
running the following command shows the address and port of your
Kubernetes cluster is exposed on</p>
<div class="sourceCode" id="cb90"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl cluster-info</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Kubernetes</span> control plane is running at https://127.0.0.1:60703</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="ex">CoreDNS</span> is running at https://127.0.0.1:60703/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> further debug and diagnose cluster problems, use <span class="st">&#39;kubectl cluster-info dump&#39;</span>.</span></code></pre></div>
<p>A few words on how the RESTful nature of the client maps to the
KUBERNETES API server. REST is short for -
<code>REpresentational State Transfer</code> and its the de-facto
standard for communicating with web based API. System such as Kubernetes
that use REST are often referred to as RESTful. REST requests comprise a
verb and a path to a resource. Verbs related to actions and are the
standard HTTP methods you saw in the previous chapters. The following
example shows a <code>kubectl</code> command and the associated REST
path that will list all pods in the shield namespace.</p>
<div class="sourceCode" id="cb91"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get pods <span class="at">--namespace</span> shield</span></code></pre></div>
<pre class="http"><code>GET /api/v1/namespaces/shield/pods</code></pre>
<p>To visualise this, start a <code>kubectl</code> proxy and use curl to
generate the request. The <code>kubectl</code> proxy command exposes the
api on your localhost adapter and takes care of the authentication
process. You can use a different port.</p>
<div class="sourceCode" id="cb93"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl proxy <span class="at">--port</span> 9000 <span class="kw">&amp;</span> <span class="co"># start the proxy process in the background</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> curl http://localhost:9000/api/v1/namespaces/shield/pods</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="kw">{</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;kind&quot;</span><span class="ex">:</span> <span class="st">&quot;PodList&quot;</span>,</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;apiVersion&quot;</span><span class="ex">:</span> <span class="st">&quot;v1&quot;</span>,</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;metadata&quot;</span><span class="ex">:</span> {</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;resourceVersion&quot;</span><span class="ex">:</span> <span class="st">&quot;484812&quot;</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">},</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;items&quot;</span><span class="ex">:</span> []</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="kw">}</span></span></code></pre></div>
<p>The example returned an empty list because there are no pods in that
namespace. Responses from the API server include common HTTP response
codes content type and the actual payload, as you learned earlier the
chapter, Kubernetes uses JSON as its preferred content type, as a result
the previous <code>kubectl</code> get command will result in an HTTP 200
(OK) response code, the content type will be application/json and the
payload will be serialized to JSON, list all Pods in the shield
Namespace, Run one of the previous curl commands again but with the flag
-v - which sends for verbose, and that will list the headers which are
being sent and received from and to the API server.</p>
<div class="sourceCode" id="cb94"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> curl <span class="at">-v</span> http://localhost:9000/api/v1/namespaces/shield/pods</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> GET <span class="ex">/api/v1/namespaces/shield/pods</span> HTTP/1.1</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span> Accept: <span class="ex">*/*</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span> HTTP/1.1 <span class="ex">200</span> OK</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span> Content-Type: <span class="ex">application/json</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span> X-Kubernetes-Pf-Flowschema-Uid: <span class="ex">499d0001-d874-4b06-ba...c37f7</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span> X-Kubernetes-Pf-Prioritylevel-Uid: <span class="ex">aeb490e6-1890-41ab...94e82</span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="kw">{</span></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;kind&quot;</span><span class="ex">:</span> <span class="st">&quot;PodList&quot;</span>,</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;apiVersion&quot;</span><span class="ex">:</span> <span class="st">&quot;v1&quot;</span>,</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;metadata&quot;</span><span class="ex">:</span> {</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;resourceVersion&quot;</span><span class="ex">:</span> <span class="st">&quot;487845&quot;</span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>    <span class="ex">},</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;items&quot;</span><span class="ex">:</span> []</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="kw">}</span></span></code></pre></div>
<h3 id="the-api">The API</h3>
<p>The API is where all Kubernetes resources are defined, it is large,
modular. When Kubernetes was originally created teh API was monolithic
in design with all resources existing in a single global namespace,
however as kubernetes grew it became necessary to divide the API into
smaller more manageable groups, these are
<code>core, apps, rbac and netowrking</code></p>
<h3 id="the-core">The core</h3>
<p>The resources in the core group are mature objects that were created
in the early days of Kubernetes before the API was divided into groups.
They tend to be fundamental objects such as Pods, Nodes, Services,
Deployments, Secrets and so on. The are located in the API below /api/v1
(now it should be clear what /v1 refers to, the core api, the one that
Kubernetes started with). The following table lists some example paths
for resources in the core group, you will sometimes see and hear these
paths referred to as REST path.</p>
<table>
<thead>
<tr class="header">
<th>Resources</th>
<th>Path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pods</td>
<td>/api/v1/namespaces/{namespace}/pods/</td>
</tr>
<tr class="even">
<td>Services</td>
<td>/api/v1/namespaces/{namespace}/services/</td>
</tr>
<tr class="odd">
<td>Nodes</td>
<td>/api/v1/nodes/</td>
</tr>
<tr class="even">
<td>Namespaces</td>
<td>/api/v1/namespaces/</td>
</tr>
</tbody>
</table>
<p>Notice that some objects are namespaced and some are not Namespaced
objects have a longer REST paths as you have to include two additional
name segments, For example listing all pods in the shield namespace
requires the following path</p>
<pre class="http"><code>GET /api/v1/namespaces/shield/pods/</code></pre>
<p>Expected HTTP response code from read requests are either 200, or
401. On the topic of these rest paths, GVR stand for
group-version-resource and can be a good way to remember the structure
of the REST paths, in the Kubernetes API</p>
<h3 id="named-groups">Named Groups</h3>
<p>The named API groups are the future of the API, these all new
resources go into named groups, Sometimes we refer to them as
sub-groups. Each of the named groups is a collection of related
resources. For example the apps groups is where all resources that
manage apps workloads such as
<code>Deployments ReplicaSets, DaemonSets, StatefulSets</code> are
defined. Likewise, the <code>networking.k8s.io</code> group is where
<code>Ingresses and Ingress Classes</code>, Networking policies and
other network related resources exist, notable exceptions to this
pattern are older resources in the core groups that came along
afterwards, like Pods were they invented today, would probably go in the
apps groups, and Services would go in the networking.k8s.io, group.
Resources in the named groups live below the
/apis/{group-name}/{version}/path. The following table lists some
examples</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>Resource</th>
<th>Path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ingress</td>
<td>/apis/networking.k8s.io/v1/namespaces/{namespace}/ingresses/</td>
</tr>
<tr class="even">
<td>RoleBinding</td>
<td>/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/</td>
</tr>
<tr class="odd">
<td>ClusterRole</td>
<td>/apis/rbac.authorization.k8s.io/v1/clusterroles/</td>
</tr>
<tr class="even">
<td>StorageClass</td>
<td>/apis/storage.k8s.io/v1/storageclasses/</td>
</tr>
</tbody>
</table>
<p>Notice how the URI paths for named groups start with /apis and
include the name of the groups. This is different to the core group that
starts with /api in the singular and does not include a group name, in
fact in places you will see the core API groups referred to by empty
double quotes. This is because when the API was first designed no
thought was given to groups - everything was just in the API. Dividing
the API into smaller groups makes it more scalable and easier to
navigate it also makes it easier to extend. The following command are
good for seeing API related info for your cluster.
<code>kubectl api-resources</code> is great for seeing which resources
are available on your cluster as well on which API groups they are
served from. It also shows resource short names and whether objects are
namespaced or cluster scoped. The command
<code>kubectl api-versions</code> will on the other hand show you what
API versions are supported on your cluster, it does not list which
resources belong to which API it is good for finding out whether you
have things such as alpha, APIS enabled or not.</p>
<div class="sourceCode" id="cb96"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl api-resources</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="ex">NAME</span>                              SHORTNAMES                      APIVERSION              NAMESPACED KIND</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="ex">bindings</span>                          v1                              true                    Binding</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="ex">----------------------------------------------------------------------------------------------------------------------</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="ex">configmaps</span>                        cm                              v1                      true       ConfigMap</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="ex">endpoints</span>                         ep                              v1                      true       Endpoints</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="ex">events</span>                            ev                              v1                      true       Event</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="ex">limitranges</span>                       limits                          v1                      true       LimitRange</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="ex">namespaces</span>                        ns                              v1                      false      Namespace</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="ex">nodes</span>                             no                              v1                      false      Node</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a><span class="ex">persistentvolumeclaims</span>            pvc                             v1                      true       PersistentVolumeClaim</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="ex">persistentvolumes</span>                 pv                              v1                      false      PersistentVolume</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="ex">pods</span>                              po                              v1                      true       Pod</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="ex">podtemplates</span>                      v1                              true                    PodTemplate</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a><span class="ex">replicationcontrollers</span>            rc                              v1                      true       ReplicationController</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a><span class="ex">resourcequotas</span>                    quota                           v1                      true       ResourceQuota</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a><span class="ex">secrets</span>                           v1                              true                    Secret</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a><span class="ex">serviceaccounts</span>                   sa                              v1                      true       ServiceAccount</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a><span class="ex">services</span>                          svc                             v1                      true       Service</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a><span class="ex">----------------------------------------------------------------------------------------------------------------------</span></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a><span class="ex">ingressclasses</span>                    networking.k8s.io/v1            false                   IngressClass</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a><span class="ex">poddisruptionbudgets</span>              pdb                             policy/v1               true       PodDisruptionBudget</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a><span class="ex">clusterrolebindings</span>               rbac.authorization.k8s.io/v1    false                   ClusterRoleBinding</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a><span class="ex">clusterroles</span>                      rbac.authorization.k8s.io/v1    false                   ClusterRole</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a><span class="ex">rolebindings</span>                      rbac.authorization.k8s.io/v1    true                    RoleBinding</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a><span class="ex">roles</span>                             rbac.authorization.k8s.io/v1    true                    Role</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a><span class="ex">priorityclasses</span>                   pc                              scheduling.k8s.io/v1    false      PriorityClass</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a><span class="ex">csidrivers</span>                        storage.k8s.io/v1               false                   CSIDriver</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a><span class="ex">csinodes</span>                          storage.k8s.io/v1               false                   CSINode</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a><span class="ex">csistoragecapacities</span>              storage.k8s.io/v1               true                    CSIStorageCapacity</span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a><span class="ex">storageclasses</span>                    sc                              storage.k8s.io/v1       false      StorageClass</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a><span class="ex">volumeattachments</span>                 storage.k8s.io/v1               false                   VolumeAttachment</span></code></pre></div>
<div class="sourceCode" id="cb97"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> kubectl api-versions</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="ex">admissionregistration.k8s.io/v1</span></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="ex">apiextensions.k8s.io/v1</span></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="ex">apiregistration.k8s.io/v1</span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="ex">apps/v1</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="ex">authentication.k8s.io/v1</span></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="ex">authorization.k8s.io/v1</span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="ex">autoscaling/v1</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="ex">autoscaling/v2</span></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="ex">batch/v1</span></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a><span class="ex">certificates.k8s.io/v1</span></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a><span class="ex">coordination.k8s.io/v1</span></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a><span class="ex">discovery.k8s.io/v1</span></span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a><span class="ex">events.k8s.io/v1</span></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a><span class="ex">flowcontrol.apiserver.k8s.io/v1</span></span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a><span class="ex">networking.k8s.io/v1</span></span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a><span class="ex">node.k8s.io/v1</span></span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a><span class="ex">policy/v1</span></span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a><span class="ex">rbac.authorization.k8s.io/v1</span></span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a><span class="ex">scheduling.k8s.io/v1</span></span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a><span class="ex">storage.k8s.io/v1</span></span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a><span class="ex">v1</span></span></code></pre></div>
<p>This one command can be quote useful when one would like to inspect
the different kind and version fields of api resources, supported on
your cluster, the output is trimmed but it should give the general idea
of what one might receive as output</p>
<div class="sourceCode" id="cb98"><pre
class="sourceCode sh"><code class="sourceCode bash"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> for kind in <span class="kw">`</span><span class="ex">kubectl</span> api-resources <span class="kw">|</span> <span class="fu">tail</span> +2 <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{ print $1 }&#39;</span><span class="kw">`;</span> <span class="dt">\</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>            <span class="cf">do</span> <span class="ex">kubectl</span> explain <span class="va">$kind</span><span class="kw">;</span> <span class="cf">done</span> <span class="kw">|</span> <span class="fu">grep</span> <span class="at">-e</span> <span class="st">&quot;KIND:&quot;</span> <span class="at">-e</span> <span class="st">&quot;VERSION:&quot;</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="ex">KIND:</span>    Namespace</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="ex">VERSION:</span> v1</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="ex">KIND:</span>    Node</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="ex">VERSION:</span> v1</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="ex">--------------------------------</span></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="ex">KIND:</span>    HorizontalPodAutoscaler</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a><span class="ex">VERSION:</span> autoscaling/v1</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a><span class="ex">KIND:</span>    CronJob</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a><span class="ex">VERSION:</span> batch/v1beta1</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a><span class="ex">KIND:</span>    Job</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a><span class="ex">VERSION:</span> batch/v1</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a><span class="ex">--------------------------------</span></span></code></pre></div>
<p>The command could be used on actual self hosted and no premise hosted
Kubernetes cluster and services. This could turn out to be useful for
debugging or auguring information on resources and their source API</p>
<h2 id="thread-modeling">Thread modeling</h2>
<p>Thread modeling is the process of identifying vulnerabilities so you
can put measures in place to prevent and mitigate them. This section
will introduce the popular <code>STRIDE</code> model and shows how it
can applied to Kubernetes. <code>STRIDE</code> defines the six
categories of potential thread. The word <code>STRIDE</code> is an
abbreviation which stands for the following:</p>
<ul>
<li><code>S</code>poofing</li>
<li><code>T</code>ampering</li>
<li><code>R</code>epudiation</li>
<li><code>I</code>nformation disclosure</li>
<li><code>D</code>enial of service</li>
<li><code>E</code>levation of privilege</li>
</ul>
<p>While the model is good it is important to keep in mind that it is
just a model, and models do not guarantee to cover all possible threats
possible. However they are a good at providing a structured way to look
at things, for the rest of the section, we will take a look at each of
the six thread categories in turn, and how we can prevent and mitigate
them.</p>
<p>In 2019 the <code>CNCF</code> (Cloud Native Computing Foundation)
commissioned a third party security audit of Kubernetes. There were
several findings including threat modeling manual code reviews, dynamic
penetration testing and cryptography review. All findings were given a
difficulty and severity level, and all high severity</p>
<h3 id="spoofing">Spoofing</h3>
<p>Spoofing is pretending to be somebody else with the aim of gaining
extra privilege on a system. Kubernetes is comprised of lots of small
components that work together, these include control plane service such
as the API server, controller manager scheduler cluster store, and
others. It also includes Node components such as the kubelet and
container runtime. Each of these has its own set of privileges that
allow it to interact with and even modify the cluster, even though
Kubernetes implements a least privilege model, spoofing the identity of
any of these can cause problems</p>
<p>If you read the RBAC and API security section, you will know that
Kubernetes requires all components to authenticate, via a
cryptographically signed certificates. This is good and Kubernetes makes
it easy to auto rotate certificates and the likes. However it is vital
you consider the following.</p>
<ol type="1">
<li><p>A typical Kubernetes installation will auto generate a self
signed certificate authority (CA). This is the CA that will issue
certificates to all cluster components. And while its better than
nothing on this own its probably not enough for production
environments</p></li>
<li><p>Mutual TLS, is only as secure as the CA issuing the certificates,
compromising the CA can render the entire <code>mTLS</code> layer
ineffective, so keep the CA secure.</p></li>
</ol>
<p>A good practice is to ensure that certificates issued by the internal
Kubernetes CA are only used and trusted within the Kubernetes cluster.
This requires careful approval of certificate signing requests and you
need to make sure the Kubernetes CA does not get added as a trusted CA
for any system outside of Kubernetes</p>
<p>As mentioned in previous sections all internal and external requests
to the API server are subject to authentication and authorization
checks, as a result in API server needs a way to authenticate internal
and external sources, a good way to do this is having two trusted key
pairs:</p>
<ul>
<li>one for authenticating internal systems</li>
<li>the other for authenticating external systems</li>
</ul>
<p>In this model you would use the cluster’s self signed CA to issue
keys to internal systems, you would also configure Kubernetes to trust
one or more trusted 3rd party CAs to issue keys to external systems.</p>
<p>As well as spoofing access to the cluster there is also the threat of
spoofing an app for app-to-app communications. This is when one Pod
spoofs another. Fortunately you can leverage Secrets to mount
certificates into the Pods that are used to authenticate Pod
identity.</p>
<p>While on the topic of Pods every Pod has an associated
<code>ServiceAccount</code> that is used to provide an identity, for the
Pod within the cluster. This is achieved by automatically mounting a
service account token into every Pod as a Secret. Two points to
note:</p>
<ol type="1">
<li>The service account token allows access to the API server</li>
<li>Most Pods probably do not need to access the API server</li>
</ol>
<p>With these two points in mind, it is often recommended to set
<code>automountServiceAccountToken</code> to false for Pods that you
know do not need to communicate wit the API server. The following Pod
manifest shows how to do this. ###</p>
<div class="sourceCode" id="cb99"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> service-account-example-pod</span></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">serviceAccountName</span><span class="kw">:</span><span class="at"> some-service-account</span></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">automountServiceAccountToken</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="at">&lt;Snip&gt;</span></span></code></pre></div>
<p>If the Pod does need to talk to the API server, the following non
default configurations are worth exploring. -
<code>expiratoinSeconds</code> and <code>audience</code>. These two let
you force a time when the token will expire as well as restrict the
entities in works with. The following example inspired from official
Kubernetes docs sets and expiry period of one hour and restricts it to
the vault audience in a projected volume.</p>
<div class="sourceCode" id="cb100"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">image</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /var/run/secrets/tokens</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">name</span><span class="kw">:</span><span class="at"> vault-token</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">serviceAccountName</span><span class="kw">:</span><span class="at"> my-pod</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> vault-token</span></span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">projected</span><span class="kw">:</span></span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">sources</span><span class="kw">:</span></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="kw">-</span><span class="at"> </span><span class="fu">serviceAccountToken</span><span class="kw">:</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">path</span><span class="kw">:</span><span class="at"> vault-token</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">expirationSeconds</span><span class="kw">:</span><span class="at"> </span><span class="dv">3600</span></span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a><span class="at">                </span><span class="fu">audience</span><span class="kw">:</span><span class="at"> vault</span></span></code></pre></div>
<h3 id="tampering">Tampering</h3>
<p>Tampering is the act of changing something in a malicious way so you
can cause one of the following, denial of service, and elevation of
privilege. Tampering can hard to avoid so a common counter measure is to
make it obvious when something has been tampered with. A common example
outside of information security is packaging medication. Most over the
counter drugs are packaged with tamper proof seals. jthese make it easy
to see if the product has been tampered with. Let us have a quick look
at some of the cluster components that can be tampered with.</p>
<h4 id="kubernetes-components">Kubernetes components</h4>
<p>All of the following Kubernetes components, if tampered with can
cause harm or issues - <code>etcd</code>, configuration files, container
runtime binaries, container images, kubernetes binaries and more.
Generally speaking tampering happens either in transit or at rest. In
transit refers to data while it is being transmitted over the network,
where as at rest refers to data stored in memory or on disk. TLS is a
great tool for protecting against in transit tampering, as it provides
built in integrity guarantees you will be warned if the data has been
tampered with. The following recommendations can also help prevent
tampering with data when it is at rest in Kubernetes</p>
<p>. Restrict access to the servers that are running Kubernetes
components i.e the control plane.</p>
<ul>
<li>Restrict access to repositories that store Kubernetes configuration
files</li>
<li>Only perform remote bootstrapping over SSH</li>
<li>Restrict access to your image repository and associated
repositories</li>
</ul>
<p>This is not and exhaustive list, but if you implement it you will
greatly reduce the changes of having your data tampered with while at
rest. As well as the items listed it is goog production hygiene to
configure auditing and alerting for important binaries and config files.
If configured and monitored correctly these can help detect potential
tampering attacks. The following example uses a common Linux audit
daemon to audit access to the docker binary it also audits attempts to
the change the binary file attributes</p>
<p><code>sh $ auditctl -w /usr/bin/docker -p wxa -k audit-docker</code></p>
<h4 id="kubernetes-applications">Kubernetes applications</h4>
<p>As well as infrastructure components, apps components are also
potential tampering targets. A good way to prevent live Pod from being
tampered with is setting its filesystems to read only. This guarantees
filesystem immutability and can be accomplished through a Pod Security
Policy or the <code>securityContext</code> section of a Pod manifest
file.</p>
<p>You can make a container root filesystem read only by setting the
<code>readOnlyFilesystem</code> property, As previously mentioned, this
can be set via a <code>PodSecurityPolicy</code>, object or in Pod
manifest files. The same can be done for other filesystems that are
mounted into containers, via the <code>allowedHostPaths</code>
property</p>
<p>The following YAML manifest shows how to use both settings in a Pod
manifest spec, the <code>allowedHostPaths</code> section makes sure
anything mounted beneath /test will be read only. The two examples
represent the same functionality, one is simply a separate object
represented by the <code>PodSecurityPolicy</code>, which is relatively
new in the Kubernetes spec and that is why it is not under
<code>v1</code>, the stable <code>apiVersion</code>, but rather under a
new beta version api path <code>policy/v1beta1</code>. In the future
when the spec of that object is cleaned up, it will be put under the
<code>policy/v1</code> <code>apiVersion</code> path</p>
<div class="sourceCode" id="cb101"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> readonly-test</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">readOnlyRootFilesystem</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">allowedHostPaths</span><span class="kw">:</span></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="kw">-</span><span class="at"> </span><span class="fu">pathPrefix</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;/test&quot;</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">readOnly</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> policy/v1beta1</span><span class="co"> # Will change in a future versions, when the object spec gets out of the beta stage</span></span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PodSecurityPolicy</span></span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tampering-example</span></span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">readOnlyRootFilesystem</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">allowedHostPaths</span><span class="kw">:</span></span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">pathPrefix</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;/test&quot;</span></span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">readOnly</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code></pre></div>
<h3 id="repudiation">Repudiation</h3>
<p>At a very high level Repudiation is creating doubt about something.
Non Repudiation is providing proof about something. In the context of
information security non Repudiation is proving certain actions were
carried out by certain individuals. Digging a little deeper
non-repudiation includes the ability to provide</p>
<ul>
<li><code>What</code> happened</li>
<li><code>When</code> it happened</li>
<li><code>Who</code> made it happen</li>
<li><code>Where</code> is happened</li>
<li><code>Why</code> is happened</li>
<li><code>How</code> it happened</li>
</ul>
<p>Answering the last two usually requires the correlation of several
events over a period of time fortunately auditing of Kubernetes API
server events can usually help answer these questions the Following is
an example of an API server audit event (you may need to manually enable
auditing on your API server).</p>
<div class="sourceCode" id="cb102"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;kind&quot;</span><span class="fu">:</span> <span class="st">&quot;Event&quot;</span><span class="fu">,</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;apiVersion&quot;</span><span class="fu">:</span> <span class="st">&quot;audit.k8s.io/v1&quot;</span><span class="fu">,</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;metadata&quot;</span><span class="fu">:</span> <span class="fu">{</span> <span class="dt">&quot;creationTimestamp&quot;</span><span class="fu">:</span> <span class="st">&quot;2020-03-03T10:10:00Z&quot;</span> <span class="fu">},</span></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;level&quot;</span><span class="fu">:</span> <span class="st">&quot;Metadata&quot;</span><span class="fu">,</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;timestamp&quot;</span><span class="fu">:</span> <span class="st">&quot;2020-03-03T10:10:00Z&quot;</span><span class="fu">,</span></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;auditID&quot;</span><span class="fu">:</span> <span class="st">&quot;7e0cbccf-8d8a-4f5f-aefb-60b8af2d2ad5&quot;</span><span class="fu">,</span></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;stage&quot;</span><span class="fu">:</span> <span class="st">&quot;RequestReceived&quot;</span><span class="fu">,</span></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;requestURI&quot;</span><span class="fu">:</span> <span class="st">&quot;/api/v1/namespaces/default/persistentvolumeclaims&quot;</span><span class="fu">,</span></span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;verb&quot;</span><span class="fu">:</span> <span class="st">&quot;list&quot;</span><span class="fu">,</span></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;user&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;username&quot;</span><span class="fu">:</span> <span class="st">&quot;fname.lname@example.com&quot;</span><span class="fu">,</span></span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;groups&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;system:authenticated&quot;</span><span class="ot">]</span></span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">},</span></span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;sourceIPs&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;123.45.67.123&quot;</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;objectRef&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;resource&quot;</span><span class="fu">:</span> <span class="st">&quot;persistentvolumeclaims&quot;</span><span class="fu">,</span></span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;namespace&quot;</span><span class="fu">:</span> <span class="st">&quot;default&quot;</span><span class="fu">,</span></span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a>        <span class="dt">&quot;apiVersion&quot;</span><span class="fu">:</span> <span class="st">&quot;v1&quot;</span></span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">},</span></span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;requestReceivedTimestamp&quot;</span><span class="fu">:</span> <span class="st">&quot;2010-03-03T10:10:00.123456Z&quot;</span><span class="fu">,</span></span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;stageTimestamp&quot;</span><span class="fu">:</span> <span class="st">&quot;2020-03-03T10:10:00.123456Z&quot;</span></span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div>
<p>Although the API server is central to most things in Kubernetes it is
not the only components that requires auditing for non repudiation, at a
minimum you should collect audit logs from container runtimes, kubelets
and the apps running on your cluster. This is without even mentioning
network firewalls and the likes.</p>
<p>Once you start auditing multiple components you quickly need a
centralized location to store and correlate events, a common way to do
this is deploying an agent to all nodes via a <code>DeamonSet</code>.
The agent collects logs (runtime, kubelet, apps…)</p>
<p>If you do this it is vital the centralized log store is secure, if
the security of the store is compromised you can no longer trust the
logs, and their contents can be repudiated. To provide non repudiation
relative to tampering with binaries and configuration files it might be
useful to use an audit daemon that watches for write actions on certain
files and directories on your Kubernetes Masters and Nodes. For example
earlier in the chapter you saw an example that enabled auditing of
changes to the docker binary, with this enabled starting a new container
with the docker run command will generate an event like this:</p>
<div class="sourceCode" id="cb103"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>type=SYSCALL msg=audit(1234567890.123:12345): arch=abc123 syscall=59 success=yes exit=0 a0=12345678abc\</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>a1=0 a2=abc12345678 a3=a items=1 ppid=1234 pid=12345 auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 \</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>s\</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>gid=0 fsgid=0 tty=pts0 ses=1 comm=&quot;docker&quot; exe=&quot;/usr/bin/docker&quot; subj=system_u:object_r:container_runt\</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>ime_exec_t:s0 key=&quot;audit-docker&quot;</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>type=CWD msg=audit(1234567890.123:12345): cwd=&quot;/home/firstname&quot;</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>type=PATH msg=audit(1234567890.123:12345): item=0 name=&quot;/usr/bin/docker&quot; inode=123456 dev=fd:00 mode=0\</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>100600 ouid=0 ogid=0 rdev=00:00 obj=system_u:object_r:container_runtime_exec_t:s0</span></code></pre></div>
<p>Audit logs like this when combined and correlated with Kubernetes
audit features create a comprehensive and trustworthy picture that can
not be repudiated.</p>
<h3 id="information-disclosure">Information Disclosure</h3>
<p>Information disclosure is when sensitive data is leaked, there are
lots of ways it can happen including hacked data stores and API that
unintentionally expose sensitive data.</p>
<h4 id="protecting-cluster-data">Protecting cluster data</h4>
<p>In the Kubernetes world the entire configuration of the cluster is
stored in the cluster store (usually etcd). This includes network and
storage configuration as well as passwords and other sensitive data in
Secrets. For obvious reasons this makes the cluster store a prime target
for information disclosure attacks. As a minimum you should limit and
audit access to the nodes hosting the cluster store, as will be seen in
the next paragraph, gaining access to the cluster node can allow the
logged-on user to bypass some of the security layers</p>
<p>Kubernetes 1.7 introduced encryption of Secrets but does not enable
it by default. Even when this becomes default, the data encryption key
(DEK) is stored on the same node as the Secret. This means gaining
access to a node lets you to bypass encryption completely. This is
especially worrying on nodes that host the cluster store (etcd
nodes).</p>
<p>Fortunately Kubernetes 1.11 enabled a beta feature that lets you
store key encryption keys (KEK), outside of your Kubernetes cluster.
These types of keys are used to encrypt and decrypt data encryption keys
and should be safely guarded. You should seriously consider Hardware
security modules (HSM) or cloud based key management stores (KMS) for
storing your encryption keys. Keep an eye on upcoming versions of
Kubernetes for further improvements of Secrets.</p>
<h4 id="protecting-data-in-pods">Protecting data in Pods</h4>
<p>As previously mentioned, Kubernetes has an API resource called a
Secret that is the preferred way to store and share sensitive data such
as passwords. For example a front end container accessing an encrypted
back end database can have the key to decrypt the database mounted as a
secret. This is far better solution than storing the decryption keys in
plain text file or environment variable.</p>
<p>It is also common to store data and configuration information outside
of Pods and containers in persistent volumes and config maps. If the
data on these is encrypted keys for decrypting them should also be
stored in Secrets. With all of this in mind it is vital that you
consider the caveats outlined in the previous section relative to
Secrets and how their encryption keys are stored. You do not want to do
the hard work of locking the house but leaving the key on the door</p>
<h3 id="denial-of-service">Denial of Service</h3>
<p>Denial of Service is all about making something unavailable. There
are many types of <code>DoS</code> attacks, but a well known variation
is overloading a System to the point it can no longer service requests.
In the Kubernetes world a potential attack might be to overload the API
server so that cluster operations grind to a halt, even essential system
services have to communicate via the API server</p>
<h4 id="protecting-cluster-resources">Protecting cluster resources</h4>
<p>It is a time honored best practice to replicate essential control
plane service on a multiple nodes for a high availability. Kubernetes is
no different and you should run multiple Masters in an HA configuration,
for your production environments. Doing this prevents a single Master
from becoming a single point of failure. In relation to certain types of
denial of service attacks, an attacker may need to attack more than one
Master to have a meaningful impact. You should also consider replicating
control plane nodes across availability zones. This may prevent a denial
of service attack on the network of a particular availability zone from
taking down your entire control plane. The same principle applies to
worker nodes. Having multiple worker nodes not only allows the scheduler
to spread your app over multiple nodes and availability zones, it may
also render DOS attacks on a single node or zone ineffective (or less
effective). You should also configure appropriate limits for the
following:</p>
<ul>
<li>Memory</li>
<li>Processor</li>
<li>Storage</li>
<li>Kubernetes objects</li>
</ul>
<p>Placing limits on things can help prevent important systems resources
from being starved therefore preventing potential denial of service
attacks. Limiting Kubernetes objects includes things like limiting the
number of <code>ReplicaSets</code>, Pods, Services, Secrets and
<code>ConfigMaps</code> in particular Namespace. Here is an example
manifest file/snippet that limits the number of Pod objects in the
<code>skippy</code> namespace to 100</p>
<div class="sourceCode" id="cb104"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ResourceQuota</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pod-quota</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">hard</span><span class="kw">:</span></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">pods</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;100&quot;</span></span></code></pre></div>
<p>There is another feature also called <code>podPidsLimit</code>, which
restricts the number of processes a Pod can create. Assume a scenario
where a Pod is the target of a fork bomb attack, this is a specialised
attack where a rogue process creates as many new processes as possible
in an attempt to consume all resources on a system and grind it to a
halt. Placing a limit on the number of processes a Pod can create will
prevent the Pod from exhausting the resources of the node and confine
the impact of the attack to the Pod. Once the <code>podPidsLimit</code>
is exhausted a Pod will typically be restarted.</p>
<h4 id="protecting-the-api-server">Protecting the API server</h4>
<p>The API server exposes a RESTful interface over a TCP socket making
it susceptible to botnet based denial of service attacks, the following
may be helpful in either preventing or mitigating such attacks.</p>
<ul>
<li><p>Highly available masters. Having multiple API server replicas
running on multiple nodes across multiple availability zones</p></li>
<li><p>Monitoring and alerting API server requests based on sane
thresholds</p></li>
<li><p>Using things like firewalls to limit API server exposure to the
internet.</p></li>
</ul>
<p>As well as botnet DOS attacks an attacker may also attempt to spoof a
user or other control plane service in an attempt to cause an overload.
Fortunately, Kubernetes has robust authentication and authorization
controls to prevent spoofing. However even with a robust RBAC model, it
is vital that you safeguard access to accounts with high privileges.</p>
<h4 id="protecting-the-cluster-store">Protecting the cluster store</h4>
<p>Cluster configuration is stored in etcd, making it vital that etcd be
available and secure. The following recommendations help accomplish
this:</p>
<ul>
<li><p>Configure an HA etcd cluster with either 3 or 5 nodes</p></li>
<li><p>Configure monitoring and alerting of requests to etcd</p></li>
<li><p>Isolate etcd at the network level so that only members of the
control plane can interact with it</p></li>
</ul>
<p>A default installation of Kubernetes installs etcd on the same
servers as the rest of the control plane. This is usually fine for
development and testing however large production clusters should
seriously consider a dedicated etcd, cluster. This will provide better
performance and greater resilience. On the performance front etcd is
probably the most common choking point for large Kubernetes clusters.
With this in mind, you should perform testing to ensure the
infrastructure it runs on is capable of sustaining performance at scale,
a poorly performing etcd can be as bad as an etcd cluster under a
sustained attack. Operating a dedicated etcd cluster also provides
additional resilience by protecing it from other parts of the control
plane that might be compromised. Monitoring and alerting etcd should be
based on sane thresholds and a good place to start is by monitoring etcd
log entries.</p>
<h4 id="protecting-application-components">Protecting application
components</h4>
<p>Most Pods expose their main service on the network and without
additional controls in place anyone with access to the network can
perform a DOS attack on the POD. Fortunately, Kubernetes provides Pod
resource requests limits to prevent such attacks from exhausting Pod and
Node resources, As well as these the following will be helpful.</p>
<ul>
<li><p>Define Kubernetes Network Policies to restrict Pod to Pod and Pod
to external communications</p></li>
<li><p>Utilize mutual TLS and API token based authentication for
application level authentication reject any unauthenticated
requests</p></li>
</ul>
<h3 id="elevation-of-privilege">Elevation of privilege</h3>
<p>Privilege escalation is gaining higher access than what is granted
usually in order to cause damage or gain unauthorized access. Let us
look at a few way to prevent this in a Kubernetes environment.</p>
<h4 id="protecting-the-api-server-1">Protecting the API server</h4>
<p>Kubernetes offers several authorization modes, that help safeguard
access to the API server. These include - RBAC, Webhook, Node. You
should run multiple authorizers at the same time. For example a common
best practice is to always have RBAC and node enabled.</p>
<p>RBAC mode lets you restrict API operations to sub sets of users.
These users can be regular users accounts as well as system services.
The idea is that all requests to the API server must be authenticated
and authorized. Authentication ensures that requests are coming from the
validated user, whereas authorization ensures that validated user is
allowed to perform the requested operation. For example, can Lily create
Pods ? In this example Lily is the user, create is the operation, and
Pods is the resource. Authentication makes sure that it really is Lily
that is making the request and authorization determines if she is
allowed to create Pods.</p>
<p>Webhook mode lets you offload authorization to an external REST based
policy engine. However it requires additional effort to build and
maintain the external engine. It also makes the external engine a
potential single point of failure for every request, to the API server.
For example if the external webhook system becomes unavailable you may
not be able to make any requests to the API server. With this in mind,
you should be rigorous in vetting and implementing any webhook
authorization service</p>
<h4 id="protecting-pods">Protecting Pods</h4>
<p>The next few sections will look at a few of the technologies that
help reduce the risk of elevation of privilege attacks against Pods and
containers, we will look at the following - preventing processes from
running as root, dropping capabilities filtering syscalls.</p>
<h5 id="root-processes">Root processes</h5>
<p>The root user is the most powerful user on a Linux system and it
always User ID 0. Therefore running application processes as root is
almost always a bad idea as it grants the app process full access to the
container. This is mad even worse by the fact that the root user of
container often has unrestricted root access on the host system as well.
Fortunately Kubernetes lets you force container processes to run as
unprivileged non root users. The following Pod manifest configures all
containers that are part of this Pod to run processes as UID 1000. If
the Pod has multiple containers all processes in all containers will run
as UID 1000.</p>
<div class="sourceCode" id="cb105"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> demo</span></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span><span class="co"> # Applies to all containers in this Pod</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">1000</span><span class="co"> # Non-root user</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> demo</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">image</span><span class="kw">:</span><span class="at"> example.io/simple:1.0</span></span></code></pre></div>
<p>The <code>runAsUser</code> is one of the many settings that can be
configured as part of what we refer to as
<code>PodSecurityPolicy</code>. It is possible for two or more Pods to
be configured with the same <code>runAsUser</code>. When this happens,
the containers from both Pods will run with the same security context
and potentially have access to the same resources. This might be fine if
they are replicas of the same Pod or container. However there is a high
chance this will cause problems if they are different containers. For
example two different containers with R/W access to the same host
directory or volume can cause data corruption bot writing to the same
dataset without coordinating write operations). Shared security contexts
also increase the possibility of compromised container tampering with a
dataset it should not have access to</p>
<p>With this in mid, it is possible to use the
<code>securityContext.runAsUser</code> property at the container level
instead of the Pod level:</p>
<div class="sourceCode" id="cb106"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> demo</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span><span class="co"> # Applies to all containers in this Pod</span></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">1000</span><span class="co"> # Non-root user</span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> demo</span></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">image</span><span class="kw">:</span><span class="at"> example.io/simple:1.0</span></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">2000</span><span class="co"> # Overrides the Pod setting</span></span></code></pre></div>
<p>This example sets the UID to 1000 at the pod level, but overrides it
at the container level, so that processes in one particular container
run a UID 2000. Unless otherwise specified all other containers in the
Pod will use UID 1000. A couple of other things that might help get
around the issue of multiple Pods and containers using the same UID
include - enabling user namespaces, maintaining a map of UID usage.</p>
<p>User namespaces is a Linux kernel technology that allows a process to
run as root within a container, but run as a different user outside of
the container. For example a process can run as UID 0 (the root user) in
the container). But get mapped to UID 1000 on the host. This can be a
good solution for processes that need to run as root inside the
container but you should check it has full support from your version of
Kubernetes and your container runtime. Maintaining a map of UID usage is
a clunky way to prevent multiple different Pods and containers using
overlapping UID, it is a bit of a hack and requires strict adherence to
a gated release process for releasing Pods into production.</p>
<h4 id="drop-capabilities">Drop capabilities</h4>
<p>While user namespaces allow container processes to run as root inside
the container but not on the host machine it remains a fact that most
processes do not really need to run as full root inside the container.
However it is equally true that many processes do require more
privileges than a typical non root user. What is needed is a way to
grant the exact set of privileges to process requires in order to run.
Enter capabilities.</p>
<p>Time for some theory and background history. We have already said
that the root user is the most powerful user on the Linux system.
However its power is a combination of lots of small privileges that we
call capabilities. For example the <code>SYS_TIME</code>, capability
allows a user to set the system clock, whereas the NET_ADMIN capability
allows a user to perform network related operations such as modifying
the local routing table and configuring local interfaces, the root user
holds every capability and is therefore extremely powerful.</p>
<p>Having a modular set of capabilities like this allows you to be
extremely granular when granting permissions. Instead of an all or
nothing (root or non-root) approach you can grant a process the exact
set of capabilities it requires to run.</p>
<p>There are currently over 30 capabilities and choosing the right ones
can be daunting. With this in mind, an out of the box Docker runtime
drops over half of them by default. This is a sensible default that is
designed to allow most processes to run, without leaving the keys in the
front door. While sensible defaults like these are better than nothing
they are often not good enough for a lot of production environments.</p>
<p>A common way to find the absolute minimum set of capabilities an app
requires is to run it in a test environment, with all capabilities
dropped. This will cause the app to fail and log messages about the
missing permissions. You map those permissions to capabilities add them
to the app Pod spec and run the app again. You rinse and repeat this
process until the app runs properly with the minimum set of
capabilities.</p>
<p>As good as this is there are a few things to consider. Firstly you
must perform extensive testing of your app. The last thing you want is a
production edge case that you had not accounted for in your test
environment. Such occurrences can crash your app in production. Secondly
every fix and update to your app requires the exact same extensive
testing against the capability set.</p>
<p>With these considerations in mind, it is vital that you have testing
procedures and production release process that can handle all of this.
By default Kubernetes implements the default set of capabilities
implemented by your chosen container runtime. However you can override
this in a pod security policy, or as part of the container
<code>securityContext</code></p>
<div class="sourceCode" id="cb107"><pre
class="sourceCode yml"><code class="sourceCode yaml"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> capability-test</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> demo</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> example.io/simple:1.0</span></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">capabilities</span><span class="kw">:</span></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">add</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;NET_ADMIN&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;CHOWN&quot;</span><span class="kw">]</span></span></code></pre></div>
<h4 id="filter-syscalls">Filter syscalls</h4>
<p>Seccomp short for secure computing is similar in concept to
capabilities but works by filtering syscalls rather than capabilities.
The way a Linux process asks the kernel to perform an operation is by
issuing a syscall . Seccomp lets you control which syscalls a particular
container can make to the host kernel. As with capabilities a least
privilege model is preferred where the only syscalls a container is
allowed to make are those the ones it needs to in order to function and
run properly. Seccomp went GA in Kubernetes in 1.19 and can be used in
different ways based on the following seccomp profiles.</p>
<ol type="1">
<li><p>Non-blocking: Allow a Pod to run and records every syscall it
makes to an audit log, you can use to create a custom profile. The idea
is to run your app, Pod in a <code>dev/test</code> environment and make
it do everything designed to do. When you are done you will have a log
file listing every syscall the Pod needs in order to run. You then use
this to create a custom profile that only allows the syscalls the app
needs least privilege</p></li>
<li><p>Blocking: Blocks all syscalls, it is extremely secure, but
prevents Pod from doing anything useful</p></li>
<li><p>Runtime: forced a Pod to use the seccomp profile, defined by its
container runtime. This is a common place to start if you have not
created a custom profile yet. Profiles that ship with container runtimes
(like Docker and containerd) are not the most secure in the world, but
they are not wide open either. They are usually designed to be balance
of usable and secure and they are thoroughly tested.</p></li>
<li><p>Custom: A profile that only allows the syscalls your app needs in
order to run. Everything else is blocked. It is a common to extensively
test your app in dev/test with a non blocking profile that records all
syscalls to a log. You then use this log to identify the syscalls your
app makes and build the customized profile. The danger with this
approach is that your app has some edge case that you miss with your
testing. If you this happens your app can fail in production when i when
it hits an edge case and uses a syscall not captured in the logs during
testing</p></li>
</ol>
<h4 id="privilege-escalation">Privilege escalation</h4>
<p>The only way to create a new process in Linux is for one process to
clone itself and then load a new instructions to the new process (very
simplified, but the original process is called the parent process and
the copy is the child, this is called process forking).</p>
<p>By default Linux allows a child process to claim more privileges than
its parent. This is usually a bad idea. In fact you will often want a
child process to have the same or less privileges than its parent. This
is especially true for containers as their security configurations are
defined against their initial configuration and not against potentially
escalated privileges.</p>
<p>Fortunately it is possible to prevent privilege escalation through a
<code>PodSecurityPolicy</code> or the <code>securityContext</code>
property of an individual container.</p>
<h4 id="pod-security-policies">Pod security policies</h4>
<p>As you have seen throughout the sections one can enable security
settings on a per Pod basis by setting security context attributes in
individual Pod manifest files. However this approach does not scale
well, it requires developers and operators to remember to do this for
every Pod manifest spec, and is prone to errors. Pod security Policies
offer a better way.</p>
<p>Pod security policies allow you to define a security settings at the
cluster level. You can then apply them to targeted set of Pods as part
of the deployment process. This approach scales better requires less
effort from developers and admins and is less prone to error, it also
lends itself to situations where you have a team dedicated to securing
apps in production.</p>
<p>Pod Security Policies are implemented as an admission controller and
in order to use them, a Pod <code>ServiceAccount</code> must be
authorized to use it. Once this is done policies are applied to new
requests to create Pods as they pass through the API admission
chain.</p>
<h2 id="real-world-security">Real world security</h2>
<p>In the previous sections it was described how to thread model the
Kubernetes security using the <code>STRIDE</code> model. In this section
the common security related challenges will be covered that are likely
to be encountered in the real world, when implementing a Kubernetes
cluster.</p>
<p>While every Kubernetes deployment is different there are many
similarities as a result the examples you will see will apply to most
Kubernetes deployments large and small. Now then we are not offering
cookbook style solutions, instead we will be looking at things from the
kind of high level view a security architect has. The section is split
into the following sub-sections</p>
<ul>
<li>CI/CD pipeline</li>
<li>Infrastructure and networking</li>
<li>Identity and access management</li>
<li>Security monitoring and auditing</li>
</ul>
<h3 id="cicd-pipeline">CI/CD pipeline</h3>
<p>Containers are a revolutionary app packaging and runtime technology.
On the packaging front they bundle apps code and dependencies into an
image, as well as code and dependencies images contain the commands
required to run the app. This has enabled containers to hugely simplify
the process of building sharing and running apps, it also overcome the
infamous - it worked on my laptop.</p>
<p>However containers make running dangerous code easier than ever
before, With this in mind let us look at some ways you can secure the
flow of app code from a developer laptop to production servers.</p>
<h4 id="image-repositories">Image Repositories</h4>
</body>
</html>
